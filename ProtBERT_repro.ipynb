{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch, torch.nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import re\n",
    "\n",
    "import re, gc, os\n",
    "import requests\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_validation_splits(df):\n",
    "    test,train_tot = df.loc[df['set']=='test'],df.loc[df['set']=='train']\n",
    "    train, val = train_tot.loc[train_tot['validation']!=True], train_tot.loc[train_tot['validation']==True]\n",
    "    return train,val, test\n",
    "\n",
    "def embed_dataset(dataset_seqs, model, tokenizer, device, shift_left = 1, shift_right = -1):\n",
    "  inputs_embedding = []\n",
    "\n",
    "  for sample in tqdm(dataset_seqs):\n",
    "    with torch.no_grad():\n",
    "      ids = tokenizer.batch_encode_plus([sample], add_special_tokens=True, padding=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "      embedding = model(input_ids=ids['input_ids'].to(device))[0]\n",
    "      inputs_embedding.append(embedding[0].detach().cpu().numpy()[shift_left:shift_right])\n",
    "\n",
    "  return inputs_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "path_meltome = 'datasets/FLIP/splits/meltome/splits/mixed_split.csv'\n",
    "\n",
    "splits_meltome = pd.read_csv(path_meltome, sep=',')\n",
    "train, val, test = train_test_validation_splits(splits_meltome)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "if torch.cuda.is_available():\n",
    "  model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 10359/22335 [26:16<34:35,  5.77it/s] "
     ]
    }
   ],
   "source": [
    "train_embeddings = embed_dataset(train.sequence, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)   \n",
    "\n",
    "class regressionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, channel_shape, input_shape, kernel):\n",
    "        super(regressionHead, self).__init__()\n",
    "        self.channel_dim = channel_shape\n",
    "        self.input_shape = input_shape # pos0 = #channels, pos1 = #diagonal comps, or viseversa\n",
    "\n",
    "        self.pooling = \n",
    "\n",
    "        self.FFNN = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.channel_dim),\n",
    "            nn.Conv1d(self.channel_dim, self.channel_dim, kernel_size=kernel, stride=1, padding=kernel//2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(self.channel_dim, self.channel_dim, kernel_size=kernel, stride=1, padding=kernel//2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(self.channel_dim, self.channel_dim, kernel_size=kernel, stride=1, padding=kernel//2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            Flatten(),\n",
    "            nn.Linear(self.channel_dim*self.input_shape[1], 1)\n",
    "        )\n",
    "        \n",
    "        #self.encoder.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.FFNN(x)\n",
    "        return z #z.permute(0,2,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0454,  0.1140, -0.0117,  ..., -0.0875, -0.1143,  0.0204],\n",
      "         [ 0.0923,  0.1391, -0.0524,  ..., -0.1395, -0.0428,  0.0743],\n",
      "         [ 0.1151,  0.0200, -0.0863,  ..., -0.0095, -0.1873,  0.1317],\n",
      "         ...,\n",
      "         [ 0.1079,  0.0977, -0.0583,  ..., -0.1277, -0.0649,  0.1289],\n",
      "         [ 0.0546,  0.0364, -0.0782,  ..., -0.0302, -0.0602,  0.0890],\n",
      "         [ 0.0515,  0.0571, -0.0693,  ..., -0.0394, -0.0663,  0.0977]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.2487,  0.2626, -0.2367,  ...,  0.2503,  0.2339, -0.2556]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "sequence_Example = \"A E T C Z A O\"\n",
    "sequence_Example = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\n",
    "encoded_input = tokenizer(sequence_Example, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latent_msaprot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
