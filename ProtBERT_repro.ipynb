{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4fnkwCP5lAB",
        "outputId": "c32303fd-f2a8-4677-894e-ac700df948e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/gdrive/')\\n\\n%cd /content/gdrive/MyDrive/ThermostabilityProject/\\n!pip install transformers tqdm\""
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "%cd /content/gdrive/MyDrive/ThermostabilityProject/\n",
        "!pip install transformers tqdm\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bBvNg9-S5eNV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kvr226/miniconda3/envs/latent_msaprot_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch, torch.nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import re\n",
        "\n",
        "import re, gc, os\n",
        "import requests\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9PMfsMzb5eNZ"
      },
      "outputs": [],
      "source": [
        "def train_test_validation_splits(df):\n",
        "    test,train_tot = df.loc[df['set']=='test'],df.loc[df['set']=='train']\n",
        "    train, val = train_tot.loc[train_tot['validation']!=True], train_tot.loc[train_tot['validation']==True]\n",
        "\n",
        "    \n",
        "    return train,val, test\n",
        "\n",
        "def embed_dataset(dataset_seqs, model, tokenizer, device, shift_left = 1, shift_right = -1):\n",
        "  inputs_embedding = []\n",
        "\n",
        "  for sample in tqdm(dataset_seqs):\n",
        "    with torch.no_grad():\n",
        "      ids = tokenizer.batch_encode_plus([sample], add_special_tokens=True, padding=True, is_split_into_words=True, return_tensors=\"pt\")\n",
        "      embedding = model(input_ids=ids['input_ids'].to(device))[0]\n",
        "      inputs_embedding.append(embedding[0].detach().cpu().numpy()[shift_left:shift_right])\n",
        "\n",
        "  return torch.from_numpy(np.stack(inputs_embedding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F9Qo57a5eNa",
        "outputId": "acb8a20f-5bea-41d8-aec9-384f3ef27216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "path_meltome = 'datasets/FLIP/splits/meltome/splits/mixed_split.csv'\n",
        "\n",
        "splits_meltome = pd.read_csv(path_meltome, sep=',')\n",
        "train, val, test = train_test_validation_splits(splits_meltome)\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251,
          "referenced_widgets": [
            "7822a461ad034eacacacb56f82d8dfb4",
            "db4c9b0b328c4917abe4401d3f9dc033",
            "4bfb0e43cc2f4991b4510a055baca842",
            "e4f552e570964de98ab9e0310ad9b33f",
            "3dda1a7c8ab140a8ad2595b21af90c6f",
            "44c8715553cf4a3a95936549cc74bb89",
            "7dc0fd88e2864a1e8fc2e1215e9a4c8d",
            "befa4bccfd4e494a8abda6c807b71528",
            "9b4eb5c771e7425db55130f7b16162db",
            "a16db5ff745b454eb37563e0936f05c2",
            "ba1440563002411791477f1eb9be3423",
            "f2814c1cd10f4d819acaaa57126dc3e1",
            "45a2a0e196f742c59dcb34a6e9346718",
            "8766f189f5034beda0e0d7aa90c2459c",
            "c213b24d72a343e9b8e97737914ac89c",
            "db61f8623f99475687c3d1300b0fdf3e",
            "787271a9e3164beaa8a1195854e7325e",
            "0179d901a24a49bebcf619dd9b23f499",
            "105c86e728904d7c868a64e72122fcaa",
            "515397afe01c47d083f764ed86033ccb",
            "b1d408429eaa4b3b90055bc2f587f0fc",
            "a85157025c7449e5939f8343888be8f3",
            "4bffb5bb6bae4a86851743382bbda61c",
            "3a8b58ac59844cebb6207b37fe6c7be2",
            "388ad816a93643fea127cd9c33afd97a",
            "e4dc6ab93bac45a28824e19a1f829c1c",
            "8e0ce9e69c564fbc98ce7b5eab24d310",
            "80678ab5c8e546119affbb9b6446fa43",
            "18a7e1404b6049e4af2f4bbabfdefb36",
            "5d634485f9ef4cd88cfd45397b9bfac0",
            "3ea050c4f9324584b513f835c95218fe",
            "71155af0479f4b59806c50857e5f1c18",
            "5812d055512344d299a37b583163c601",
            "941d8050984b4331a0936a1da042768c",
            "ade7c5f1507a4f7690dd110f43da6667",
            "77b3df2f0e084407b8cc442539e65bdf",
            "e0cff3879a01467e9325af0f09e7a18b",
            "3d4ea54c2fb944978474e6ee09e96afc",
            "22a0da479c2849cbb4ba95617989dc40",
            "2c7db96ef65d45e185ba2b3b5fc6f6bd",
            "9cd9a2aa719242419d7bc8273aa4a80a",
            "244e52841e8e406cbc5609b5aa8ffe2f",
            "b3dbe6a79a954a33b6c51f2f56bafd9c",
            "f7f24861c60f4f189170bb7ade386147",
            "e568a877298f43a0b51dda9f618ee888",
            "b949f362b2054ffb85c5995fc15159eb",
            "e9126118107f416a85f3c9ad8e49d0ac",
            "13f8c6cc382d4731ab5324bd90fb352b",
            "20d45acbe62f4e94bf702a4ace6d61fc",
            "051662d4e71b4ec4b580f161c9cb5238",
            "be8e30dcb69745499ba1da6c109591c2",
            "baf8a5845c7547048a6e8e70a2718747",
            "229c8aab3d6a45d79295c7df8e3f586e",
            "368d849d84f94ec0a421534f2a84d1cf",
            "e4092e002d6a4930bdb5782e94faa99d"
          ]
        },
        "id": "60ltFG8m5eNb",
        "outputId": "2b6dcf65-e755-4ff5-940a-be14571df61f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
        "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = model.to(device=device)\n",
        "model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w-zm1vGvxwEb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Premade Embeddings...\n",
            "Loaded\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dir_pre_generated_embeddings = ''\n",
        "path_train= dir_pre_generated_embeddings+'train_set_embeddings.pth'\n",
        "path_test= dir_pre_generated_embeddings+'test_set_embeddings.pth'\n",
        "path_val= dir_pre_generated_embeddings+'val_set_embeddings.pth'\n",
        "\n",
        "\n",
        "\n",
        "if os.path.isfile(path_train) and os.path.isfile(path_test) and os.path.isfile(path_val):\n",
        "    print (\"Loading Premade Embeddings...\")\n",
        "    train_embeddings = torch.load(path_train, map_location=device)\n",
        "    test_embeddings = torch.load(path_test, map_location=device)\n",
        "    val_embeddings = torch.load(path_val, map_location=device)\n",
        "    print (\"Loaded\")\n",
        "else:\n",
        "    train_embeddings = embed_dataset(train.sequence, model, tokenizer, device)\n",
        "    test_embeddings = embed_dataset(test.sequence, model, tokenizer, device)\n",
        "    val_embeddings = embed_dataset(val.sequence, model, tokenizer, device)\n",
        "    torch.save(train_embeddings, 'train_set_embeddings.pth')\n",
        "    torch.save(test_embeddings, 'test_set_embeddings.pth')\n",
        "    torch.save(val_embeddings, 'val_set_embeddings.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1s-VG_FMmlh",
        "outputId": "7f732ce6-b246-4649-85b0-349a21ec2957"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data_utils\n",
        "\n",
        "batch_size=200\n",
        "\n",
        "\n",
        "trainDataset = data_utils.TensorDataset(train_embeddings, torch.from_numpy( train.target.to_numpy() ))\n",
        "testDataset = data_utils.TensorDataset(test_embeddings, torch.from_numpy( test.target.to_numpy() ))\n",
        "valDataset = data_utils.TensorDataset(val_embeddings, torch.from_numpy( val.target.to_numpy() ))\n",
        "\n",
        "train_loader = data_utils.DataLoader(trainDataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = data_utils.DataLoader(testDataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = data_utils.DataLoader(valDataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vmQ0v7Cv5eNc"
      },
      "outputs": [],
      "source": [
        "class Flatten(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)   \n",
        "\n",
        "class regressionHead(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, kernel):\n",
        "        super(regressionHead, self).__init__()\n",
        "        #self.input_shape = input_shape # pos0 = #channels, pos1 = #diagonal comps, or viseversa\n",
        "\n",
        "        self.pooling = torch.nn.Sequential(\n",
        "            torch.nn.AvgPool1d(kernel, stride=2, padding=kernel//2),\n",
        "            Flatten())\n",
        "\n",
        "        self.FFNN = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512, 512),\n",
        "            torch.nn.LeakyReLU(0.1), #nn.ReLU(),\n",
        "            torch.nn.Linear(512, 512),\n",
        "            torch.nn.LeakyReLU(0.1), #nn.ReLU(),\n",
        "            torch.nn.Linear(512, 1)\n",
        "        )\n",
        "        \n",
        "        #self.encoder.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.kaiming_normal_(module.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pooling(x)\n",
        "        z = self.FFNN(x)\n",
        "        return z #z.permute(0,2,1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_LLMRegresor(train_iterator, val_iterator, model, device, criterion, optimizer, epoch_num):\n",
        "    \n",
        "        \n",
        "        val_loss = []\n",
        "        \n",
        "        model.to(device)\n",
        "\n",
        "        for epoch in range(epoch_num):\n",
        "            model.train() \n",
        "            for i, (input, label) in enumerate(train_iterator):\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                input = input.to(dtype=torch.float32, device=device)\n",
        "                label = label.to(dtype=torch.float32, device=device) \n",
        "                out = model(input)\n",
        "                loss = criterion(out, label.unsqueeze(-1))\n",
        "                loss.backward() \n",
        "                optimizer.step()\n",
        "\n",
        "            with torch.no_grad(): # evaluate validation loss here \n",
        "\n",
        "                model.eval()\n",
        "                val_loss_epochs = []\n",
        "\n",
        "                for (inputval, labelval) in val_iterator:\n",
        "                    \n",
        "                    inputval = inputval.to(device)\n",
        "                    labelval = labelval.to(device)\n",
        "                    outval = model(inputval)\n",
        "                    lossval = criterion(outval, labelval.unsqueeze(-1)) # Calculate validation loss \n",
        "                    val_loss_epochs.append(lossval.item())\n",
        "\n",
        "                val_loss_epoch = np.mean(val_loss_epochs)\n",
        "                val_loss.append(round(val_loss_epoch, 3))\n",
        "\n",
        "            print('epoch: %d loss: %.3f val loss: %.3f' % (epoch + 1, loss.item(), val_loss_epoch))\n",
        "\n",
        "            '''# evalutate whether validation loss is dropping; if not, stop\n",
        "            if epoch > 21:\n",
        "                if val_loss[-1] >= np.min(val_loss[:-20]): \n",
        "                    print('Finished training at epoch {0}'.format(epoch))\n",
        "                    return epoch'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1 loss: 2909.012 val loss: 2762.943\n",
            "epoch: 2 loss: 2715.153 val loss: 2722.006\n",
            "epoch: 3 loss: 2624.065 val loss: 2635.666\n",
            "epoch: 4 loss: 2592.348 val loss: 2514.049\n",
            "epoch: 5 loss: 2269.957 val loss: 2322.357\n",
            "epoch: 6 loss: 2175.083 val loss: 2095.253\n",
            "epoch: 7 loss: 1767.318 val loss: 1825.978\n",
            "epoch: 8 loss: 1376.160 val loss: 1509.756\n",
            "epoch: 9 loss: 1405.571 val loss: 1212.661\n",
            "epoch: 10 loss: 879.089 val loss: 923.968\n",
            "epoch: 11 loss: 555.846 val loss: 669.342\n",
            "epoch: 12 loss: 519.346 val loss: 469.327\n",
            "epoch: 13 loss: 364.412 val loss: 320.379\n",
            "epoch: 14 loss: 158.262 val loss: 221.962\n",
            "epoch: 15 loss: 147.460 val loss: 161.804\n",
            "epoch: 16 loss: 115.980 val loss: 133.168\n",
            "epoch: 17 loss: 113.839 val loss: 121.008\n",
            "epoch: 18 loss: 125.127 val loss: 116.824\n",
            "epoch: 19 loss: 91.354 val loss: 114.968\n",
            "epoch: 20 loss: 135.085 val loss: 114.476\n",
            "epoch: 21 loss: 113.085 val loss: 113.057\n",
            "epoch: 22 loss: 99.349 val loss: 111.923\n",
            "epoch: 23 loss: 71.572 val loss: 112.447\n",
            "epoch: 24 loss: 95.032 val loss: 113.961\n",
            "epoch: 25 loss: 116.235 val loss: 115.968\n",
            "epoch: 26 loss: 95.800 val loss: 111.249\n",
            "epoch: 27 loss: 117.263 val loss: 112.860\n",
            "epoch: 28 loss: 167.512 val loss: 112.171\n",
            "epoch: 29 loss: 111.406 val loss: 113.309\n",
            "epoch: 30 loss: 87.299 val loss: 111.895\n",
            "epoch: 31 loss: 135.955 val loss: 113.148\n",
            "epoch: 32 loss: 122.918 val loss: 115.862\n",
            "epoch: 33 loss: 101.699 val loss: 110.951\n",
            "epoch: 34 loss: 86.642 val loss: 111.601\n",
            "epoch: 35 loss: 127.575 val loss: 111.107\n",
            "epoch: 36 loss: 187.102 val loss: 112.347\n",
            "epoch: 37 loss: 133.590 val loss: 112.600\n",
            "epoch: 38 loss: 98.618 val loss: 111.944\n",
            "epoch: 39 loss: 171.414 val loss: 111.276\n",
            "epoch: 40 loss: 87.300 val loss: 113.310\n",
            "epoch: 41 loss: 98.079 val loss: 111.269\n",
            "epoch: 42 loss: 125.097 val loss: 113.596\n",
            "epoch: 43 loss: 147.591 val loss: 114.313\n",
            "epoch: 44 loss: 117.420 val loss: 110.903\n",
            "epoch: 45 loss: 104.961 val loss: 113.853\n",
            "epoch: 46 loss: 100.146 val loss: 112.188\n",
            "epoch: 47 loss: 89.423 val loss: 111.970\n",
            "epoch: 48 loss: 89.257 val loss: 112.437\n",
            "epoch: 49 loss: 100.952 val loss: 111.884\n",
            "epoch: 50 loss: 111.527 val loss: 111.551\n",
            "epoch: 51 loss: 113.483 val loss: 112.149\n",
            "epoch: 52 loss: 109.370 val loss: 112.901\n",
            "epoch: 53 loss: 89.417 val loss: 111.302\n",
            "epoch: 54 loss: 133.153 val loss: 113.279\n",
            "epoch: 55 loss: 88.614 val loss: 111.649\n",
            "epoch: 56 loss: 109.862 val loss: 111.888\n",
            "epoch: 57 loss: 77.249 val loss: 113.415\n",
            "epoch: 58 loss: 131.743 val loss: 114.089\n",
            "epoch: 59 loss: 84.567 val loss: 112.474\n",
            "epoch: 60 loss: 114.173 val loss: 111.620\n",
            "epoch: 61 loss: 101.941 val loss: 112.842\n",
            "epoch: 62 loss: 133.888 val loss: 113.719\n",
            "epoch: 63 loss: 123.244 val loss: 113.458\n",
            "epoch: 64 loss: 89.550 val loss: 111.497\n",
            "epoch: 65 loss: 105.692 val loss: 111.800\n",
            "epoch: 66 loss: 79.503 val loss: 113.914\n",
            "epoch: 67 loss: 123.781 val loss: 113.473\n",
            "epoch: 68 loss: 127.718 val loss: 111.663\n",
            "epoch: 69 loss: 130.909 val loss: 112.285\n",
            "epoch: 70 loss: 111.190 val loss: 112.095\n",
            "epoch: 71 loss: 139.745 val loss: 113.102\n",
            "epoch: 72 loss: 99.861 val loss: 115.256\n",
            "epoch: 73 loss: 99.752 val loss: 112.010\n",
            "epoch: 74 loss: 80.749 val loss: 111.369\n",
            "epoch: 75 loss: 84.408 val loss: 113.017\n",
            "epoch: 76 loss: 101.837 val loss: 114.879\n",
            "epoch: 77 loss: 76.045 val loss: 112.573\n",
            "epoch: 78 loss: 72.950 val loss: 111.054\n",
            "epoch: 79 loss: 99.853 val loss: 112.669\n",
            "epoch: 80 loss: 108.646 val loss: 111.249\n",
            "epoch: 81 loss: 118.675 val loss: 115.986\n",
            "epoch: 82 loss: 107.584 val loss: 112.777\n",
            "epoch: 83 loss: 146.338 val loss: 114.679\n",
            "epoch: 84 loss: 98.286 val loss: 113.877\n",
            "epoch: 85 loss: 54.468 val loss: 111.512\n",
            "epoch: 86 loss: 137.682 val loss: 113.067\n",
            "epoch: 87 loss: 94.004 val loss: 111.085\n",
            "epoch: 88 loss: 106.277 val loss: 111.750\n",
            "epoch: 89 loss: 106.011 val loss: 112.990\n",
            "epoch: 90 loss: 132.521 val loss: 112.194\n",
            "epoch: 91 loss: 139.667 val loss: 113.254\n",
            "epoch: 92 loss: 102.762 val loss: 112.406\n",
            "epoch: 93 loss: 94.204 val loss: 112.102\n",
            "epoch: 94 loss: 119.494 val loss: 113.502\n",
            "epoch: 95 loss: 103.459 val loss: 113.704\n",
            "epoch: 96 loss: 87.584 val loss: 118.487\n",
            "epoch: 97 loss: 103.841 val loss: 112.888\n",
            "epoch: 98 loss: 115.652 val loss: 113.013\n",
            "epoch: 99 loss: 93.826 val loss: 113.768\n",
            "epoch: 100 loss: 109.607 val loss: 112.610\n",
            "epoch: 101 loss: 93.146 val loss: 112.247\n",
            "epoch: 102 loss: 145.632 val loss: 112.246\n",
            "epoch: 103 loss: 186.235 val loss: 112.128\n",
            "epoch: 104 loss: 120.271 val loss: 110.965\n",
            "epoch: 105 loss: 110.291 val loss: 110.350\n",
            "epoch: 106 loss: 136.588 val loss: 112.781\n",
            "epoch: 107 loss: 113.397 val loss: 113.939\n",
            "epoch: 108 loss: 95.161 val loss: 112.559\n",
            "epoch: 109 loss: 139.664 val loss: 113.576\n",
            "epoch: 110 loss: 120.659 val loss: 114.518\n",
            "epoch: 111 loss: 104.368 val loss: 113.050\n",
            "epoch: 112 loss: 109.559 val loss: 112.669\n",
            "epoch: 113 loss: 139.256 val loss: 111.988\n",
            "epoch: 114 loss: 128.442 val loss: 111.756\n",
            "epoch: 115 loss: 140.753 val loss: 112.425\n",
            "epoch: 116 loss: 115.060 val loss: 112.843\n",
            "epoch: 117 loss: 156.584 val loss: 112.355\n",
            "epoch: 118 loss: 113.024 val loss: 111.998\n",
            "epoch: 119 loss: 144.157 val loss: 111.579\n",
            "epoch: 120 loss: 78.646 val loss: 114.168\n",
            "epoch: 121 loss: 128.466 val loss: 112.118\n",
            "epoch: 122 loss: 158.915 val loss: 109.408\n",
            "epoch: 123 loss: 83.945 val loss: 110.585\n",
            "epoch: 124 loss: 125.410 val loss: 112.250\n",
            "epoch: 125 loss: 127.359 val loss: 112.732\n",
            "epoch: 126 loss: 109.230 val loss: 113.383\n",
            "epoch: 127 loss: 90.232 val loss: 111.826\n",
            "epoch: 128 loss: 129.812 val loss: 113.192\n",
            "epoch: 129 loss: 99.958 val loss: 110.836\n",
            "epoch: 130 loss: 118.203 val loss: 113.528\n",
            "epoch: 131 loss: 99.893 val loss: 113.732\n",
            "epoch: 132 loss: 139.924 val loss: 112.934\n",
            "epoch: 133 loss: 93.202 val loss: 114.010\n",
            "epoch: 134 loss: 112.479 val loss: 112.968\n",
            "epoch: 135 loss: 133.928 val loss: 111.260\n",
            "epoch: 136 loss: 103.738 val loss: 111.782\n",
            "epoch: 137 loss: 88.563 val loss: 111.465\n",
            "epoch: 138 loss: 116.430 val loss: 111.177\n",
            "epoch: 139 loss: 94.437 val loss: 114.200\n",
            "epoch: 140 loss: 123.907 val loss: 113.498\n",
            "epoch: 141 loss: 99.141 val loss: 114.577\n",
            "epoch: 142 loss: 105.696 val loss: 112.052\n",
            "epoch: 143 loss: 136.910 val loss: 113.283\n",
            "epoch: 144 loss: 126.005 val loss: 112.124\n",
            "epoch: 145 loss: 124.453 val loss: 109.580\n",
            "epoch: 146 loss: 117.010 val loss: 111.831\n",
            "epoch: 147 loss: 93.342 val loss: 112.606\n",
            "epoch: 148 loss: 97.614 val loss: 112.402\n",
            "epoch: 149 loss: 114.768 val loss: 114.123\n",
            "epoch: 150 loss: 93.306 val loss: 111.166\n",
            "epoch: 151 loss: 114.423 val loss: 110.454\n",
            "epoch: 152 loss: 91.495 val loss: 113.351\n",
            "epoch: 153 loss: 115.750 val loss: 111.948\n",
            "epoch: 154 loss: 167.764 val loss: 111.978\n",
            "epoch: 155 loss: 135.088 val loss: 114.955\n",
            "epoch: 156 loss: 143.357 val loss: 113.755\n",
            "epoch: 157 loss: 116.595 val loss: 114.864\n",
            "epoch: 158 loss: 139.597 val loss: 114.626\n",
            "epoch: 159 loss: 77.312 val loss: 113.118\n",
            "epoch: 160 loss: 110.762 val loss: 112.837\n",
            "epoch: 161 loss: 124.908 val loss: 113.814\n",
            "epoch: 162 loss: 105.301 val loss: 114.004\n",
            "epoch: 163 loss: 90.743 val loss: 112.218\n",
            "epoch: 164 loss: 85.989 val loss: 110.408\n",
            "epoch: 165 loss: 87.982 val loss: 113.380\n",
            "epoch: 166 loss: 127.992 val loss: 111.789\n",
            "epoch: 167 loss: 139.696 val loss: 111.608\n",
            "epoch: 168 loss: 110.402 val loss: 111.376\n",
            "epoch: 169 loss: 100.291 val loss: 112.425\n",
            "epoch: 170 loss: 85.562 val loss: 109.875\n",
            "epoch: 171 loss: 132.643 val loss: 110.429\n",
            "epoch: 172 loss: 136.412 val loss: 113.353\n",
            "epoch: 173 loss: 81.176 val loss: 112.349\n",
            "epoch: 174 loss: 129.075 val loss: 111.241\n",
            "epoch: 175 loss: 84.745 val loss: 110.371\n",
            "epoch: 176 loss: 95.527 val loss: 115.101\n",
            "epoch: 177 loss: 82.279 val loss: 114.112\n",
            "epoch: 178 loss: 170.698 val loss: 112.359\n",
            "epoch: 179 loss: 120.544 val loss: 112.473\n",
            "epoch: 180 loss: 146.341 val loss: 113.430\n",
            "epoch: 181 loss: 122.714 val loss: 110.651\n",
            "epoch: 182 loss: 97.729 val loss: 112.697\n",
            "epoch: 183 loss: 91.524 val loss: 111.633\n",
            "epoch: 184 loss: 105.607 val loss: 114.729\n",
            "epoch: 185 loss: 133.955 val loss: 112.051\n",
            "epoch: 186 loss: 98.313 val loss: 111.413\n",
            "epoch: 187 loss: 137.793 val loss: 114.426\n",
            "epoch: 188 loss: 177.019 val loss: 112.187\n",
            "epoch: 189 loss: 95.418 val loss: 112.967\n",
            "epoch: 190 loss: 159.367 val loss: 114.423\n",
            "epoch: 191 loss: 129.972 val loss: 113.609\n",
            "epoch: 192 loss: 103.003 val loss: 114.010\n",
            "epoch: 193 loss: 93.313 val loss: 111.437\n",
            "epoch: 194 loss: 85.052 val loss: 114.756\n",
            "epoch: 195 loss: 128.628 val loss: 113.683\n",
            "epoch: 196 loss: 142.398 val loss: 112.981\n",
            "epoch: 197 loss: 134.783 val loss: 113.391\n",
            "epoch: 198 loss: 137.990 val loss: 111.123\n",
            "epoch: 199 loss: 82.723 val loss: 111.731\n",
            "epoch: 200 loss: 146.584 val loss: 112.026\n",
            "epoch: 201 loss: 123.513 val loss: 114.506\n",
            "epoch: 202 loss: 155.377 val loss: 110.232\n",
            "epoch: 203 loss: 156.748 val loss: 113.762\n",
            "epoch: 204 loss: 96.544 val loss: 114.450\n",
            "epoch: 205 loss: 113.372 val loss: 112.418\n",
            "epoch: 206 loss: 84.779 val loss: 113.255\n",
            "epoch: 207 loss: 107.082 val loss: 112.140\n",
            "epoch: 208 loss: 144.591 val loss: 114.261\n",
            "epoch: 209 loss: 115.505 val loss: 112.882\n",
            "epoch: 210 loss: 129.239 val loss: 113.526\n",
            "epoch: 211 loss: 103.932 val loss: 112.093\n",
            "epoch: 212 loss: 102.937 val loss: 114.310\n",
            "epoch: 213 loss: 120.485 val loss: 112.098\n",
            "epoch: 214 loss: 148.056 val loss: 115.607\n",
            "epoch: 215 loss: 139.040 val loss: 112.546\n",
            "epoch: 216 loss: 103.257 val loss: 115.482\n",
            "epoch: 217 loss: 95.794 val loss: 112.577\n",
            "epoch: 218 loss: 122.570 val loss: 112.051\n",
            "epoch: 219 loss: 123.814 val loss: 113.134\n",
            "epoch: 220 loss: 114.735 val loss: 113.035\n",
            "epoch: 221 loss: 131.717 val loss: 111.049\n",
            "epoch: 222 loss: 130.471 val loss: 113.379\n",
            "epoch: 223 loss: 105.855 val loss: 113.619\n",
            "epoch: 224 loss: 115.201 val loss: 111.607\n",
            "epoch: 225 loss: 77.869 val loss: 113.350\n",
            "epoch: 226 loss: 93.501 val loss: 114.367\n",
            "epoch: 227 loss: 147.136 val loss: 114.637\n",
            "epoch: 228 loss: 104.397 val loss: 111.485\n",
            "epoch: 229 loss: 109.308 val loss: 113.485\n",
            "epoch: 230 loss: 118.453 val loss: 111.798\n",
            "epoch: 231 loss: 106.464 val loss: 112.660\n",
            "epoch: 232 loss: 85.186 val loss: 113.382\n",
            "epoch: 233 loss: 126.712 val loss: 111.853\n",
            "epoch: 234 loss: 119.165 val loss: 111.512\n",
            "epoch: 235 loss: 153.344 val loss: 113.033\n",
            "epoch: 236 loss: 147.116 val loss: 111.852\n",
            "epoch: 237 loss: 98.722 val loss: 111.173\n",
            "epoch: 238 loss: 108.969 val loss: 112.741\n",
            "epoch: 239 loss: 83.657 val loss: 112.212\n",
            "epoch: 240 loss: 132.739 val loss: 114.127\n",
            "epoch: 241 loss: 124.163 val loss: 113.648\n",
            "epoch: 242 loss: 85.628 val loss: 111.724\n",
            "epoch: 243 loss: 129.889 val loss: 115.690\n",
            "epoch: 244 loss: 115.606 val loss: 111.883\n",
            "epoch: 245 loss: 78.378 val loss: 111.596\n",
            "epoch: 246 loss: 113.127 val loss: 111.232\n",
            "epoch: 247 loss: 162.854 val loss: 114.143\n",
            "epoch: 248 loss: 124.442 val loss: 112.982\n",
            "epoch: 249 loss: 127.039 val loss: 113.948\n",
            "epoch: 250 loss: 92.837 val loss: 113.935\n",
            "epoch: 251 loss: 178.643 val loss: 112.271\n",
            "epoch: 252 loss: 105.470 val loss: 114.359\n",
            "epoch: 253 loss: 174.786 val loss: 113.658\n",
            "epoch: 254 loss: 136.185 val loss: 112.273\n",
            "epoch: 255 loss: 53.907 val loss: 112.511\n",
            "epoch: 256 loss: 112.698 val loss: 113.210\n",
            "epoch: 257 loss: 161.765 val loss: 114.919\n",
            "epoch: 258 loss: 122.642 val loss: 114.813\n",
            "epoch: 259 loss: 69.112 val loss: 115.381\n",
            "epoch: 260 loss: 100.450 val loss: 111.158\n",
            "epoch: 261 loss: 89.501 val loss: 111.528\n",
            "epoch: 262 loss: 138.798 val loss: 113.824\n",
            "epoch: 263 loss: 121.514 val loss: 113.336\n",
            "epoch: 264 loss: 149.103 val loss: 111.146\n",
            "epoch: 265 loss: 106.552 val loss: 111.840\n",
            "epoch: 266 loss: 130.077 val loss: 111.681\n",
            "epoch: 267 loss: 82.717 val loss: 113.830\n",
            "epoch: 268 loss: 105.205 val loss: 111.662\n",
            "epoch: 269 loss: 142.503 val loss: 114.033\n",
            "epoch: 270 loss: 106.929 val loss: 112.600\n",
            "epoch: 271 loss: 101.595 val loss: 113.608\n",
            "epoch: 272 loss: 125.811 val loss: 113.257\n",
            "epoch: 273 loss: 144.274 val loss: 111.137\n",
            "epoch: 274 loss: 80.323 val loss: 112.483\n",
            "epoch: 275 loss: 88.366 val loss: 112.217\n",
            "epoch: 276 loss: 98.611 val loss: 113.435\n",
            "epoch: 277 loss: 128.502 val loss: 112.809\n",
            "epoch: 278 loss: 125.042 val loss: 112.146\n",
            "epoch: 279 loss: 73.494 val loss: 114.081\n",
            "epoch: 280 loss: 85.552 val loss: 111.679\n",
            "epoch: 281 loss: 131.612 val loss: 114.183\n",
            "epoch: 282 loss: 78.901 val loss: 113.331\n",
            "epoch: 283 loss: 100.655 val loss: 111.884\n",
            "epoch: 284 loss: 157.521 val loss: 113.125\n",
            "epoch: 285 loss: 98.271 val loss: 113.203\n",
            "epoch: 286 loss: 86.010 val loss: 115.785\n",
            "epoch: 287 loss: 104.130 val loss: 111.744\n",
            "epoch: 288 loss: 142.170 val loss: 111.617\n",
            "epoch: 289 loss: 81.068 val loss: 112.653\n",
            "epoch: 290 loss: 115.698 val loss: 117.373\n",
            "epoch: 291 loss: 115.311 val loss: 111.829\n",
            "epoch: 292 loss: 127.490 val loss: 114.508\n",
            "epoch: 293 loss: 83.085 val loss: 112.517\n",
            "epoch: 294 loss: 148.105 val loss: 110.360\n",
            "epoch: 295 loss: 87.835 val loss: 112.610\n",
            "epoch: 296 loss: 122.475 val loss: 111.161\n",
            "epoch: 297 loss: 120.817 val loss: 111.427\n",
            "epoch: 298 loss: 96.853 val loss: 115.374\n",
            "epoch: 299 loss: 152.677 val loss: 110.421\n",
            "epoch: 300 loss: 103.029 val loss: 112.961\n",
            "epoch: 301 loss: 115.300 val loss: 111.602\n",
            "epoch: 302 loss: 152.174 val loss: 111.869\n",
            "epoch: 303 loss: 124.078 val loss: 114.829\n",
            "epoch: 304 loss: 94.876 val loss: 113.730\n",
            "epoch: 305 loss: 150.319 val loss: 112.535\n",
            "epoch: 306 loss: 104.509 val loss: 112.731\n",
            "epoch: 307 loss: 98.554 val loss: 110.288\n",
            "epoch: 308 loss: 112.348 val loss: 112.896\n",
            "epoch: 309 loss: 128.005 val loss: 113.157\n",
            "epoch: 310 loss: 132.885 val loss: 115.907\n",
            "epoch: 311 loss: 110.951 val loss: 113.029\n",
            "epoch: 312 loss: 107.385 val loss: 113.911\n",
            "epoch: 313 loss: 125.181 val loss: 111.546\n",
            "epoch: 314 loss: 164.058 val loss: 114.656\n",
            "epoch: 315 loss: 145.414 val loss: 110.462\n",
            "epoch: 316 loss: 136.538 val loss: 113.607\n",
            "epoch: 317 loss: 79.786 val loss: 113.982\n",
            "epoch: 318 loss: 164.454 val loss: 110.902\n",
            "epoch: 319 loss: 85.881 val loss: 112.401\n",
            "epoch: 320 loss: 137.004 val loss: 111.137\n",
            "epoch: 321 loss: 151.524 val loss: 110.923\n",
            "epoch: 322 loss: 129.977 val loss: 112.630\n",
            "epoch: 323 loss: 116.257 val loss: 114.559\n",
            "epoch: 324 loss: 173.118 val loss: 112.766\n",
            "epoch: 325 loss: 194.081 val loss: 112.663\n",
            "epoch: 326 loss: 128.122 val loss: 115.296\n",
            "epoch: 327 loss: 104.235 val loss: 111.087\n",
            "epoch: 328 loss: 86.050 val loss: 113.681\n",
            "epoch: 329 loss: 122.095 val loss: 112.393\n",
            "epoch: 330 loss: 113.233 val loss: 111.630\n",
            "epoch: 331 loss: 84.021 val loss: 111.374\n",
            "epoch: 332 loss: 109.177 val loss: 113.425\n",
            "epoch: 333 loss: 139.015 val loss: 111.597\n",
            "epoch: 334 loss: 132.364 val loss: 111.800\n",
            "epoch: 335 loss: 126.008 val loss: 111.376\n",
            "epoch: 336 loss: 89.675 val loss: 112.314\n",
            "epoch: 337 loss: 87.558 val loss: 112.867\n",
            "epoch: 338 loss: 134.822 val loss: 112.606\n",
            "epoch: 339 loss: 142.208 val loss: 112.449\n",
            "epoch: 340 loss: 111.120 val loss: 113.341\n",
            "epoch: 341 loss: 138.681 val loss: 113.388\n",
            "epoch: 342 loss: 119.408 val loss: 113.186\n",
            "epoch: 343 loss: 100.010 val loss: 111.209\n",
            "epoch: 344 loss: 86.450 val loss: 111.984\n",
            "epoch: 345 loss: 76.138 val loss: 112.062\n",
            "epoch: 346 loss: 100.541 val loss: 111.561\n",
            "epoch: 347 loss: 92.523 val loss: 113.134\n",
            "epoch: 348 loss: 157.776 val loss: 110.145\n",
            "epoch: 349 loss: 92.929 val loss: 112.703\n",
            "epoch: 350 loss: 92.449 val loss: 112.781\n",
            "epoch: 351 loss: 109.122 val loss: 112.551\n",
            "epoch: 352 loss: 164.237 val loss: 112.486\n",
            "epoch: 353 loss: 114.145 val loss: 111.070\n",
            "epoch: 354 loss: 107.775 val loss: 112.435\n",
            "epoch: 355 loss: 111.881 val loss: 115.327\n",
            "epoch: 356 loss: 134.107 val loss: 110.640\n",
            "epoch: 357 loss: 113.858 val loss: 112.172\n",
            "epoch: 358 loss: 99.222 val loss: 111.378\n",
            "epoch: 359 loss: 64.527 val loss: 112.993\n",
            "epoch: 360 loss: 81.539 val loss: 112.974\n",
            "epoch: 361 loss: 136.513 val loss: 112.925\n",
            "epoch: 362 loss: 111.138 val loss: 112.678\n",
            "epoch: 363 loss: 94.263 val loss: 111.547\n",
            "epoch: 364 loss: 113.190 val loss: 112.174\n",
            "epoch: 365 loss: 115.504 val loss: 112.452\n",
            "epoch: 366 loss: 164.822 val loss: 111.570\n",
            "epoch: 367 loss: 118.831 val loss: 112.874\n",
            "epoch: 368 loss: 91.039 val loss: 114.821\n",
            "epoch: 369 loss: 116.238 val loss: 112.456\n",
            "epoch: 370 loss: 110.823 val loss: 114.421\n",
            "epoch: 371 loss: 134.734 val loss: 111.453\n",
            "epoch: 372 loss: 68.000 val loss: 112.547\n",
            "epoch: 373 loss: 78.695 val loss: 112.484\n",
            "epoch: 374 loss: 139.912 val loss: 111.880\n",
            "epoch: 375 loss: 117.699 val loss: 110.500\n",
            "epoch: 376 loss: 131.111 val loss: 112.793\n",
            "epoch: 377 loss: 104.461 val loss: 114.328\n",
            "epoch: 378 loss: 100.828 val loss: 112.967\n",
            "epoch: 379 loss: 90.748 val loss: 112.957\n",
            "epoch: 380 loss: 98.519 val loss: 110.474\n",
            "epoch: 381 loss: 115.246 val loss: 113.525\n",
            "epoch: 382 loss: 130.729 val loss: 114.526\n",
            "epoch: 383 loss: 109.217 val loss: 111.582\n",
            "epoch: 384 loss: 129.762 val loss: 114.614\n",
            "epoch: 385 loss: 133.304 val loss: 113.984\n",
            "epoch: 386 loss: 108.065 val loss: 110.824\n",
            "epoch: 387 loss: 132.850 val loss: 111.683\n",
            "epoch: 388 loss: 125.991 val loss: 111.309\n",
            "epoch: 389 loss: 110.278 val loss: 112.630\n",
            "epoch: 390 loss: 80.317 val loss: 116.009\n",
            "epoch: 391 loss: 113.184 val loss: 112.804\n",
            "epoch: 392 loss: 98.482 val loss: 115.079\n",
            "epoch: 393 loss: 157.281 val loss: 113.448\n",
            "epoch: 394 loss: 133.594 val loss: 112.535\n",
            "epoch: 395 loss: 129.082 val loss: 113.021\n",
            "epoch: 396 loss: 130.728 val loss: 113.037\n",
            "epoch: 397 loss: 116.335 val loss: 114.339\n",
            "epoch: 398 loss: 74.790 val loss: 112.021\n",
            "epoch: 399 loss: 114.359 val loss: 111.310\n",
            "epoch: 400 loss: 126.135 val loss: 110.680\n",
            "epoch: 401 loss: 131.319 val loss: 112.863\n",
            "epoch: 402 loss: 130.335 val loss: 111.461\n",
            "epoch: 403 loss: 112.968 val loss: 115.972\n",
            "epoch: 404 loss: 78.956 val loss: 112.861\n",
            "epoch: 405 loss: 94.581 val loss: 111.819\n",
            "epoch: 406 loss: 97.007 val loss: 111.623\n",
            "epoch: 407 loss: 124.268 val loss: 112.462\n",
            "epoch: 408 loss: 113.249 val loss: 112.833\n",
            "epoch: 409 loss: 124.591 val loss: 112.697\n",
            "epoch: 410 loss: 88.417 val loss: 116.925\n",
            "epoch: 411 loss: 124.300 val loss: 112.440\n",
            "epoch: 412 loss: 118.874 val loss: 116.424\n",
            "epoch: 413 loss: 76.464 val loss: 110.995\n",
            "epoch: 414 loss: 109.612 val loss: 110.875\n",
            "epoch: 415 loss: 112.678 val loss: 112.449\n",
            "epoch: 416 loss: 117.659 val loss: 113.797\n",
            "epoch: 417 loss: 121.978 val loss: 113.364\n",
            "epoch: 418 loss: 108.185 val loss: 113.637\n",
            "epoch: 419 loss: 151.410 val loss: 110.732\n",
            "epoch: 420 loss: 100.679 val loss: 113.558\n",
            "epoch: 421 loss: 162.678 val loss: 111.074\n",
            "epoch: 422 loss: 134.300 val loss: 111.351\n",
            "epoch: 423 loss: 98.509 val loss: 112.825\n",
            "epoch: 424 loss: 83.505 val loss: 112.846\n",
            "epoch: 425 loss: 114.989 val loss: 111.918\n",
            "epoch: 426 loss: 126.998 val loss: 114.026\n",
            "epoch: 427 loss: 112.779 val loss: 113.285\n",
            "epoch: 428 loss: 117.003 val loss: 111.694\n",
            "epoch: 429 loss: 108.302 val loss: 112.130\n",
            "epoch: 430 loss: 85.596 val loss: 112.191\n",
            "epoch: 431 loss: 117.415 val loss: 112.554\n",
            "epoch: 432 loss: 120.716 val loss: 113.504\n",
            "epoch: 433 loss: 124.330 val loss: 115.260\n",
            "epoch: 434 loss: 131.022 val loss: 111.867\n",
            "epoch: 435 loss: 119.033 val loss: 113.532\n",
            "epoch: 436 loss: 122.778 val loss: 110.358\n",
            "epoch: 437 loss: 142.057 val loss: 113.554\n",
            "epoch: 438 loss: 109.368 val loss: 112.349\n",
            "epoch: 439 loss: 133.616 val loss: 110.614\n",
            "epoch: 440 loss: 115.051 val loss: 111.385\n",
            "epoch: 441 loss: 101.046 val loss: 114.643\n",
            "epoch: 442 loss: 103.103 val loss: 114.845\n",
            "epoch: 443 loss: 116.556 val loss: 112.650\n",
            "epoch: 444 loss: 114.359 val loss: 113.991\n",
            "epoch: 445 loss: 116.983 val loss: 111.859\n",
            "epoch: 446 loss: 106.850 val loss: 113.070\n",
            "epoch: 447 loss: 148.052 val loss: 111.457\n",
            "epoch: 448 loss: 108.276 val loss: 110.694\n",
            "epoch: 449 loss: 94.934 val loss: 113.442\n",
            "epoch: 450 loss: 86.634 val loss: 113.595\n",
            "epoch: 451 loss: 86.725 val loss: 110.937\n",
            "epoch: 452 loss: 171.451 val loss: 113.363\n",
            "epoch: 453 loss: 144.047 val loss: 111.446\n",
            "epoch: 454 loss: 92.963 val loss: 116.121\n",
            "epoch: 455 loss: 138.969 val loss: 113.970\n",
            "epoch: 456 loss: 124.837 val loss: 114.995\n",
            "epoch: 457 loss: 91.651 val loss: 114.054\n",
            "epoch: 458 loss: 90.216 val loss: 111.747\n",
            "epoch: 459 loss: 95.646 val loss: 112.808\n",
            "epoch: 460 loss: 94.390 val loss: 111.785\n",
            "epoch: 461 loss: 91.720 val loss: 116.760\n",
            "epoch: 462 loss: 101.607 val loss: 112.529\n",
            "epoch: 463 loss: 131.159 val loss: 114.630\n",
            "epoch: 464 loss: 77.088 val loss: 113.852\n",
            "epoch: 465 loss: 171.039 val loss: 113.534\n",
            "epoch: 466 loss: 127.025 val loss: 110.296\n",
            "epoch: 467 loss: 155.961 val loss: 111.368\n",
            "epoch: 468 loss: 82.764 val loss: 111.934\n",
            "epoch: 469 loss: 132.750 val loss: 112.922\n",
            "epoch: 470 loss: 79.667 val loss: 115.223\n",
            "epoch: 471 loss: 137.771 val loss: 114.028\n",
            "epoch: 472 loss: 118.137 val loss: 112.587\n",
            "epoch: 473 loss: 106.635 val loss: 110.643\n",
            "epoch: 474 loss: 186.975 val loss: 111.677\n",
            "epoch: 475 loss: 125.232 val loss: 111.000\n",
            "epoch: 476 loss: 92.707 val loss: 112.345\n",
            "epoch: 477 loss: 164.336 val loss: 111.521\n",
            "epoch: 478 loss: 129.394 val loss: 114.445\n",
            "epoch: 479 loss: 113.921 val loss: 112.660\n",
            "epoch: 480 loss: 129.154 val loss: 112.888\n",
            "epoch: 481 loss: 112.675 val loss: 112.395\n",
            "epoch: 482 loss: 75.508 val loss: 110.578\n",
            "epoch: 483 loss: 133.638 val loss: 112.014\n",
            "epoch: 484 loss: 95.683 val loss: 115.433\n",
            "epoch: 485 loss: 112.030 val loss: 113.558\n",
            "epoch: 486 loss: 105.383 val loss: 111.153\n",
            "epoch: 487 loss: 138.040 val loss: 111.987\n",
            "epoch: 488 loss: 151.769 val loss: 115.386\n",
            "epoch: 489 loss: 92.295 val loss: 113.743\n",
            "epoch: 490 loss: 99.605 val loss: 110.954\n",
            "epoch: 491 loss: 105.628 val loss: 112.219\n",
            "epoch: 492 loss: 98.860 val loss: 112.590\n",
            "epoch: 493 loss: 125.277 val loss: 114.475\n",
            "epoch: 494 loss: 93.467 val loss: 111.680\n",
            "epoch: 495 loss: 87.109 val loss: 110.850\n",
            "epoch: 496 loss: 111.331 val loss: 116.223\n",
            "epoch: 497 loss: 136.192 val loss: 115.387\n",
            "epoch: 498 loss: 109.655 val loss: 112.798\n",
            "epoch: 499 loss: 91.613 val loss: 112.544\n",
            "epoch: 500 loss: 116.058 val loss: 113.328\n",
            "epoch: 501 loss: 140.710 val loss: 113.128\n",
            "epoch: 502 loss: 172.956 val loss: 111.598\n",
            "epoch: 503 loss: 81.499 val loss: 111.571\n",
            "epoch: 504 loss: 134.754 val loss: 110.841\n",
            "epoch: 505 loss: 128.995 val loss: 112.742\n",
            "epoch: 506 loss: 113.167 val loss: 114.871\n",
            "epoch: 507 loss: 96.406 val loss: 112.390\n",
            "epoch: 508 loss: 106.563 val loss: 114.157\n",
            "epoch: 509 loss: 96.687 val loss: 112.801\n",
            "epoch: 510 loss: 127.708 val loss: 113.115\n",
            "epoch: 511 loss: 101.956 val loss: 113.111\n",
            "epoch: 512 loss: 114.276 val loss: 113.969\n",
            "epoch: 513 loss: 158.141 val loss: 116.910\n",
            "epoch: 514 loss: 159.020 val loss: 112.558\n",
            "epoch: 515 loss: 141.203 val loss: 110.973\n",
            "epoch: 516 loss: 150.937 val loss: 112.553\n",
            "epoch: 517 loss: 158.441 val loss: 114.405\n",
            "epoch: 518 loss: 113.244 val loss: 112.254\n",
            "epoch: 519 loss: 79.223 val loss: 113.443\n",
            "epoch: 520 loss: 118.480 val loss: 110.975\n",
            "epoch: 521 loss: 135.078 val loss: 111.562\n",
            "epoch: 522 loss: 116.502 val loss: 111.921\n",
            "epoch: 523 loss: 73.495 val loss: 111.913\n",
            "epoch: 524 loss: 133.224 val loss: 111.648\n",
            "epoch: 525 loss: 90.530 val loss: 112.251\n",
            "epoch: 526 loss: 98.436 val loss: 115.109\n",
            "epoch: 527 loss: 113.447 val loss: 113.605\n",
            "epoch: 528 loss: 121.025 val loss: 111.987\n",
            "epoch: 529 loss: 70.946 val loss: 113.518\n",
            "epoch: 530 loss: 95.115 val loss: 112.738\n",
            "epoch: 531 loss: 124.036 val loss: 111.082\n",
            "epoch: 532 loss: 135.890 val loss: 112.877\n",
            "epoch: 533 loss: 99.612 val loss: 112.318\n",
            "epoch: 534 loss: 130.292 val loss: 113.931\n",
            "epoch: 535 loss: 124.016 val loss: 112.429\n",
            "epoch: 536 loss: 84.416 val loss: 112.149\n",
            "epoch: 537 loss: 106.761 val loss: 112.432\n",
            "epoch: 538 loss: 77.039 val loss: 112.573\n",
            "epoch: 539 loss: 106.194 val loss: 111.565\n",
            "epoch: 540 loss: 110.488 val loss: 112.646\n",
            "epoch: 541 loss: 90.086 val loss: 111.444\n",
            "epoch: 542 loss: 144.685 val loss: 111.737\n",
            "epoch: 543 loss: 103.739 val loss: 113.069\n",
            "epoch: 544 loss: 111.552 val loss: 112.554\n",
            "epoch: 545 loss: 126.094 val loss: 113.058\n",
            "epoch: 546 loss: 126.532 val loss: 115.343\n",
            "epoch: 547 loss: 103.342 val loss: 112.226\n",
            "epoch: 548 loss: 83.220 val loss: 115.794\n",
            "epoch: 549 loss: 88.493 val loss: 113.482\n",
            "epoch: 550 loss: 102.148 val loss: 113.373\n",
            "epoch: 551 loss: 126.899 val loss: 114.653\n",
            "epoch: 552 loss: 131.868 val loss: 112.695\n",
            "epoch: 553 loss: 94.468 val loss: 112.736\n",
            "epoch: 554 loss: 138.863 val loss: 112.557\n",
            "epoch: 555 loss: 104.429 val loss: 112.798\n",
            "epoch: 556 loss: 80.295 val loss: 116.692\n",
            "epoch: 557 loss: 108.758 val loss: 112.033\n",
            "epoch: 558 loss: 98.235 val loss: 112.315\n",
            "epoch: 559 loss: 134.844 val loss: 112.634\n",
            "epoch: 560 loss: 136.405 val loss: 113.069\n",
            "epoch: 561 loss: 150.259 val loss: 112.268\n",
            "epoch: 562 loss: 122.797 val loss: 113.800\n",
            "epoch: 563 loss: 122.201 val loss: 112.665\n",
            "epoch: 564 loss: 113.985 val loss: 112.437\n",
            "epoch: 565 loss: 95.793 val loss: 114.112\n",
            "epoch: 566 loss: 114.959 val loss: 113.814\n",
            "epoch: 567 loss: 97.232 val loss: 115.455\n",
            "epoch: 568 loss: 105.831 val loss: 113.849\n",
            "epoch: 569 loss: 78.694 val loss: 114.705\n",
            "epoch: 570 loss: 93.033 val loss: 115.028\n",
            "epoch: 571 loss: 93.832 val loss: 111.651\n",
            "epoch: 572 loss: 99.421 val loss: 114.981\n",
            "epoch: 573 loss: 98.797 val loss: 113.266\n",
            "epoch: 574 loss: 96.227 val loss: 111.438\n",
            "epoch: 575 loss: 161.785 val loss: 111.998\n",
            "epoch: 576 loss: 104.146 val loss: 112.901\n",
            "epoch: 577 loss: 126.548 val loss: 115.566\n",
            "epoch: 578 loss: 94.078 val loss: 113.731\n",
            "epoch: 579 loss: 149.989 val loss: 114.255\n",
            "epoch: 580 loss: 84.333 val loss: 111.633\n",
            "epoch: 581 loss: 116.124 val loss: 111.708\n",
            "epoch: 582 loss: 123.067 val loss: 113.398\n",
            "epoch: 583 loss: 140.084 val loss: 113.425\n",
            "epoch: 584 loss: 154.795 val loss: 114.634\n",
            "epoch: 585 loss: 123.806 val loss: 113.986\n",
            "epoch: 586 loss: 123.649 val loss: 113.182\n",
            "epoch: 587 loss: 105.560 val loss: 112.572\n",
            "epoch: 588 loss: 141.388 val loss: 111.632\n",
            "epoch: 589 loss: 117.961 val loss: 110.800\n",
            "epoch: 590 loss: 99.486 val loss: 114.189\n",
            "epoch: 591 loss: 136.543 val loss: 113.929\n",
            "epoch: 592 loss: 119.701 val loss: 114.854\n",
            "epoch: 593 loss: 127.158 val loss: 113.397\n",
            "epoch: 594 loss: 107.383 val loss: 111.060\n",
            "epoch: 595 loss: 90.908 val loss: 111.302\n",
            "epoch: 596 loss: 134.673 val loss: 112.410\n",
            "epoch: 597 loss: 107.755 val loss: 114.743\n",
            "epoch: 598 loss: 104.131 val loss: 114.522\n",
            "epoch: 599 loss: 99.182 val loss: 113.208\n",
            "epoch: 600 loss: 120.277 val loss: 111.695\n",
            "epoch: 601 loss: 100.037 val loss: 112.764\n",
            "epoch: 602 loss: 104.987 val loss: 110.559\n",
            "epoch: 603 loss: 104.590 val loss: 111.569\n",
            "epoch: 604 loss: 142.123 val loss: 112.976\n",
            "epoch: 605 loss: 93.882 val loss: 113.024\n",
            "epoch: 606 loss: 134.526 val loss: 113.034\n",
            "epoch: 607 loss: 91.005 val loss: 113.426\n",
            "epoch: 608 loss: 84.908 val loss: 111.413\n",
            "epoch: 609 loss: 111.507 val loss: 111.488\n",
            "epoch: 610 loss: 85.041 val loss: 112.473\n",
            "epoch: 611 loss: 77.207 val loss: 112.475\n",
            "epoch: 612 loss: 97.400 val loss: 111.693\n",
            "epoch: 613 loss: 108.801 val loss: 112.080\n",
            "epoch: 614 loss: 109.614 val loss: 111.448\n",
            "epoch: 615 loss: 77.326 val loss: 112.662\n",
            "epoch: 616 loss: 114.278 val loss: 112.034\n",
            "epoch: 617 loss: 84.386 val loss: 114.332\n",
            "epoch: 618 loss: 107.794 val loss: 110.816\n",
            "epoch: 619 loss: 111.238 val loss: 111.095\n",
            "epoch: 620 loss: 135.138 val loss: 112.747\n",
            "epoch: 621 loss: 108.545 val loss: 113.221\n",
            "epoch: 622 loss: 129.550 val loss: 112.249\n",
            "epoch: 623 loss: 129.620 val loss: 113.394\n",
            "epoch: 624 loss: 127.278 val loss: 115.174\n",
            "epoch: 625 loss: 124.195 val loss: 112.077\n",
            "epoch: 626 loss: 82.723 val loss: 112.723\n",
            "epoch: 627 loss: 115.248 val loss: 113.063\n",
            "epoch: 628 loss: 71.081 val loss: 114.684\n",
            "epoch: 629 loss: 99.319 val loss: 111.674\n",
            "epoch: 630 loss: 104.822 val loss: 113.669\n",
            "epoch: 631 loss: 97.993 val loss: 113.595\n",
            "epoch: 632 loss: 145.957 val loss: 112.863\n",
            "epoch: 633 loss: 145.385 val loss: 112.782\n",
            "epoch: 634 loss: 132.746 val loss: 110.838\n",
            "epoch: 635 loss: 93.548 val loss: 111.916\n",
            "epoch: 636 loss: 93.013 val loss: 112.114\n",
            "epoch: 637 loss: 149.734 val loss: 114.797\n",
            "epoch: 638 loss: 108.770 val loss: 110.606\n",
            "epoch: 639 loss: 79.807 val loss: 113.198\n",
            "epoch: 640 loss: 155.839 val loss: 109.726\n",
            "epoch: 641 loss: 123.351 val loss: 110.055\n",
            "epoch: 642 loss: 143.830 val loss: 111.866\n",
            "epoch: 643 loss: 78.069 val loss: 112.718\n",
            "epoch: 644 loss: 109.026 val loss: 111.414\n",
            "epoch: 645 loss: 110.039 val loss: 114.441\n",
            "epoch: 646 loss: 144.764 val loss: 112.962\n",
            "epoch: 647 loss: 92.251 val loss: 114.907\n",
            "epoch: 648 loss: 100.225 val loss: 112.926\n",
            "epoch: 649 loss: 82.307 val loss: 112.937\n",
            "epoch: 650 loss: 134.736 val loss: 113.198\n",
            "epoch: 651 loss: 116.347 val loss: 112.961\n",
            "epoch: 652 loss: 99.758 val loss: 112.085\n",
            "epoch: 653 loss: 78.828 val loss: 113.435\n",
            "epoch: 654 loss: 153.296 val loss: 112.872\n",
            "epoch: 655 loss: 137.442 val loss: 111.520\n",
            "epoch: 656 loss: 95.166 val loss: 113.177\n",
            "epoch: 657 loss: 121.873 val loss: 115.570\n",
            "epoch: 658 loss: 95.791 val loss: 113.987\n",
            "epoch: 659 loss: 96.104 val loss: 113.455\n",
            "epoch: 660 loss: 103.161 val loss: 113.737\n",
            "epoch: 661 loss: 105.430 val loss: 111.663\n",
            "epoch: 662 loss: 92.128 val loss: 113.639\n",
            "epoch: 663 loss: 117.362 val loss: 110.404\n",
            "epoch: 664 loss: 112.543 val loss: 112.079\n",
            "epoch: 665 loss: 117.124 val loss: 112.836\n",
            "epoch: 666 loss: 139.878 val loss: 111.957\n",
            "epoch: 667 loss: 104.878 val loss: 111.769\n",
            "epoch: 668 loss: 116.919 val loss: 111.485\n",
            "epoch: 669 loss: 98.007 val loss: 114.879\n",
            "epoch: 670 loss: 131.805 val loss: 114.068\n",
            "epoch: 671 loss: 87.297 val loss: 110.588\n",
            "epoch: 672 loss: 104.918 val loss: 111.520\n",
            "epoch: 673 loss: 96.546 val loss: 112.323\n",
            "epoch: 674 loss: 93.971 val loss: 114.390\n",
            "epoch: 675 loss: 116.626 val loss: 113.835\n",
            "epoch: 676 loss: 112.010 val loss: 114.133\n",
            "epoch: 677 loss: 139.537 val loss: 111.341\n",
            "epoch: 678 loss: 160.636 val loss: 111.937\n",
            "epoch: 679 loss: 92.590 val loss: 111.046\n",
            "epoch: 680 loss: 98.170 val loss: 111.992\n",
            "epoch: 681 loss: 118.489 val loss: 113.368\n",
            "epoch: 682 loss: 103.109 val loss: 114.216\n",
            "epoch: 683 loss: 115.932 val loss: 112.091\n",
            "epoch: 684 loss: 97.446 val loss: 114.147\n",
            "epoch: 685 loss: 174.817 val loss: 113.639\n",
            "epoch: 686 loss: 127.190 val loss: 115.388\n",
            "epoch: 687 loss: 91.910 val loss: 111.948\n",
            "epoch: 688 loss: 106.748 val loss: 112.955\n",
            "epoch: 689 loss: 143.425 val loss: 112.510\n",
            "epoch: 690 loss: 112.349 val loss: 112.088\n",
            "epoch: 691 loss: 149.587 val loss: 113.337\n",
            "epoch: 692 loss: 105.162 val loss: 113.373\n",
            "epoch: 693 loss: 104.692 val loss: 114.862\n",
            "epoch: 694 loss: 104.454 val loss: 114.763\n",
            "epoch: 695 loss: 151.785 val loss: 113.668\n",
            "epoch: 696 loss: 95.015 val loss: 114.530\n",
            "epoch: 697 loss: 88.016 val loss: 112.715\n",
            "epoch: 698 loss: 91.511 val loss: 112.237\n",
            "epoch: 699 loss: 132.183 val loss: 112.941\n",
            "epoch: 700 loss: 92.555 val loss: 113.481\n",
            "epoch: 701 loss: 71.375 val loss: 113.138\n",
            "epoch: 702 loss: 138.324 val loss: 113.279\n",
            "epoch: 703 loss: 113.789 val loss: 115.926\n",
            "epoch: 704 loss: 89.051 val loss: 112.540\n",
            "epoch: 705 loss: 101.737 val loss: 111.153\n",
            "epoch: 706 loss: 110.235 val loss: 115.142\n",
            "epoch: 707 loss: 68.471 val loss: 112.344\n",
            "epoch: 708 loss: 103.998 val loss: 114.180\n",
            "epoch: 709 loss: 117.000 val loss: 112.101\n",
            "epoch: 710 loss: 92.491 val loss: 111.526\n",
            "epoch: 711 loss: 143.764 val loss: 113.821\n",
            "epoch: 712 loss: 123.330 val loss: 115.451\n",
            "epoch: 713 loss: 142.295 val loss: 111.533\n",
            "epoch: 714 loss: 116.492 val loss: 113.109\n",
            "epoch: 715 loss: 100.251 val loss: 112.155\n",
            "epoch: 716 loss: 141.790 val loss: 112.175\n",
            "epoch: 717 loss: 139.057 val loss: 114.950\n",
            "epoch: 718 loss: 115.607 val loss: 114.148\n",
            "epoch: 719 loss: 106.723 val loss: 112.100\n",
            "epoch: 720 loss: 143.066 val loss: 111.582\n",
            "epoch: 721 loss: 94.233 val loss: 112.656\n",
            "epoch: 722 loss: 77.754 val loss: 113.440\n",
            "epoch: 723 loss: 87.095 val loss: 113.648\n",
            "epoch: 724 loss: 111.200 val loss: 115.031\n",
            "epoch: 725 loss: 97.781 val loss: 111.350\n",
            "epoch: 726 loss: 104.262 val loss: 110.518\n",
            "epoch: 727 loss: 113.983 val loss: 111.384\n",
            "epoch: 728 loss: 87.693 val loss: 112.682\n",
            "epoch: 729 loss: 122.815 val loss: 115.336\n",
            "epoch: 730 loss: 125.097 val loss: 111.477\n",
            "epoch: 731 loss: 105.286 val loss: 112.238\n",
            "epoch: 732 loss: 83.341 val loss: 110.625\n",
            "epoch: 733 loss: 82.380 val loss: 111.838\n",
            "epoch: 734 loss: 148.791 val loss: 112.878\n",
            "epoch: 735 loss: 94.950 val loss: 114.299\n",
            "epoch: 736 loss: 131.517 val loss: 113.302\n",
            "epoch: 737 loss: 112.962 val loss: 112.089\n",
            "epoch: 738 loss: 77.132 val loss: 115.177\n",
            "epoch: 739 loss: 128.888 val loss: 112.815\n",
            "epoch: 740 loss: 117.274 val loss: 112.709\n",
            "epoch: 741 loss: 101.421 val loss: 113.226\n",
            "epoch: 742 loss: 128.276 val loss: 114.009\n",
            "epoch: 743 loss: 109.793 val loss: 110.601\n",
            "epoch: 744 loss: 121.311 val loss: 112.287\n",
            "epoch: 745 loss: 117.312 val loss: 111.435\n",
            "epoch: 746 loss: 143.163 val loss: 112.212\n",
            "epoch: 747 loss: 68.637 val loss: 111.511\n",
            "epoch: 748 loss: 108.530 val loss: 113.210\n",
            "epoch: 749 loss: 83.221 val loss: 110.156\n",
            "epoch: 750 loss: 103.298 val loss: 113.638\n",
            "epoch: 751 loss: 105.626 val loss: 113.244\n",
            "epoch: 752 loss: 93.139 val loss: 113.061\n",
            "epoch: 753 loss: 116.785 val loss: 113.906\n",
            "epoch: 754 loss: 149.702 val loss: 112.828\n",
            "epoch: 755 loss: 82.759 val loss: 113.672\n",
            "epoch: 756 loss: 123.704 val loss: 110.243\n",
            "epoch: 757 loss: 129.311 val loss: 113.101\n",
            "epoch: 758 loss: 79.713 val loss: 113.185\n",
            "epoch: 759 loss: 102.714 val loss: 114.730\n",
            "epoch: 760 loss: 116.068 val loss: 112.615\n",
            "epoch: 761 loss: 72.941 val loss: 113.553\n",
            "epoch: 762 loss: 108.384 val loss: 114.054\n",
            "epoch: 763 loss: 146.464 val loss: 112.622\n",
            "epoch: 764 loss: 66.828 val loss: 114.598\n",
            "epoch: 765 loss: 123.228 val loss: 112.825\n",
            "epoch: 766 loss: 121.467 val loss: 112.133\n",
            "epoch: 767 loss: 129.664 val loss: 113.411\n",
            "epoch: 768 loss: 93.829 val loss: 110.845\n",
            "epoch: 769 loss: 112.474 val loss: 112.593\n",
            "epoch: 770 loss: 83.230 val loss: 112.066\n",
            "epoch: 771 loss: 122.694 val loss: 112.051\n",
            "epoch: 772 loss: 100.420 val loss: 112.431\n",
            "epoch: 773 loss: 79.043 val loss: 111.764\n",
            "epoch: 774 loss: 130.182 val loss: 114.127\n",
            "epoch: 775 loss: 145.593 val loss: 112.170\n",
            "epoch: 776 loss: 118.491 val loss: 113.072\n",
            "epoch: 777 loss: 87.379 val loss: 112.684\n",
            "epoch: 778 loss: 130.532 val loss: 113.882\n",
            "epoch: 779 loss: 117.388 val loss: 112.674\n",
            "epoch: 780 loss: 159.756 val loss: 111.290\n",
            "epoch: 781 loss: 74.643 val loss: 112.521\n",
            "epoch: 782 loss: 110.751 val loss: 114.017\n",
            "epoch: 783 loss: 139.581 val loss: 113.657\n",
            "epoch: 784 loss: 150.722 val loss: 111.892\n",
            "epoch: 785 loss: 128.682 val loss: 109.829\n",
            "epoch: 786 loss: 89.658 val loss: 114.033\n",
            "epoch: 787 loss: 109.283 val loss: 114.580\n",
            "epoch: 788 loss: 71.157 val loss: 112.256\n",
            "epoch: 789 loss: 111.215 val loss: 111.917\n",
            "epoch: 790 loss: 83.025 val loss: 112.290\n",
            "epoch: 791 loss: 142.399 val loss: 110.980\n",
            "epoch: 792 loss: 152.350 val loss: 111.116\n",
            "epoch: 793 loss: 121.995 val loss: 112.640\n",
            "epoch: 794 loss: 121.722 val loss: 112.507\n",
            "epoch: 795 loss: 107.056 val loss: 112.982\n",
            "epoch: 796 loss: 115.230 val loss: 111.621\n",
            "epoch: 797 loss: 90.482 val loss: 112.660\n",
            "epoch: 798 loss: 121.082 val loss: 112.266\n",
            "epoch: 799 loss: 113.921 val loss: 112.801\n",
            "epoch: 800 loss: 119.343 val loss: 111.447\n",
            "epoch: 801 loss: 77.397 val loss: 110.219\n",
            "epoch: 802 loss: 113.903 val loss: 113.033\n",
            "epoch: 803 loss: 105.259 val loss: 114.567\n",
            "epoch: 804 loss: 118.366 val loss: 113.877\n",
            "epoch: 805 loss: 138.916 val loss: 112.376\n",
            "epoch: 806 loss: 103.156 val loss: 111.617\n",
            "epoch: 807 loss: 137.605 val loss: 111.746\n",
            "epoch: 808 loss: 127.809 val loss: 111.482\n",
            "epoch: 809 loss: 169.248 val loss: 112.137\n",
            "epoch: 810 loss: 110.760 val loss: 111.693\n",
            "epoch: 811 loss: 134.700 val loss: 114.650\n",
            "epoch: 812 loss: 111.697 val loss: 110.953\n",
            "epoch: 813 loss: 84.804 val loss: 112.104\n",
            "epoch: 814 loss: 116.499 val loss: 111.085\n",
            "epoch: 815 loss: 112.978 val loss: 113.101\n",
            "epoch: 816 loss: 113.255 val loss: 112.555\n",
            "epoch: 817 loss: 101.688 val loss: 112.162\n",
            "epoch: 818 loss: 155.234 val loss: 111.517\n",
            "epoch: 819 loss: 104.188 val loss: 111.555\n",
            "epoch: 820 loss: 145.280 val loss: 112.901\n",
            "epoch: 821 loss: 115.983 val loss: 111.080\n",
            "epoch: 822 loss: 124.530 val loss: 115.136\n",
            "epoch: 823 loss: 114.116 val loss: 113.813\n",
            "epoch: 824 loss: 124.494 val loss: 113.682\n",
            "epoch: 825 loss: 107.272 val loss: 114.894\n",
            "epoch: 826 loss: 126.992 val loss: 115.687\n",
            "epoch: 827 loss: 122.633 val loss: 112.707\n",
            "epoch: 828 loss: 153.282 val loss: 116.006\n",
            "epoch: 829 loss: 109.899 val loss: 113.706\n",
            "epoch: 830 loss: 99.198 val loss: 110.776\n",
            "epoch: 831 loss: 104.508 val loss: 113.343\n",
            "epoch: 832 loss: 78.694 val loss: 111.585\n",
            "epoch: 833 loss: 111.954 val loss: 115.399\n",
            "epoch: 834 loss: 129.118 val loss: 112.396\n",
            "epoch: 835 loss: 121.204 val loss: 112.494\n",
            "epoch: 836 loss: 122.819 val loss: 114.313\n",
            "epoch: 837 loss: 101.714 val loss: 111.859\n",
            "epoch: 838 loss: 123.820 val loss: 111.987\n",
            "epoch: 839 loss: 111.806 val loss: 113.712\n",
            "epoch: 840 loss: 97.398 val loss: 112.659\n",
            "epoch: 841 loss: 125.149 val loss: 112.559\n",
            "epoch: 842 loss: 126.633 val loss: 112.827\n",
            "epoch: 843 loss: 76.413 val loss: 111.141\n",
            "epoch: 844 loss: 110.396 val loss: 115.246\n",
            "epoch: 845 loss: 138.025 val loss: 111.782\n",
            "epoch: 846 loss: 125.230 val loss: 113.352\n",
            "epoch: 847 loss: 69.012 val loss: 111.148\n",
            "epoch: 848 loss: 127.463 val loss: 111.122\n",
            "epoch: 849 loss: 87.522 val loss: 113.998\n",
            "epoch: 850 loss: 82.926 val loss: 114.230\n",
            "epoch: 851 loss: 141.705 val loss: 114.185\n",
            "epoch: 852 loss: 77.367 val loss: 115.395\n",
            "epoch: 853 loss: 119.807 val loss: 112.455\n",
            "epoch: 854 loss: 120.178 val loss: 114.603\n",
            "epoch: 855 loss: 100.322 val loss: 114.703\n",
            "epoch: 856 loss: 135.173 val loss: 112.346\n",
            "epoch: 857 loss: 119.682 val loss: 111.691\n",
            "epoch: 858 loss: 114.226 val loss: 113.961\n",
            "epoch: 859 loss: 147.296 val loss: 111.837\n",
            "epoch: 860 loss: 90.665 val loss: 113.608\n",
            "epoch: 861 loss: 133.415 val loss: 113.209\n",
            "epoch: 862 loss: 72.945 val loss: 111.824\n",
            "epoch: 863 loss: 94.146 val loss: 114.230\n",
            "epoch: 864 loss: 146.083 val loss: 110.881\n",
            "epoch: 865 loss: 123.035 val loss: 114.776\n",
            "epoch: 866 loss: 165.411 val loss: 113.334\n",
            "epoch: 867 loss: 155.596 val loss: 111.050\n",
            "epoch: 868 loss: 101.042 val loss: 111.378\n",
            "epoch: 869 loss: 116.047 val loss: 112.221\n",
            "epoch: 870 loss: 86.538 val loss: 112.418\n",
            "epoch: 871 loss: 94.677 val loss: 112.966\n",
            "epoch: 872 loss: 131.002 val loss: 111.073\n",
            "epoch: 873 loss: 111.353 val loss: 111.999\n",
            "epoch: 874 loss: 150.899 val loss: 113.752\n",
            "epoch: 875 loss: 96.238 val loss: 113.644\n",
            "epoch: 876 loss: 113.955 val loss: 113.360\n",
            "epoch: 877 loss: 127.519 val loss: 112.964\n",
            "epoch: 878 loss: 102.730 val loss: 110.590\n",
            "epoch: 879 loss: 118.617 val loss: 113.938\n",
            "epoch: 880 loss: 124.605 val loss: 112.396\n",
            "epoch: 881 loss: 62.017 val loss: 112.849\n",
            "epoch: 882 loss: 83.585 val loss: 112.702\n",
            "epoch: 883 loss: 93.208 val loss: 112.272\n",
            "epoch: 884 loss: 90.203 val loss: 114.931\n",
            "epoch: 885 loss: 99.028 val loss: 113.505\n",
            "epoch: 886 loss: 122.632 val loss: 112.610\n",
            "epoch: 887 loss: 84.296 val loss: 114.131\n",
            "epoch: 888 loss: 120.666 val loss: 113.772\n",
            "epoch: 889 loss: 127.183 val loss: 112.296\n",
            "epoch: 890 loss: 134.862 val loss: 111.572\n",
            "epoch: 891 loss: 135.449 val loss: 114.700\n",
            "epoch: 892 loss: 129.566 val loss: 112.540\n",
            "epoch: 893 loss: 149.436 val loss: 110.784\n",
            "epoch: 894 loss: 113.230 val loss: 112.551\n",
            "epoch: 895 loss: 107.970 val loss: 112.732\n",
            "epoch: 896 loss: 84.563 val loss: 112.232\n",
            "epoch: 897 loss: 144.254 val loss: 112.263\n",
            "epoch: 898 loss: 106.683 val loss: 111.626\n",
            "epoch: 899 loss: 119.958 val loss: 112.781\n",
            "epoch: 900 loss: 120.850 val loss: 114.885\n",
            "epoch: 901 loss: 74.002 val loss: 115.098\n",
            "epoch: 902 loss: 67.184 val loss: 112.525\n",
            "epoch: 903 loss: 125.442 val loss: 112.481\n",
            "epoch: 904 loss: 121.863 val loss: 110.047\n",
            "epoch: 905 loss: 109.158 val loss: 112.863\n",
            "epoch: 906 loss: 122.402 val loss: 114.434\n",
            "epoch: 907 loss: 113.490 val loss: 114.881\n",
            "epoch: 908 loss: 94.304 val loss: 113.641\n",
            "epoch: 909 loss: 91.217 val loss: 113.232\n",
            "epoch: 910 loss: 136.697 val loss: 112.295\n",
            "epoch: 911 loss: 53.827 val loss: 113.544\n",
            "epoch: 912 loss: 98.665 val loss: 112.311\n",
            "epoch: 913 loss: 102.684 val loss: 111.805\n",
            "epoch: 914 loss: 82.928 val loss: 111.254\n",
            "epoch: 915 loss: 109.302 val loss: 113.118\n",
            "epoch: 916 loss: 95.331 val loss: 111.047\n",
            "epoch: 917 loss: 90.604 val loss: 112.000\n",
            "epoch: 918 loss: 126.723 val loss: 111.894\n",
            "epoch: 919 loss: 86.305 val loss: 114.272\n",
            "epoch: 920 loss: 106.581 val loss: 112.966\n",
            "epoch: 921 loss: 155.533 val loss: 116.610\n",
            "epoch: 922 loss: 76.570 val loss: 113.014\n",
            "epoch: 923 loss: 115.584 val loss: 114.710\n",
            "epoch: 924 loss: 123.710 val loss: 111.076\n",
            "epoch: 925 loss: 107.157 val loss: 113.511\n",
            "epoch: 926 loss: 99.841 val loss: 112.143\n",
            "epoch: 927 loss: 114.909 val loss: 113.227\n",
            "epoch: 928 loss: 108.077 val loss: 111.720\n",
            "epoch: 929 loss: 95.510 val loss: 110.842\n",
            "epoch: 930 loss: 111.085 val loss: 112.563\n",
            "epoch: 931 loss: 103.638 val loss: 111.871\n",
            "epoch: 932 loss: 81.344 val loss: 112.146\n",
            "epoch: 933 loss: 130.977 val loss: 112.392\n",
            "epoch: 934 loss: 89.870 val loss: 113.253\n",
            "epoch: 935 loss: 122.356 val loss: 115.860\n",
            "epoch: 936 loss: 149.255 val loss: 112.503\n",
            "epoch: 937 loss: 131.790 val loss: 113.583\n",
            "epoch: 938 loss: 125.462 val loss: 112.943\n",
            "epoch: 939 loss: 95.180 val loss: 110.756\n",
            "epoch: 940 loss: 108.430 val loss: 111.836\n",
            "epoch: 941 loss: 110.052 val loss: 115.582\n",
            "epoch: 942 loss: 80.678 val loss: 112.485\n",
            "epoch: 943 loss: 146.498 val loss: 113.702\n",
            "epoch: 944 loss: 80.026 val loss: 112.764\n",
            "epoch: 945 loss: 96.316 val loss: 112.090\n",
            "epoch: 946 loss: 129.222 val loss: 111.647\n",
            "epoch: 947 loss: 98.145 val loss: 111.888\n",
            "epoch: 948 loss: 102.228 val loss: 110.701\n",
            "epoch: 949 loss: 136.003 val loss: 113.077\n",
            "epoch: 950 loss: 170.453 val loss: 112.913\n",
            "epoch: 951 loss: 119.910 val loss: 111.926\n",
            "epoch: 952 loss: 104.599 val loss: 111.952\n",
            "epoch: 953 loss: 102.307 val loss: 111.664\n",
            "epoch: 954 loss: 103.385 val loss: 109.968\n",
            "epoch: 955 loss: 79.936 val loss: 112.481\n",
            "epoch: 956 loss: 139.830 val loss: 112.009\n",
            "epoch: 957 loss: 124.846 val loss: 113.144\n",
            "epoch: 958 loss: 120.637 val loss: 113.062\n",
            "epoch: 959 loss: 115.856 val loss: 112.473\n",
            "epoch: 960 loss: 135.282 val loss: 112.815\n",
            "epoch: 961 loss: 112.239 val loss: 114.878\n",
            "epoch: 962 loss: 150.758 val loss: 112.133\n",
            "epoch: 963 loss: 111.754 val loss: 112.968\n",
            "epoch: 964 loss: 144.090 val loss: 111.564\n",
            "epoch: 965 loss: 119.589 val loss: 110.892\n",
            "epoch: 966 loss: 151.374 val loss: 112.397\n",
            "epoch: 967 loss: 109.946 val loss: 112.980\n",
            "epoch: 968 loss: 103.590 val loss: 111.888\n",
            "epoch: 969 loss: 92.153 val loss: 112.994\n",
            "epoch: 970 loss: 124.311 val loss: 111.904\n",
            "epoch: 971 loss: 115.338 val loss: 113.231\n",
            "epoch: 972 loss: 89.395 val loss: 113.005\n",
            "epoch: 973 loss: 105.406 val loss: 111.114\n",
            "epoch: 974 loss: 91.581 val loss: 114.588\n",
            "epoch: 975 loss: 103.460 val loss: 114.942\n",
            "epoch: 976 loss: 108.568 val loss: 111.686\n",
            "epoch: 977 loss: 112.451 val loss: 112.118\n",
            "epoch: 978 loss: 108.549 val loss: 111.146\n",
            "epoch: 979 loss: 83.905 val loss: 112.551\n",
            "epoch: 980 loss: 113.026 val loss: 110.936\n",
            "epoch: 981 loss: 94.899 val loss: 111.606\n",
            "epoch: 982 loss: 122.020 val loss: 112.182\n",
            "epoch: 983 loss: 102.810 val loss: 114.065\n",
            "epoch: 984 loss: 72.293 val loss: 111.668\n",
            "epoch: 985 loss: 112.177 val loss: 113.627\n",
            "epoch: 986 loss: 80.262 val loss: 112.293\n",
            "epoch: 987 loss: 130.474 val loss: 110.882\n",
            "epoch: 988 loss: 98.090 val loss: 112.611\n",
            "epoch: 989 loss: 99.576 val loss: 112.820\n",
            "epoch: 990 loss: 129.656 val loss: 112.399\n",
            "epoch: 991 loss: 128.087 val loss: 113.832\n",
            "epoch: 992 loss: 114.755 val loss: 111.093\n",
            "epoch: 993 loss: 102.987 val loss: 112.383\n",
            "epoch: 994 loss: 116.092 val loss: 112.815\n",
            "epoch: 995 loss: 120.479 val loss: 112.654\n",
            "epoch: 996 loss: 121.246 val loss: 113.095\n",
            "epoch: 997 loss: 88.244 val loss: 112.739\n",
            "epoch: 998 loss: 90.088 val loss: 114.201\n",
            "epoch: 999 loss: 106.717 val loss: 112.044\n",
            "epoch: 1000 loss: 164.996 val loss: 114.277\n",
            "epoch: 1001 loss: 96.895 val loss: 112.572\n",
            "epoch: 1002 loss: 106.331 val loss: 111.147\n",
            "epoch: 1003 loss: 94.860 val loss: 111.634\n",
            "epoch: 1004 loss: 151.359 val loss: 112.350\n",
            "epoch: 1005 loss: 83.844 val loss: 112.069\n",
            "epoch: 1006 loss: 117.066 val loss: 110.971\n",
            "epoch: 1007 loss: 90.714 val loss: 110.750\n",
            "epoch: 1008 loss: 119.468 val loss: 113.853\n",
            "epoch: 1009 loss: 104.107 val loss: 111.262\n",
            "epoch: 1010 loss: 188.790 val loss: 115.408\n",
            "epoch: 1011 loss: 160.348 val loss: 113.412\n",
            "epoch: 1012 loss: 156.273 val loss: 112.598\n",
            "epoch: 1013 loss: 85.581 val loss: 112.781\n",
            "epoch: 1014 loss: 96.104 val loss: 112.225\n",
            "epoch: 1015 loss: 116.334 val loss: 113.168\n",
            "epoch: 1016 loss: 105.467 val loss: 113.227\n",
            "epoch: 1017 loss: 95.558 val loss: 111.145\n",
            "epoch: 1018 loss: 104.517 val loss: 113.684\n",
            "epoch: 1019 loss: 128.810 val loss: 112.656\n",
            "epoch: 1020 loss: 83.668 val loss: 116.305\n",
            "epoch: 1021 loss: 132.373 val loss: 113.566\n",
            "epoch: 1022 loss: 107.652 val loss: 114.067\n",
            "epoch: 1023 loss: 135.875 val loss: 112.211\n",
            "epoch: 1024 loss: 118.009 val loss: 112.880\n",
            "epoch: 1025 loss: 99.136 val loss: 112.363\n",
            "epoch: 1026 loss: 134.947 val loss: 114.624\n",
            "epoch: 1027 loss: 134.954 val loss: 111.762\n",
            "epoch: 1028 loss: 129.205 val loss: 112.189\n",
            "epoch: 1029 loss: 110.543 val loss: 114.573\n",
            "epoch: 1030 loss: 117.772 val loss: 113.430\n",
            "epoch: 1031 loss: 92.372 val loss: 114.228\n",
            "epoch: 1032 loss: 71.533 val loss: 112.916\n",
            "epoch: 1033 loss: 155.208 val loss: 111.109\n",
            "epoch: 1034 loss: 110.045 val loss: 111.388\n",
            "epoch: 1035 loss: 124.316 val loss: 112.093\n",
            "epoch: 1036 loss: 98.503 val loss: 113.058\n",
            "epoch: 1037 loss: 106.716 val loss: 111.878\n",
            "epoch: 1038 loss: 108.548 val loss: 114.715\n",
            "epoch: 1039 loss: 100.961 val loss: 113.646\n",
            "epoch: 1040 loss: 89.461 val loss: 111.338\n",
            "epoch: 1041 loss: 108.018 val loss: 112.341\n",
            "epoch: 1042 loss: 137.817 val loss: 112.445\n",
            "epoch: 1043 loss: 101.179 val loss: 111.384\n",
            "epoch: 1044 loss: 92.098 val loss: 112.066\n",
            "epoch: 1045 loss: 113.978 val loss: 115.922\n",
            "epoch: 1046 loss: 153.905 val loss: 113.138\n",
            "epoch: 1047 loss: 121.346 val loss: 113.618\n",
            "epoch: 1048 loss: 111.774 val loss: 113.452\n",
            "epoch: 1049 loss: 118.109 val loss: 112.962\n",
            "epoch: 1050 loss: 142.643 val loss: 115.040\n",
            "epoch: 1051 loss: 120.376 val loss: 113.986\n",
            "epoch: 1052 loss: 95.649 val loss: 116.977\n",
            "epoch: 1053 loss: 126.437 val loss: 114.310\n",
            "epoch: 1054 loss: 95.396 val loss: 114.811\n",
            "epoch: 1055 loss: 115.903 val loss: 110.699\n",
            "epoch: 1056 loss: 121.865 val loss: 114.550\n",
            "epoch: 1057 loss: 102.457 val loss: 109.992\n",
            "epoch: 1058 loss: 110.699 val loss: 112.444\n",
            "epoch: 1059 loss: 134.292 val loss: 113.458\n",
            "epoch: 1060 loss: 97.827 val loss: 111.317\n",
            "epoch: 1061 loss: 139.340 val loss: 113.663\n",
            "epoch: 1062 loss: 111.030 val loss: 111.069\n",
            "epoch: 1063 loss: 112.202 val loss: 114.679\n",
            "epoch: 1064 loss: 83.628 val loss: 113.593\n",
            "epoch: 1065 loss: 111.698 val loss: 114.065\n",
            "epoch: 1066 loss: 158.085 val loss: 112.819\n",
            "epoch: 1067 loss: 105.893 val loss: 112.273\n",
            "epoch: 1068 loss: 91.769 val loss: 112.278\n",
            "epoch: 1069 loss: 84.565 val loss: 114.841\n",
            "epoch: 1070 loss: 109.364 val loss: 113.730\n",
            "epoch: 1071 loss: 66.754 val loss: 111.051\n",
            "epoch: 1072 loss: 103.746 val loss: 112.614\n",
            "epoch: 1073 loss: 149.951 val loss: 112.892\n",
            "epoch: 1074 loss: 113.541 val loss: 113.044\n",
            "epoch: 1075 loss: 94.401 val loss: 113.240\n",
            "epoch: 1076 loss: 86.375 val loss: 110.997\n",
            "epoch: 1077 loss: 106.007 val loss: 112.986\n",
            "epoch: 1078 loss: 111.403 val loss: 111.758\n",
            "epoch: 1079 loss: 109.860 val loss: 113.553\n",
            "epoch: 1080 loss: 120.624 val loss: 113.999\n",
            "epoch: 1081 loss: 83.706 val loss: 112.679\n",
            "epoch: 1082 loss: 148.362 val loss: 110.588\n",
            "epoch: 1083 loss: 135.082 val loss: 115.666\n",
            "epoch: 1084 loss: 128.317 val loss: 110.840\n",
            "epoch: 1085 loss: 139.418 val loss: 113.092\n",
            "epoch: 1086 loss: 109.450 val loss: 115.163\n",
            "epoch: 1087 loss: 162.098 val loss: 114.468\n",
            "epoch: 1088 loss: 111.135 val loss: 113.935\n",
            "epoch: 1089 loss: 131.327 val loss: 114.022\n",
            "epoch: 1090 loss: 125.606 val loss: 111.473\n",
            "epoch: 1091 loss: 167.826 val loss: 114.559\n",
            "epoch: 1092 loss: 200.866 val loss: 113.055\n",
            "epoch: 1093 loss: 140.367 val loss: 111.530\n",
            "epoch: 1094 loss: 87.742 val loss: 112.124\n",
            "epoch: 1095 loss: 88.253 val loss: 111.903\n",
            "epoch: 1096 loss: 96.093 val loss: 113.986\n",
            "epoch: 1097 loss: 147.303 val loss: 113.756\n",
            "epoch: 1098 loss: 104.833 val loss: 112.418\n",
            "epoch: 1099 loss: 112.856 val loss: 112.838\n",
            "epoch: 1100 loss: 130.710 val loss: 112.997\n",
            "epoch: 1101 loss: 93.798 val loss: 112.712\n",
            "epoch: 1102 loss: 132.798 val loss: 110.658\n",
            "epoch: 1103 loss: 173.366 val loss: 112.654\n",
            "epoch: 1104 loss: 104.244 val loss: 111.669\n",
            "epoch: 1105 loss: 110.319 val loss: 114.619\n",
            "epoch: 1106 loss: 126.684 val loss: 114.052\n",
            "epoch: 1107 loss: 145.384 val loss: 110.611\n",
            "epoch: 1108 loss: 104.934 val loss: 115.862\n",
            "epoch: 1109 loss: 84.059 val loss: 111.706\n",
            "epoch: 1110 loss: 153.508 val loss: 111.443\n",
            "epoch: 1111 loss: 113.236 val loss: 113.136\n",
            "epoch: 1112 loss: 128.054 val loss: 113.181\n",
            "epoch: 1113 loss: 136.507 val loss: 114.332\n",
            "epoch: 1114 loss: 153.273 val loss: 112.120\n",
            "epoch: 1115 loss: 92.236 val loss: 113.607\n",
            "epoch: 1116 loss: 140.245 val loss: 113.529\n",
            "epoch: 1117 loss: 90.888 val loss: 111.682\n",
            "epoch: 1118 loss: 136.894 val loss: 111.116\n",
            "epoch: 1119 loss: 98.325 val loss: 112.189\n",
            "epoch: 1120 loss: 97.062 val loss: 112.822\n",
            "epoch: 1121 loss: 118.510 val loss: 111.133\n",
            "epoch: 1122 loss: 105.053 val loss: 113.645\n",
            "epoch: 1123 loss: 131.680 val loss: 111.380\n",
            "epoch: 1124 loss: 121.473 val loss: 114.179\n",
            "epoch: 1125 loss: 88.035 val loss: 110.530\n",
            "epoch: 1126 loss: 78.168 val loss: 113.559\n",
            "epoch: 1127 loss: 80.531 val loss: 113.649\n",
            "epoch: 1128 loss: 110.882 val loss: 112.332\n",
            "epoch: 1129 loss: 121.037 val loss: 111.650\n",
            "epoch: 1130 loss: 100.441 val loss: 112.433\n",
            "epoch: 1131 loss: 112.627 val loss: 111.839\n",
            "epoch: 1132 loss: 121.912 val loss: 112.033\n",
            "epoch: 1133 loss: 111.836 val loss: 114.748\n",
            "epoch: 1134 loss: 126.465 val loss: 112.393\n",
            "epoch: 1135 loss: 118.312 val loss: 112.355\n",
            "epoch: 1136 loss: 92.401 val loss: 110.743\n",
            "epoch: 1137 loss: 94.524 val loss: 113.639\n",
            "epoch: 1138 loss: 84.894 val loss: 111.806\n",
            "epoch: 1139 loss: 160.535 val loss: 113.142\n",
            "epoch: 1140 loss: 102.407 val loss: 113.146\n",
            "epoch: 1141 loss: 137.421 val loss: 110.566\n",
            "epoch: 1142 loss: 121.859 val loss: 115.173\n",
            "epoch: 1143 loss: 126.788 val loss: 113.762\n",
            "epoch: 1144 loss: 129.085 val loss: 113.318\n",
            "epoch: 1145 loss: 93.031 val loss: 114.387\n",
            "epoch: 1146 loss: 106.087 val loss: 112.789\n",
            "epoch: 1147 loss: 111.225 val loss: 113.235\n",
            "epoch: 1148 loss: 111.003 val loss: 110.469\n",
            "epoch: 1149 loss: 147.960 val loss: 112.229\n",
            "epoch: 1150 loss: 99.571 val loss: 116.034\n",
            "epoch: 1151 loss: 127.584 val loss: 110.453\n",
            "epoch: 1152 loss: 96.937 val loss: 116.007\n",
            "epoch: 1153 loss: 129.698 val loss: 112.493\n",
            "epoch: 1154 loss: 100.567 val loss: 113.750\n",
            "epoch: 1155 loss: 78.769 val loss: 113.386\n",
            "epoch: 1156 loss: 107.421 val loss: 111.933\n",
            "epoch: 1157 loss: 120.571 val loss: 111.791\n",
            "epoch: 1158 loss: 64.897 val loss: 110.791\n",
            "epoch: 1159 loss: 112.238 val loss: 113.128\n",
            "epoch: 1160 loss: 129.699 val loss: 112.796\n",
            "epoch: 1161 loss: 100.540 val loss: 113.283\n",
            "epoch: 1162 loss: 120.123 val loss: 112.488\n",
            "epoch: 1163 loss: 142.781 val loss: 111.119\n",
            "epoch: 1164 loss: 96.382 val loss: 114.167\n",
            "epoch: 1165 loss: 87.528 val loss: 114.553\n",
            "epoch: 1166 loss: 103.614 val loss: 113.018\n",
            "epoch: 1167 loss: 127.051 val loss: 112.869\n",
            "epoch: 1168 loss: 104.296 val loss: 111.132\n",
            "epoch: 1169 loss: 79.926 val loss: 116.547\n",
            "epoch: 1170 loss: 121.472 val loss: 111.887\n",
            "epoch: 1171 loss: 99.625 val loss: 112.355\n",
            "epoch: 1172 loss: 94.157 val loss: 112.872\n",
            "epoch: 1173 loss: 81.750 val loss: 112.297\n",
            "epoch: 1174 loss: 77.618 val loss: 111.959\n",
            "epoch: 1175 loss: 180.941 val loss: 112.743\n",
            "epoch: 1176 loss: 120.773 val loss: 112.229\n",
            "epoch: 1177 loss: 90.326 val loss: 113.445\n",
            "epoch: 1178 loss: 100.358 val loss: 115.316\n",
            "epoch: 1179 loss: 84.236 val loss: 114.311\n",
            "epoch: 1180 loss: 164.794 val loss: 113.301\n",
            "epoch: 1181 loss: 155.970 val loss: 113.353\n",
            "epoch: 1182 loss: 122.696 val loss: 112.507\n",
            "epoch: 1183 loss: 110.953 val loss: 113.307\n",
            "epoch: 1184 loss: 105.946 val loss: 114.781\n",
            "epoch: 1185 loss: 125.035 val loss: 114.224\n",
            "epoch: 1186 loss: 102.595 val loss: 114.233\n",
            "epoch: 1187 loss: 65.319 val loss: 111.560\n",
            "epoch: 1188 loss: 123.176 val loss: 110.534\n",
            "epoch: 1189 loss: 155.794 val loss: 114.472\n",
            "epoch: 1190 loss: 75.910 val loss: 112.232\n",
            "epoch: 1191 loss: 147.827 val loss: 111.159\n",
            "epoch: 1192 loss: 96.342 val loss: 115.901\n",
            "epoch: 1193 loss: 77.975 val loss: 112.340\n",
            "epoch: 1194 loss: 78.549 val loss: 114.855\n",
            "epoch: 1195 loss: 112.460 val loss: 113.666\n",
            "epoch: 1196 loss: 88.067 val loss: 111.446\n",
            "epoch: 1197 loss: 104.492 val loss: 113.239\n",
            "epoch: 1198 loss: 128.157 val loss: 111.869\n",
            "epoch: 1199 loss: 151.562 val loss: 112.216\n",
            "epoch: 1200 loss: 131.236 val loss: 112.665\n",
            "epoch: 1201 loss: 101.305 val loss: 114.048\n",
            "epoch: 1202 loss: 139.427 val loss: 111.991\n",
            "epoch: 1203 loss: 82.495 val loss: 116.001\n",
            "epoch: 1204 loss: 130.075 val loss: 113.274\n",
            "epoch: 1205 loss: 106.335 val loss: 114.807\n",
            "epoch: 1206 loss: 151.589 val loss: 113.461\n",
            "epoch: 1207 loss: 108.951 val loss: 113.541\n",
            "epoch: 1208 loss: 99.963 val loss: 115.413\n",
            "epoch: 1209 loss: 154.621 val loss: 111.200\n",
            "epoch: 1210 loss: 75.655 val loss: 115.195\n",
            "epoch: 1211 loss: 137.432 val loss: 110.297\n",
            "epoch: 1212 loss: 117.898 val loss: 113.844\n",
            "epoch: 1213 loss: 108.821 val loss: 113.586\n",
            "epoch: 1214 loss: 124.471 val loss: 114.088\n",
            "epoch: 1215 loss: 102.846 val loss: 110.555\n",
            "epoch: 1216 loss: 97.996 val loss: 112.047\n",
            "epoch: 1217 loss: 126.689 val loss: 114.760\n",
            "epoch: 1218 loss: 102.278 val loss: 115.127\n",
            "epoch: 1219 loss: 119.785 val loss: 111.800\n",
            "epoch: 1220 loss: 50.825 val loss: 111.961\n",
            "epoch: 1221 loss: 98.529 val loss: 112.968\n",
            "epoch: 1222 loss: 143.917 val loss: 114.970\n",
            "epoch: 1223 loss: 94.937 val loss: 114.035\n",
            "epoch: 1224 loss: 97.291 val loss: 116.643\n",
            "epoch: 1225 loss: 135.024 val loss: 114.088\n",
            "epoch: 1226 loss: 88.495 val loss: 113.772\n",
            "epoch: 1227 loss: 117.469 val loss: 113.370\n",
            "epoch: 1228 loss: 165.923 val loss: 113.787\n",
            "epoch: 1229 loss: 79.756 val loss: 112.774\n",
            "epoch: 1230 loss: 140.316 val loss: 113.193\n",
            "epoch: 1231 loss: 100.967 val loss: 114.342\n",
            "epoch: 1232 loss: 103.702 val loss: 113.495\n",
            "epoch: 1233 loss: 113.183 val loss: 112.560\n",
            "epoch: 1234 loss: 111.746 val loss: 113.706\n",
            "epoch: 1235 loss: 135.529 val loss: 111.519\n",
            "epoch: 1236 loss: 179.433 val loss: 112.379\n",
            "epoch: 1237 loss: 80.669 val loss: 111.849\n",
            "epoch: 1238 loss: 129.209 val loss: 110.806\n",
            "epoch: 1239 loss: 95.933 val loss: 113.366\n",
            "epoch: 1240 loss: 106.542 val loss: 111.663\n",
            "epoch: 1241 loss: 119.368 val loss: 113.007\n",
            "epoch: 1242 loss: 130.872 val loss: 112.139\n",
            "epoch: 1243 loss: 114.939 val loss: 113.218\n",
            "epoch: 1244 loss: 120.965 val loss: 113.671\n",
            "epoch: 1245 loss: 124.658 val loss: 110.660\n",
            "epoch: 1246 loss: 85.071 val loss: 113.390\n",
            "epoch: 1247 loss: 106.397 val loss: 111.108\n",
            "epoch: 1248 loss: 119.317 val loss: 112.554\n",
            "epoch: 1249 loss: 96.876 val loss: 112.091\n",
            "epoch: 1250 loss: 83.027 val loss: 112.761\n",
            "epoch: 1251 loss: 85.019 val loss: 111.989\n",
            "epoch: 1252 loss: 115.973 val loss: 113.385\n",
            "epoch: 1253 loss: 101.779 val loss: 112.733\n",
            "epoch: 1254 loss: 116.408 val loss: 113.322\n",
            "epoch: 1255 loss: 62.920 val loss: 113.187\n",
            "epoch: 1256 loss: 120.013 val loss: 110.889\n",
            "epoch: 1257 loss: 96.419 val loss: 114.826\n",
            "epoch: 1258 loss: 100.795 val loss: 114.793\n",
            "epoch: 1259 loss: 133.420 val loss: 112.635\n",
            "epoch: 1260 loss: 75.689 val loss: 112.064\n",
            "epoch: 1261 loss: 111.472 val loss: 113.192\n",
            "epoch: 1262 loss: 121.354 val loss: 112.028\n",
            "epoch: 1263 loss: 113.691 val loss: 112.783\n",
            "epoch: 1264 loss: 112.628 val loss: 112.735\n",
            "epoch: 1265 loss: 90.397 val loss: 111.689\n",
            "epoch: 1266 loss: 103.825 val loss: 112.527\n",
            "epoch: 1267 loss: 100.949 val loss: 112.341\n",
            "epoch: 1268 loss: 106.655 val loss: 112.401\n",
            "epoch: 1269 loss: 116.976 val loss: 111.374\n",
            "epoch: 1270 loss: 144.242 val loss: 115.896\n",
            "epoch: 1271 loss: 143.015 val loss: 111.897\n",
            "epoch: 1272 loss: 70.606 val loss: 113.528\n",
            "epoch: 1273 loss: 73.932 val loss: 113.091\n",
            "epoch: 1274 loss: 146.909 val loss: 112.653\n",
            "epoch: 1275 loss: 129.593 val loss: 114.858\n",
            "epoch: 1276 loss: 147.568 val loss: 114.382\n",
            "epoch: 1277 loss: 99.155 val loss: 115.364\n",
            "epoch: 1278 loss: 100.687 val loss: 111.413\n",
            "epoch: 1279 loss: 105.496 val loss: 113.113\n",
            "epoch: 1280 loss: 132.025 val loss: 111.621\n",
            "epoch: 1281 loss: 157.924 val loss: 115.430\n",
            "epoch: 1282 loss: 120.802 val loss: 111.673\n",
            "epoch: 1283 loss: 112.253 val loss: 110.525\n",
            "epoch: 1284 loss: 101.779 val loss: 114.546\n",
            "epoch: 1285 loss: 141.948 val loss: 113.940\n",
            "epoch: 1286 loss: 100.326 val loss: 111.990\n",
            "epoch: 1287 loss: 91.633 val loss: 112.663\n",
            "epoch: 1288 loss: 106.259 val loss: 114.713\n",
            "epoch: 1289 loss: 89.259 val loss: 113.957\n",
            "epoch: 1290 loss: 127.071 val loss: 112.413\n",
            "epoch: 1291 loss: 106.525 val loss: 112.074\n",
            "epoch: 1292 loss: 73.842 val loss: 111.509\n",
            "epoch: 1293 loss: 119.868 val loss: 114.948\n",
            "epoch: 1294 loss: 104.805 val loss: 112.301\n",
            "epoch: 1295 loss: 106.463 val loss: 111.487\n",
            "epoch: 1296 loss: 100.011 val loss: 110.962\n",
            "epoch: 1297 loss: 102.650 val loss: 113.666\n",
            "epoch: 1298 loss: 72.125 val loss: 111.361\n",
            "epoch: 1299 loss: 97.136 val loss: 114.022\n",
            "epoch: 1300 loss: 115.319 val loss: 111.184\n",
            "epoch: 1301 loss: 99.636 val loss: 114.760\n",
            "epoch: 1302 loss: 112.466 val loss: 111.893\n",
            "epoch: 1303 loss: 103.326 val loss: 112.323\n",
            "epoch: 1304 loss: 104.829 val loss: 111.574\n",
            "epoch: 1305 loss: 92.582 val loss: 114.470\n",
            "epoch: 1306 loss: 81.361 val loss: 111.204\n",
            "epoch: 1307 loss: 114.454 val loss: 113.620\n",
            "epoch: 1308 loss: 88.833 val loss: 114.297\n",
            "epoch: 1309 loss: 110.863 val loss: 113.929\n",
            "epoch: 1310 loss: 110.866 val loss: 114.877\n",
            "epoch: 1311 loss: 103.015 val loss: 112.210\n",
            "epoch: 1312 loss: 129.271 val loss: 112.985\n",
            "epoch: 1313 loss: 98.994 val loss: 113.352\n",
            "epoch: 1314 loss: 118.792 val loss: 113.459\n",
            "epoch: 1315 loss: 76.383 val loss: 113.605\n",
            "epoch: 1316 loss: 78.013 val loss: 114.188\n",
            "epoch: 1317 loss: 154.998 val loss: 113.221\n",
            "epoch: 1318 loss: 127.966 val loss: 112.407\n",
            "epoch: 1319 loss: 101.510 val loss: 111.884\n",
            "epoch: 1320 loss: 75.506 val loss: 116.093\n",
            "epoch: 1321 loss: 110.127 val loss: 111.367\n",
            "epoch: 1322 loss: 104.011 val loss: 111.256\n",
            "epoch: 1323 loss: 136.578 val loss: 113.262\n",
            "epoch: 1324 loss: 75.981 val loss: 114.277\n",
            "epoch: 1325 loss: 96.079 val loss: 112.104\n",
            "epoch: 1326 loss: 137.270 val loss: 112.199\n",
            "epoch: 1327 loss: 110.608 val loss: 112.839\n",
            "epoch: 1328 loss: 114.703 val loss: 115.156\n",
            "epoch: 1329 loss: 108.337 val loss: 112.949\n",
            "epoch: 1330 loss: 125.111 val loss: 113.283\n",
            "epoch: 1331 loss: 116.065 val loss: 111.713\n",
            "epoch: 1332 loss: 126.130 val loss: 113.213\n",
            "epoch: 1333 loss: 92.321 val loss: 112.753\n",
            "epoch: 1334 loss: 138.691 val loss: 112.561\n",
            "epoch: 1335 loss: 102.969 val loss: 113.872\n",
            "epoch: 1336 loss: 115.963 val loss: 115.620\n",
            "epoch: 1337 loss: 94.590 val loss: 113.995\n",
            "epoch: 1338 loss: 125.840 val loss: 112.671\n",
            "epoch: 1339 loss: 104.568 val loss: 110.123\n",
            "epoch: 1340 loss: 84.084 val loss: 114.134\n",
            "epoch: 1341 loss: 133.709 val loss: 114.559\n",
            "epoch: 1342 loss: 122.242 val loss: 110.738\n",
            "epoch: 1343 loss: 96.638 val loss: 112.164\n",
            "epoch: 1344 loss: 138.076 val loss: 112.755\n",
            "epoch: 1345 loss: 148.334 val loss: 111.410\n",
            "epoch: 1346 loss: 146.099 val loss: 112.593\n",
            "epoch: 1347 loss: 91.464 val loss: 112.414\n",
            "epoch: 1348 loss: 121.404 val loss: 113.825\n",
            "epoch: 1349 loss: 103.329 val loss: 114.264\n",
            "epoch: 1350 loss: 119.572 val loss: 111.972\n",
            "epoch: 1351 loss: 83.800 val loss: 111.920\n",
            "epoch: 1352 loss: 118.952 val loss: 111.486\n",
            "epoch: 1353 loss: 103.739 val loss: 111.235\n",
            "epoch: 1354 loss: 151.482 val loss: 111.154\n",
            "epoch: 1355 loss: 105.275 val loss: 113.403\n",
            "epoch: 1356 loss: 120.995 val loss: 112.942\n",
            "epoch: 1357 loss: 117.492 val loss: 115.235\n",
            "epoch: 1358 loss: 89.447 val loss: 110.759\n",
            "epoch: 1359 loss: 77.175 val loss: 112.990\n",
            "epoch: 1360 loss: 94.481 val loss: 112.354\n",
            "epoch: 1361 loss: 124.327 val loss: 112.296\n",
            "epoch: 1362 loss: 148.539 val loss: 112.263\n",
            "epoch: 1363 loss: 135.467 val loss: 112.647\n",
            "epoch: 1364 loss: 102.061 val loss: 112.932\n",
            "epoch: 1365 loss: 96.922 val loss: 112.046\n",
            "epoch: 1366 loss: 108.689 val loss: 112.498\n",
            "epoch: 1367 loss: 175.901 val loss: 113.328\n",
            "epoch: 1368 loss: 67.114 val loss: 113.213\n",
            "epoch: 1369 loss: 102.731 val loss: 112.286\n",
            "epoch: 1370 loss: 70.149 val loss: 113.848\n",
            "epoch: 1371 loss: 122.334 val loss: 111.860\n",
            "epoch: 1372 loss: 82.220 val loss: 112.481\n",
            "epoch: 1373 loss: 102.912 val loss: 110.807\n",
            "epoch: 1374 loss: 114.721 val loss: 114.075\n",
            "epoch: 1375 loss: 100.338 val loss: 112.765\n",
            "epoch: 1376 loss: 110.594 val loss: 114.480\n",
            "epoch: 1377 loss: 100.855 val loss: 111.698\n",
            "epoch: 1378 loss: 123.373 val loss: 111.651\n",
            "epoch: 1379 loss: 154.784 val loss: 115.749\n",
            "epoch: 1380 loss: 140.021 val loss: 115.206\n",
            "epoch: 1381 loss: 118.007 val loss: 113.444\n",
            "epoch: 1382 loss: 132.425 val loss: 112.987\n",
            "epoch: 1383 loss: 143.995 val loss: 112.391\n",
            "epoch: 1384 loss: 121.288 val loss: 113.156\n",
            "epoch: 1385 loss: 117.390 val loss: 112.455\n",
            "epoch: 1386 loss: 85.527 val loss: 113.256\n",
            "epoch: 1387 loss: 131.767 val loss: 111.689\n",
            "epoch: 1388 loss: 133.821 val loss: 111.767\n",
            "epoch: 1389 loss: 80.184 val loss: 114.709\n",
            "epoch: 1390 loss: 159.880 val loss: 112.016\n",
            "epoch: 1391 loss: 104.696 val loss: 114.348\n",
            "epoch: 1392 loss: 109.779 val loss: 110.433\n",
            "epoch: 1393 loss: 112.331 val loss: 110.765\n",
            "epoch: 1394 loss: 104.539 val loss: 112.659\n",
            "epoch: 1395 loss: 112.299 val loss: 112.393\n",
            "epoch: 1396 loss: 128.450 val loss: 112.904\n",
            "epoch: 1397 loss: 103.458 val loss: 114.883\n",
            "epoch: 1398 loss: 76.184 val loss: 112.708\n",
            "epoch: 1399 loss: 135.317 val loss: 114.137\n",
            "epoch: 1400 loss: 94.096 val loss: 112.098\n",
            "epoch: 1401 loss: 136.747 val loss: 110.920\n",
            "epoch: 1402 loss: 102.775 val loss: 111.775\n",
            "epoch: 1403 loss: 124.604 val loss: 112.075\n",
            "epoch: 1404 loss: 125.237 val loss: 111.394\n",
            "epoch: 1405 loss: 81.089 val loss: 112.164\n",
            "epoch: 1406 loss: 141.247 val loss: 112.221\n",
            "epoch: 1407 loss: 91.309 val loss: 112.316\n",
            "epoch: 1408 loss: 120.888 val loss: 112.410\n",
            "epoch: 1409 loss: 108.586 val loss: 113.567\n",
            "epoch: 1410 loss: 132.986 val loss: 114.163\n",
            "epoch: 1411 loss: 151.571 val loss: 113.145\n",
            "epoch: 1412 loss: 141.540 val loss: 110.436\n",
            "epoch: 1413 loss: 91.209 val loss: 114.762\n",
            "epoch: 1414 loss: 174.972 val loss: 113.897\n",
            "epoch: 1415 loss: 87.171 val loss: 114.813\n",
            "epoch: 1416 loss: 91.977 val loss: 111.516\n",
            "epoch: 1417 loss: 111.262 val loss: 110.870\n",
            "epoch: 1418 loss: 114.963 val loss: 112.029\n",
            "epoch: 1419 loss: 93.413 val loss: 114.140\n",
            "epoch: 1420 loss: 83.577 val loss: 111.755\n",
            "epoch: 1421 loss: 122.348 val loss: 112.184\n",
            "epoch: 1422 loss: 108.040 val loss: 114.352\n",
            "epoch: 1423 loss: 108.189 val loss: 111.212\n",
            "epoch: 1424 loss: 129.962 val loss: 111.805\n",
            "epoch: 1425 loss: 78.785 val loss: 112.173\n",
            "epoch: 1426 loss: 150.065 val loss: 111.522\n",
            "epoch: 1427 loss: 138.332 val loss: 115.740\n",
            "epoch: 1428 loss: 106.221 val loss: 113.745\n",
            "epoch: 1429 loss: 111.190 val loss: 115.092\n",
            "epoch: 1430 loss: 95.552 val loss: 113.459\n",
            "epoch: 1431 loss: 102.635 val loss: 113.417\n",
            "epoch: 1432 loss: 107.899 val loss: 116.978\n",
            "epoch: 1433 loss: 107.680 val loss: 112.106\n",
            "epoch: 1434 loss: 137.808 val loss: 113.057\n",
            "epoch: 1435 loss: 169.346 val loss: 116.517\n",
            "epoch: 1436 loss: 120.147 val loss: 112.126\n",
            "epoch: 1437 loss: 166.416 val loss: 114.718\n",
            "epoch: 1438 loss: 117.593 val loss: 113.340\n",
            "epoch: 1439 loss: 108.045 val loss: 113.614\n",
            "epoch: 1440 loss: 121.677 val loss: 109.944\n",
            "epoch: 1441 loss: 122.130 val loss: 111.290\n",
            "epoch: 1442 loss: 60.600 val loss: 116.833\n",
            "epoch: 1443 loss: 131.365 val loss: 110.885\n",
            "epoch: 1444 loss: 168.555 val loss: 113.874\n",
            "epoch: 1445 loss: 144.639 val loss: 112.003\n",
            "epoch: 1446 loss: 61.395 val loss: 113.149\n",
            "epoch: 1447 loss: 104.448 val loss: 115.370\n",
            "epoch: 1448 loss: 108.860 val loss: 115.907\n",
            "epoch: 1449 loss: 191.189 val loss: 111.945\n",
            "epoch: 1450 loss: 98.902 val loss: 114.617\n",
            "epoch: 1451 loss: 133.355 val loss: 115.707\n",
            "epoch: 1452 loss: 148.932 val loss: 113.057\n",
            "epoch: 1453 loss: 128.462 val loss: 111.302\n",
            "epoch: 1454 loss: 131.367 val loss: 115.771\n",
            "epoch: 1455 loss: 95.708 val loss: 113.612\n",
            "epoch: 1456 loss: 110.963 val loss: 111.339\n",
            "epoch: 1457 loss: 115.120 val loss: 113.390\n",
            "epoch: 1458 loss: 94.151 val loss: 112.650\n",
            "epoch: 1459 loss: 108.960 val loss: 112.094\n",
            "epoch: 1460 loss: 80.670 val loss: 110.844\n",
            "epoch: 1461 loss: 124.225 val loss: 111.576\n",
            "epoch: 1462 loss: 102.593 val loss: 114.813\n",
            "epoch: 1463 loss: 109.890 val loss: 112.826\n",
            "epoch: 1464 loss: 104.554 val loss: 113.639\n",
            "epoch: 1465 loss: 131.528 val loss: 113.892\n",
            "epoch: 1466 loss: 134.156 val loss: 114.393\n",
            "epoch: 1467 loss: 119.806 val loss: 111.640\n",
            "epoch: 1468 loss: 155.604 val loss: 112.208\n",
            "epoch: 1469 loss: 77.408 val loss: 112.778\n",
            "epoch: 1470 loss: 134.336 val loss: 113.590\n",
            "epoch: 1471 loss: 166.599 val loss: 112.651\n",
            "epoch: 1472 loss: 99.401 val loss: 114.192\n",
            "epoch: 1473 loss: 96.887 val loss: 114.762\n",
            "epoch: 1474 loss: 100.533 val loss: 112.431\n",
            "epoch: 1475 loss: 107.603 val loss: 113.431\n",
            "epoch: 1476 loss: 116.020 val loss: 113.986\n",
            "epoch: 1477 loss: 156.228 val loss: 112.499\n",
            "epoch: 1478 loss: 164.632 val loss: 113.874\n",
            "epoch: 1479 loss: 68.769 val loss: 112.963\n",
            "epoch: 1480 loss: 132.672 val loss: 112.778\n",
            "epoch: 1481 loss: 114.886 val loss: 112.332\n",
            "epoch: 1482 loss: 140.601 val loss: 110.557\n",
            "epoch: 1483 loss: 109.077 val loss: 111.959\n",
            "epoch: 1484 loss: 98.477 val loss: 114.610\n",
            "epoch: 1485 loss: 81.010 val loss: 114.081\n",
            "epoch: 1486 loss: 117.735 val loss: 112.588\n",
            "epoch: 1487 loss: 127.307 val loss: 110.495\n",
            "epoch: 1488 loss: 124.365 val loss: 112.119\n",
            "epoch: 1489 loss: 101.094 val loss: 113.107\n",
            "epoch: 1490 loss: 127.768 val loss: 112.466\n",
            "epoch: 1491 loss: 115.951 val loss: 111.575\n",
            "epoch: 1492 loss: 132.616 val loss: 114.091\n",
            "epoch: 1493 loss: 89.787 val loss: 110.190\n",
            "epoch: 1494 loss: 80.815 val loss: 110.664\n",
            "epoch: 1495 loss: 96.979 val loss: 113.048\n",
            "epoch: 1496 loss: 68.402 val loss: 114.169\n",
            "epoch: 1497 loss: 96.685 val loss: 114.247\n",
            "epoch: 1498 loss: 114.707 val loss: 110.062\n",
            "epoch: 1499 loss: 100.160 val loss: 112.484\n",
            "epoch: 1500 loss: 118.690 val loss: 112.387\n",
            "epoch: 1501 loss: 103.763 val loss: 112.815\n",
            "epoch: 1502 loss: 100.169 val loss: 111.869\n",
            "epoch: 1503 loss: 123.217 val loss: 113.337\n",
            "epoch: 1504 loss: 105.456 val loss: 114.478\n",
            "epoch: 1505 loss: 72.492 val loss: 110.693\n",
            "epoch: 1506 loss: 98.784 val loss: 114.442\n",
            "epoch: 1507 loss: 114.341 val loss: 113.788\n",
            "epoch: 1508 loss: 110.089 val loss: 113.645\n",
            "epoch: 1509 loss: 76.123 val loss: 112.766\n",
            "epoch: 1510 loss: 100.657 val loss: 114.265\n",
            "epoch: 1511 loss: 98.070 val loss: 113.414\n",
            "epoch: 1512 loss: 131.276 val loss: 113.312\n",
            "epoch: 1513 loss: 136.052 val loss: 113.431\n",
            "epoch: 1514 loss: 114.042 val loss: 113.237\n",
            "epoch: 1515 loss: 124.371 val loss: 112.134\n",
            "epoch: 1516 loss: 96.552 val loss: 112.354\n",
            "epoch: 1517 loss: 103.260 val loss: 111.534\n",
            "epoch: 1518 loss: 99.001 val loss: 113.458\n",
            "epoch: 1519 loss: 137.258 val loss: 112.483\n",
            "epoch: 1520 loss: 118.041 val loss: 112.703\n",
            "epoch: 1521 loss: 74.854 val loss: 115.098\n",
            "epoch: 1522 loss: 124.782 val loss: 113.349\n",
            "epoch: 1523 loss: 100.327 val loss: 110.153\n",
            "epoch: 1524 loss: 103.426 val loss: 114.053\n",
            "epoch: 1525 loss: 117.600 val loss: 115.520\n",
            "epoch: 1526 loss: 101.489 val loss: 111.834\n",
            "epoch: 1527 loss: 80.365 val loss: 110.650\n",
            "epoch: 1528 loss: 96.275 val loss: 111.918\n",
            "epoch: 1529 loss: 154.655 val loss: 114.024\n",
            "epoch: 1530 loss: 114.989 val loss: 112.275\n",
            "epoch: 1531 loss: 104.300 val loss: 110.802\n",
            "epoch: 1532 loss: 117.782 val loss: 113.713\n",
            "epoch: 1533 loss: 135.774 val loss: 112.681\n",
            "epoch: 1534 loss: 102.274 val loss: 113.033\n",
            "epoch: 1535 loss: 99.188 val loss: 114.312\n",
            "epoch: 1536 loss: 149.177 val loss: 112.148\n",
            "epoch: 1537 loss: 128.627 val loss: 111.330\n",
            "epoch: 1538 loss: 136.182 val loss: 112.277\n",
            "epoch: 1539 loss: 97.561 val loss: 114.609\n",
            "epoch: 1540 loss: 147.004 val loss: 111.075\n",
            "epoch: 1541 loss: 117.930 val loss: 113.731\n",
            "epoch: 1542 loss: 106.050 val loss: 114.065\n",
            "epoch: 1543 loss: 116.750 val loss: 117.244\n",
            "epoch: 1544 loss: 187.073 val loss: 114.225\n",
            "epoch: 1545 loss: 127.460 val loss: 112.285\n",
            "epoch: 1546 loss: 100.211 val loss: 112.129\n",
            "epoch: 1547 loss: 121.124 val loss: 114.246\n",
            "epoch: 1548 loss: 100.027 val loss: 111.659\n",
            "epoch: 1549 loss: 104.128 val loss: 113.146\n",
            "epoch: 1550 loss: 115.268 val loss: 113.019\n",
            "epoch: 1551 loss: 146.784 val loss: 112.433\n",
            "epoch: 1552 loss: 152.257 val loss: 112.520\n",
            "epoch: 1553 loss: 134.144 val loss: 111.502\n",
            "epoch: 1554 loss: 108.148 val loss: 112.732\n",
            "epoch: 1555 loss: 161.712 val loss: 113.646\n",
            "epoch: 1556 loss: 97.645 val loss: 111.539\n",
            "epoch: 1557 loss: 89.494 val loss: 112.178\n",
            "epoch: 1558 loss: 138.314 val loss: 113.567\n",
            "epoch: 1559 loss: 126.709 val loss: 113.997\n",
            "epoch: 1560 loss: 82.632 val loss: 113.007\n",
            "epoch: 1561 loss: 147.730 val loss: 110.028\n",
            "epoch: 1562 loss: 135.769 val loss: 111.495\n",
            "epoch: 1563 loss: 138.194 val loss: 111.537\n",
            "epoch: 1564 loss: 133.589 val loss: 111.676\n",
            "epoch: 1565 loss: 99.567 val loss: 114.080\n",
            "epoch: 1566 loss: 78.846 val loss: 113.279\n",
            "epoch: 1567 loss: 132.852 val loss: 114.843\n",
            "epoch: 1568 loss: 104.540 val loss: 111.420\n",
            "epoch: 1569 loss: 128.558 val loss: 113.219\n",
            "epoch: 1570 loss: 96.617 val loss: 111.546\n",
            "epoch: 1571 loss: 120.357 val loss: 113.042\n",
            "epoch: 1572 loss: 127.213 val loss: 111.683\n",
            "epoch: 1573 loss: 114.498 val loss: 115.150\n",
            "epoch: 1574 loss: 126.378 val loss: 112.039\n",
            "epoch: 1575 loss: 154.899 val loss: 112.830\n",
            "epoch: 1576 loss: 87.935 val loss: 113.135\n",
            "epoch: 1577 loss: 142.058 val loss: 115.052\n",
            "epoch: 1578 loss: 110.029 val loss: 114.138\n",
            "epoch: 1579 loss: 152.183 val loss: 112.591\n",
            "epoch: 1580 loss: 101.015 val loss: 112.579\n",
            "epoch: 1581 loss: 106.615 val loss: 112.744\n",
            "epoch: 1582 loss: 109.600 val loss: 112.103\n",
            "epoch: 1583 loss: 96.346 val loss: 112.637\n",
            "epoch: 1584 loss: 130.530 val loss: 111.779\n",
            "epoch: 1585 loss: 136.274 val loss: 113.255\n",
            "epoch: 1586 loss: 171.560 val loss: 111.918\n",
            "epoch: 1587 loss: 110.399 val loss: 113.218\n",
            "epoch: 1588 loss: 125.597 val loss: 112.898\n",
            "epoch: 1589 loss: 110.466 val loss: 110.622\n",
            "epoch: 1590 loss: 127.274 val loss: 113.061\n",
            "epoch: 1591 loss: 111.997 val loss: 111.251\n",
            "epoch: 1592 loss: 98.858 val loss: 113.682\n",
            "epoch: 1593 loss: 142.210 val loss: 113.254\n",
            "epoch: 1594 loss: 123.034 val loss: 112.955\n",
            "epoch: 1595 loss: 103.025 val loss: 111.593\n",
            "epoch: 1596 loss: 107.361 val loss: 112.255\n",
            "epoch: 1597 loss: 96.545 val loss: 111.600\n",
            "epoch: 1598 loss: 117.273 val loss: 113.381\n",
            "epoch: 1599 loss: 140.784 val loss: 113.664\n",
            "epoch: 1600 loss: 114.072 val loss: 116.071\n",
            "epoch: 1601 loss: 94.321 val loss: 113.578\n",
            "epoch: 1602 loss: 107.573 val loss: 112.624\n",
            "epoch: 1603 loss: 89.236 val loss: 110.464\n",
            "epoch: 1604 loss: 125.112 val loss: 113.608\n",
            "epoch: 1605 loss: 152.445 val loss: 113.003\n",
            "epoch: 1606 loss: 86.259 val loss: 111.317\n",
            "epoch: 1607 loss: 127.210 val loss: 114.229\n",
            "epoch: 1608 loss: 107.762 val loss: 110.027\n",
            "epoch: 1609 loss: 126.109 val loss: 113.994\n",
            "epoch: 1610 loss: 109.167 val loss: 114.056\n",
            "epoch: 1611 loss: 143.742 val loss: 111.830\n",
            "epoch: 1612 loss: 79.655 val loss: 114.271\n",
            "epoch: 1613 loss: 98.329 val loss: 112.666\n",
            "epoch: 1614 loss: 96.205 val loss: 112.584\n",
            "epoch: 1615 loss: 114.639 val loss: 113.432\n",
            "epoch: 1616 loss: 128.458 val loss: 111.796\n",
            "epoch: 1617 loss: 125.518 val loss: 111.430\n",
            "epoch: 1618 loss: 97.663 val loss: 112.081\n",
            "epoch: 1619 loss: 123.011 val loss: 112.941\n",
            "epoch: 1620 loss: 105.984 val loss: 113.448\n",
            "epoch: 1621 loss: 122.114 val loss: 113.760\n",
            "epoch: 1622 loss: 142.418 val loss: 113.255\n",
            "epoch: 1623 loss: 132.208 val loss: 115.587\n",
            "epoch: 1624 loss: 114.891 val loss: 110.452\n",
            "epoch: 1625 loss: 112.917 val loss: 113.935\n",
            "epoch: 1626 loss: 163.892 val loss: 114.515\n",
            "epoch: 1627 loss: 117.640 val loss: 114.123\n",
            "epoch: 1628 loss: 136.385 val loss: 112.786\n",
            "epoch: 1629 loss: 103.857 val loss: 113.223\n",
            "epoch: 1630 loss: 117.586 val loss: 111.208\n",
            "epoch: 1631 loss: 118.320 val loss: 113.694\n",
            "epoch: 1632 loss: 112.344 val loss: 114.953\n",
            "epoch: 1633 loss: 147.410 val loss: 112.089\n",
            "epoch: 1634 loss: 117.488 val loss: 112.770\n",
            "epoch: 1635 loss: 98.166 val loss: 111.955\n",
            "epoch: 1636 loss: 101.224 val loss: 113.379\n",
            "epoch: 1637 loss: 97.141 val loss: 110.979\n",
            "epoch: 1638 loss: 157.165 val loss: 112.615\n",
            "epoch: 1639 loss: 135.329 val loss: 111.372\n",
            "epoch: 1640 loss: 128.637 val loss: 112.837\n",
            "epoch: 1641 loss: 120.801 val loss: 112.868\n",
            "epoch: 1642 loss: 78.781 val loss: 113.558\n",
            "epoch: 1643 loss: 69.545 val loss: 112.413\n",
            "epoch: 1644 loss: 108.241 val loss: 113.179\n",
            "epoch: 1645 loss: 80.924 val loss: 110.465\n",
            "epoch: 1646 loss: 142.172 val loss: 112.569\n",
            "epoch: 1647 loss: 130.403 val loss: 111.106\n",
            "epoch: 1648 loss: 105.491 val loss: 112.660\n",
            "epoch: 1649 loss: 126.299 val loss: 113.316\n",
            "epoch: 1650 loss: 131.727 val loss: 114.212\n",
            "epoch: 1651 loss: 106.252 val loss: 111.518\n",
            "epoch: 1652 loss: 130.885 val loss: 113.410\n",
            "epoch: 1653 loss: 79.392 val loss: 116.911\n",
            "epoch: 1654 loss: 136.860 val loss: 111.035\n",
            "epoch: 1655 loss: 83.450 val loss: 113.836\n",
            "epoch: 1656 loss: 110.235 val loss: 113.947\n",
            "epoch: 1657 loss: 139.525 val loss: 112.228\n",
            "epoch: 1658 loss: 94.198 val loss: 111.618\n",
            "epoch: 1659 loss: 113.670 val loss: 114.190\n",
            "epoch: 1660 loss: 132.955 val loss: 110.758\n",
            "epoch: 1661 loss: 149.270 val loss: 112.088\n",
            "epoch: 1662 loss: 80.845 val loss: 110.898\n",
            "epoch: 1663 loss: 135.584 val loss: 113.330\n",
            "epoch: 1664 loss: 149.987 val loss: 115.732\n",
            "epoch: 1665 loss: 63.453 val loss: 114.032\n",
            "epoch: 1666 loss: 99.754 val loss: 112.711\n",
            "epoch: 1667 loss: 106.225 val loss: 113.754\n",
            "epoch: 1668 loss: 96.537 val loss: 112.233\n",
            "epoch: 1669 loss: 80.498 val loss: 114.153\n",
            "epoch: 1670 loss: 104.547 val loss: 113.782\n",
            "epoch: 1671 loss: 96.122 val loss: 116.874\n",
            "epoch: 1672 loss: 104.860 val loss: 113.529\n",
            "epoch: 1673 loss: 89.455 val loss: 114.488\n",
            "epoch: 1674 loss: 170.203 val loss: 112.241\n",
            "epoch: 1675 loss: 86.005 val loss: 114.568\n",
            "epoch: 1676 loss: 118.535 val loss: 114.148\n",
            "epoch: 1677 loss: 85.850 val loss: 111.755\n",
            "epoch: 1678 loss: 131.072 val loss: 111.711\n",
            "epoch: 1679 loss: 92.908 val loss: 112.558\n",
            "epoch: 1680 loss: 70.084 val loss: 114.511\n",
            "epoch: 1681 loss: 119.733 val loss: 113.891\n",
            "epoch: 1682 loss: 75.663 val loss: 110.801\n",
            "epoch: 1683 loss: 65.183 val loss: 111.805\n",
            "epoch: 1684 loss: 120.238 val loss: 113.430\n",
            "epoch: 1685 loss: 115.379 val loss: 114.550\n",
            "epoch: 1686 loss: 139.782 val loss: 110.920\n",
            "epoch: 1687 loss: 75.604 val loss: 110.065\n",
            "epoch: 1688 loss: 92.244 val loss: 112.930\n",
            "epoch: 1689 loss: 104.122 val loss: 113.397\n",
            "epoch: 1690 loss: 105.465 val loss: 113.647\n",
            "epoch: 1691 loss: 128.299 val loss: 111.368\n",
            "epoch: 1692 loss: 138.357 val loss: 112.871\n",
            "epoch: 1693 loss: 84.741 val loss: 113.656\n",
            "epoch: 1694 loss: 109.986 val loss: 112.531\n",
            "epoch: 1695 loss: 97.673 val loss: 112.248\n",
            "epoch: 1696 loss: 99.333 val loss: 111.721\n",
            "epoch: 1697 loss: 110.930 val loss: 112.594\n",
            "epoch: 1698 loss: 73.652 val loss: 113.842\n",
            "epoch: 1699 loss: 119.608 val loss: 112.760\n",
            "epoch: 1700 loss: 121.579 val loss: 115.665\n",
            "epoch: 1701 loss: 102.003 val loss: 114.897\n",
            "epoch: 1702 loss: 147.723 val loss: 111.304\n",
            "epoch: 1703 loss: 94.718 val loss: 114.875\n",
            "epoch: 1704 loss: 113.545 val loss: 112.169\n",
            "epoch: 1705 loss: 80.935 val loss: 112.590\n",
            "epoch: 1706 loss: 98.386 val loss: 113.112\n",
            "epoch: 1707 loss: 86.202 val loss: 111.243\n",
            "epoch: 1708 loss: 113.575 val loss: 113.433\n",
            "epoch: 1709 loss: 98.456 val loss: 112.174\n",
            "epoch: 1710 loss: 90.303 val loss: 111.360\n",
            "epoch: 1711 loss: 128.654 val loss: 111.099\n",
            "epoch: 1712 loss: 95.810 val loss: 111.633\n",
            "epoch: 1713 loss: 112.821 val loss: 112.706\n",
            "epoch: 1714 loss: 107.543 val loss: 112.477\n",
            "epoch: 1715 loss: 84.225 val loss: 110.633\n",
            "epoch: 1716 loss: 148.896 val loss: 113.298\n",
            "epoch: 1717 loss: 104.655 val loss: 112.608\n",
            "epoch: 1718 loss: 69.835 val loss: 111.027\n",
            "epoch: 1719 loss: 91.476 val loss: 110.910\n",
            "epoch: 1720 loss: 112.658 val loss: 113.280\n",
            "epoch: 1721 loss: 115.176 val loss: 112.734\n",
            "epoch: 1722 loss: 124.053 val loss: 116.480\n",
            "epoch: 1723 loss: 126.368 val loss: 113.503\n",
            "epoch: 1724 loss: 113.869 val loss: 111.724\n",
            "epoch: 1725 loss: 141.405 val loss: 113.213\n",
            "epoch: 1726 loss: 112.125 val loss: 111.753\n",
            "epoch: 1727 loss: 140.429 val loss: 113.385\n",
            "epoch: 1728 loss: 116.394 val loss: 111.825\n",
            "epoch: 1729 loss: 93.882 val loss: 112.252\n",
            "epoch: 1730 loss: 80.158 val loss: 113.201\n",
            "epoch: 1731 loss: 122.883 val loss: 114.887\n",
            "epoch: 1732 loss: 88.120 val loss: 116.303\n",
            "epoch: 1733 loss: 98.231 val loss: 113.359\n",
            "epoch: 1734 loss: 86.533 val loss: 111.909\n",
            "epoch: 1735 loss: 115.979 val loss: 114.522\n",
            "epoch: 1736 loss: 116.247 val loss: 112.807\n",
            "epoch: 1737 loss: 62.297 val loss: 116.672\n",
            "epoch: 1738 loss: 61.916 val loss: 111.305\n",
            "epoch: 1739 loss: 119.975 val loss: 112.977\n",
            "epoch: 1740 loss: 132.935 val loss: 111.563\n",
            "epoch: 1741 loss: 109.250 val loss: 112.403\n",
            "epoch: 1742 loss: 121.021 val loss: 111.492\n",
            "epoch: 1743 loss: 73.650 val loss: 113.469\n",
            "epoch: 1744 loss: 108.199 val loss: 113.034\n",
            "epoch: 1745 loss: 135.369 val loss: 111.596\n",
            "epoch: 1746 loss: 126.651 val loss: 114.436\n",
            "epoch: 1747 loss: 93.109 val loss: 115.419\n",
            "epoch: 1748 loss: 100.774 val loss: 114.241\n",
            "epoch: 1749 loss: 107.628 val loss: 112.645\n",
            "epoch: 1750 loss: 87.799 val loss: 112.780\n",
            "epoch: 1751 loss: 110.712 val loss: 111.803\n",
            "epoch: 1752 loss: 138.702 val loss: 111.841\n",
            "epoch: 1753 loss: 119.581 val loss: 111.997\n",
            "epoch: 1754 loss: 114.932 val loss: 112.138\n",
            "epoch: 1755 loss: 60.377 val loss: 111.315\n",
            "epoch: 1756 loss: 108.302 val loss: 112.380\n",
            "epoch: 1757 loss: 105.877 val loss: 112.922\n",
            "epoch: 1758 loss: 114.304 val loss: 111.558\n",
            "epoch: 1759 loss: 106.703 val loss: 112.725\n",
            "epoch: 1760 loss: 107.078 val loss: 110.986\n",
            "epoch: 1761 loss: 105.220 val loss: 113.518\n",
            "epoch: 1762 loss: 82.947 val loss: 112.102\n",
            "epoch: 1763 loss: 116.551 val loss: 113.424\n",
            "epoch: 1764 loss: 149.296 val loss: 113.410\n",
            "epoch: 1765 loss: 134.026 val loss: 115.370\n",
            "epoch: 1766 loss: 113.620 val loss: 113.761\n",
            "epoch: 1767 loss: 132.404 val loss: 114.670\n",
            "epoch: 1768 loss: 139.068 val loss: 112.692\n",
            "epoch: 1769 loss: 98.921 val loss: 112.192\n",
            "epoch: 1770 loss: 123.404 val loss: 114.247\n",
            "epoch: 1771 loss: 100.082 val loss: 110.720\n",
            "epoch: 1772 loss: 91.360 val loss: 111.340\n",
            "epoch: 1773 loss: 92.201 val loss: 113.636\n",
            "epoch: 1774 loss: 122.261 val loss: 114.438\n",
            "epoch: 1775 loss: 141.526 val loss: 112.241\n",
            "epoch: 1776 loss: 100.376 val loss: 115.601\n",
            "epoch: 1777 loss: 123.262 val loss: 113.277\n",
            "epoch: 1778 loss: 136.133 val loss: 111.483\n",
            "epoch: 1779 loss: 168.907 val loss: 113.260\n",
            "epoch: 1780 loss: 132.355 val loss: 111.934\n",
            "epoch: 1781 loss: 133.043 val loss: 113.040\n",
            "epoch: 1782 loss: 79.577 val loss: 111.828\n",
            "epoch: 1783 loss: 125.974 val loss: 113.168\n",
            "epoch: 1784 loss: 81.651 val loss: 115.361\n",
            "epoch: 1785 loss: 127.126 val loss: 113.299\n",
            "epoch: 1786 loss: 93.305 val loss: 115.463\n",
            "epoch: 1787 loss: 154.049 val loss: 115.479\n",
            "epoch: 1788 loss: 110.679 val loss: 111.900\n",
            "epoch: 1789 loss: 123.070 val loss: 112.537\n",
            "epoch: 1790 loss: 120.443 val loss: 111.357\n",
            "epoch: 1791 loss: 104.058 val loss: 110.884\n",
            "epoch: 1792 loss: 109.990 val loss: 111.732\n",
            "epoch: 1793 loss: 87.300 val loss: 111.830\n",
            "epoch: 1794 loss: 116.620 val loss: 113.703\n",
            "epoch: 1795 loss: 158.960 val loss: 113.863\n",
            "epoch: 1796 loss: 116.895 val loss: 112.519\n",
            "epoch: 1797 loss: 111.996 val loss: 112.208\n",
            "epoch: 1798 loss: 125.129 val loss: 112.828\n",
            "epoch: 1799 loss: 128.307 val loss: 113.028\n",
            "epoch: 1800 loss: 124.636 val loss: 113.955\n",
            "epoch: 1801 loss: 107.883 val loss: 111.159\n",
            "epoch: 1802 loss: 110.896 val loss: 112.506\n",
            "epoch: 1803 loss: 114.596 val loss: 113.124\n",
            "epoch: 1804 loss: 134.744 val loss: 112.101\n",
            "epoch: 1805 loss: 128.417 val loss: 117.688\n",
            "epoch: 1806 loss: 98.608 val loss: 111.693\n",
            "epoch: 1807 loss: 81.992 val loss: 113.597\n",
            "epoch: 1808 loss: 96.009 val loss: 112.022\n",
            "epoch: 1809 loss: 113.048 val loss: 109.934\n",
            "epoch: 1810 loss: 141.288 val loss: 111.204\n",
            "epoch: 1811 loss: 167.934 val loss: 111.746\n",
            "epoch: 1812 loss: 86.587 val loss: 111.923\n",
            "epoch: 1813 loss: 102.604 val loss: 113.624\n",
            "epoch: 1814 loss: 130.288 val loss: 112.494\n",
            "epoch: 1815 loss: 100.019 val loss: 113.990\n",
            "epoch: 1816 loss: 90.024 val loss: 111.961\n",
            "epoch: 1817 loss: 118.432 val loss: 111.711\n",
            "epoch: 1818 loss: 115.944 val loss: 113.924\n",
            "epoch: 1819 loss: 110.236 val loss: 110.612\n",
            "epoch: 1820 loss: 146.808 val loss: 111.707\n",
            "epoch: 1821 loss: 89.611 val loss: 111.668\n",
            "epoch: 1822 loss: 136.004 val loss: 115.586\n",
            "epoch: 1823 loss: 92.628 val loss: 113.969\n",
            "epoch: 1824 loss: 136.690 val loss: 113.674\n",
            "epoch: 1825 loss: 84.608 val loss: 112.330\n",
            "epoch: 1826 loss: 94.775 val loss: 110.901\n",
            "epoch: 1827 loss: 118.818 val loss: 112.955\n",
            "epoch: 1828 loss: 98.586 val loss: 112.124\n",
            "epoch: 1829 loss: 85.449 val loss: 112.400\n",
            "epoch: 1830 loss: 115.050 val loss: 114.129\n",
            "epoch: 1831 loss: 138.153 val loss: 114.062\n",
            "epoch: 1832 loss: 163.955 val loss: 112.159\n",
            "epoch: 1833 loss: 150.991 val loss: 112.688\n",
            "epoch: 1834 loss: 117.204 val loss: 114.917\n",
            "epoch: 1835 loss: 105.733 val loss: 115.220\n",
            "epoch: 1836 loss: 93.674 val loss: 112.124\n",
            "epoch: 1837 loss: 62.889 val loss: 111.205\n",
            "epoch: 1838 loss: 99.896 val loss: 115.057\n",
            "epoch: 1839 loss: 102.007 val loss: 112.671\n",
            "epoch: 1840 loss: 114.572 val loss: 112.170\n",
            "epoch: 1841 loss: 106.108 val loss: 115.234\n",
            "epoch: 1842 loss: 133.024 val loss: 114.645\n",
            "epoch: 1843 loss: 95.411 val loss: 114.319\n",
            "epoch: 1844 loss: 124.806 val loss: 115.430\n",
            "epoch: 1845 loss: 129.692 val loss: 113.860\n",
            "epoch: 1846 loss: 127.298 val loss: 111.391\n",
            "epoch: 1847 loss: 136.337 val loss: 115.198\n",
            "epoch: 1848 loss: 91.146 val loss: 113.271\n",
            "epoch: 1849 loss: 103.613 val loss: 114.477\n",
            "epoch: 1850 loss: 82.945 val loss: 114.642\n",
            "epoch: 1851 loss: 92.029 val loss: 112.413\n",
            "epoch: 1852 loss: 134.530 val loss: 111.078\n",
            "epoch: 1853 loss: 153.567 val loss: 115.327\n",
            "epoch: 1854 loss: 101.655 val loss: 113.968\n",
            "epoch: 1855 loss: 101.465 val loss: 113.246\n",
            "epoch: 1856 loss: 104.263 val loss: 114.715\n",
            "epoch: 1857 loss: 94.515 val loss: 111.523\n",
            "epoch: 1858 loss: 127.315 val loss: 112.199\n",
            "epoch: 1859 loss: 92.184 val loss: 112.495\n",
            "epoch: 1860 loss: 106.834 val loss: 112.923\n",
            "epoch: 1861 loss: 82.895 val loss: 113.698\n",
            "epoch: 1862 loss: 126.950 val loss: 110.553\n",
            "epoch: 1863 loss: 122.811 val loss: 111.437\n",
            "epoch: 1864 loss: 73.048 val loss: 114.178\n",
            "epoch: 1865 loss: 134.498 val loss: 112.836\n",
            "epoch: 1866 loss: 94.575 val loss: 115.113\n",
            "epoch: 1867 loss: 109.633 val loss: 115.462\n",
            "epoch: 1868 loss: 105.184 val loss: 113.341\n",
            "epoch: 1869 loss: 126.502 val loss: 113.573\n",
            "epoch: 1870 loss: 129.510 val loss: 114.437\n",
            "epoch: 1871 loss: 100.059 val loss: 113.419\n",
            "epoch: 1872 loss: 136.847 val loss: 113.057\n",
            "epoch: 1873 loss: 117.481 val loss: 111.955\n",
            "epoch: 1874 loss: 109.728 val loss: 112.118\n",
            "epoch: 1875 loss: 118.118 val loss: 111.387\n",
            "epoch: 1876 loss: 77.886 val loss: 112.924\n",
            "epoch: 1877 loss: 106.450 val loss: 112.370\n",
            "epoch: 1878 loss: 86.669 val loss: 113.129\n",
            "epoch: 1879 loss: 127.673 val loss: 113.493\n",
            "epoch: 1880 loss: 163.580 val loss: 113.253\n",
            "epoch: 1881 loss: 111.901 val loss: 117.088\n",
            "epoch: 1882 loss: 158.910 val loss: 112.813\n",
            "epoch: 1883 loss: 138.887 val loss: 114.497\n",
            "epoch: 1884 loss: 114.205 val loss: 114.694\n",
            "epoch: 1885 loss: 106.859 val loss: 112.064\n",
            "epoch: 1886 loss: 127.755 val loss: 113.151\n",
            "epoch: 1887 loss: 89.145 val loss: 111.135\n",
            "epoch: 1888 loss: 98.950 val loss: 112.401\n",
            "epoch: 1889 loss: 109.463 val loss: 113.826\n",
            "epoch: 1890 loss: 80.607 val loss: 112.964\n",
            "epoch: 1891 loss: 101.049 val loss: 111.366\n",
            "epoch: 1892 loss: 84.746 val loss: 111.864\n",
            "epoch: 1893 loss: 67.144 val loss: 116.087\n",
            "epoch: 1894 loss: 152.566 val loss: 112.293\n",
            "epoch: 1895 loss: 121.685 val loss: 111.618\n",
            "epoch: 1896 loss: 79.030 val loss: 113.888\n",
            "epoch: 1897 loss: 115.473 val loss: 113.794\n",
            "epoch: 1898 loss: 87.441 val loss: 110.863\n",
            "epoch: 1899 loss: 134.193 val loss: 114.855\n",
            "epoch: 1900 loss: 84.156 val loss: 111.365\n",
            "epoch: 1901 loss: 84.631 val loss: 113.600\n",
            "epoch: 1902 loss: 112.291 val loss: 112.727\n",
            "epoch: 1903 loss: 155.418 val loss: 111.631\n",
            "epoch: 1904 loss: 141.181 val loss: 113.562\n",
            "epoch: 1905 loss: 131.192 val loss: 113.992\n",
            "epoch: 1906 loss: 71.447 val loss: 112.909\n",
            "epoch: 1907 loss: 155.762 val loss: 111.372\n",
            "epoch: 1908 loss: 94.611 val loss: 114.906\n",
            "epoch: 1909 loss: 95.923 val loss: 113.180\n",
            "epoch: 1910 loss: 113.674 val loss: 111.461\n",
            "epoch: 1911 loss: 108.117 val loss: 112.179\n",
            "epoch: 1912 loss: 118.361 val loss: 113.952\n",
            "epoch: 1913 loss: 118.642 val loss: 111.976\n",
            "epoch: 1914 loss: 84.693 val loss: 111.813\n",
            "epoch: 1915 loss: 98.572 val loss: 111.715\n",
            "epoch: 1916 loss: 139.751 val loss: 113.437\n",
            "epoch: 1917 loss: 94.843 val loss: 112.461\n",
            "epoch: 1918 loss: 132.466 val loss: 112.512\n",
            "epoch: 1919 loss: 135.654 val loss: 111.973\n",
            "epoch: 1920 loss: 128.134 val loss: 113.083\n",
            "epoch: 1921 loss: 130.238 val loss: 113.264\n",
            "epoch: 1922 loss: 97.703 val loss: 115.889\n",
            "epoch: 1923 loss: 131.673 val loss: 111.660\n",
            "epoch: 1924 loss: 101.081 val loss: 114.496\n",
            "epoch: 1925 loss: 105.448 val loss: 112.241\n",
            "epoch: 1926 loss: 137.815 val loss: 112.341\n",
            "epoch: 1927 loss: 63.739 val loss: 110.604\n",
            "epoch: 1928 loss: 120.923 val loss: 112.822\n",
            "epoch: 1929 loss: 131.186 val loss: 114.396\n",
            "epoch: 1930 loss: 88.456 val loss: 109.910\n",
            "epoch: 1931 loss: 105.820 val loss: 112.874\n",
            "epoch: 1932 loss: 97.469 val loss: 112.643\n",
            "epoch: 1933 loss: 92.644 val loss: 115.633\n",
            "epoch: 1934 loss: 125.803 val loss: 112.893\n",
            "epoch: 1935 loss: 114.728 val loss: 113.770\n",
            "epoch: 1936 loss: 135.611 val loss: 113.483\n",
            "epoch: 1937 loss: 95.484 val loss: 114.125\n",
            "epoch: 1938 loss: 94.155 val loss: 111.777\n",
            "epoch: 1939 loss: 139.451 val loss: 112.552\n",
            "epoch: 1940 loss: 127.532 val loss: 112.190\n",
            "epoch: 1941 loss: 117.434 val loss: 110.926\n",
            "epoch: 1942 loss: 167.350 val loss: 113.848\n",
            "epoch: 1943 loss: 76.497 val loss: 114.180\n",
            "epoch: 1944 loss: 86.989 val loss: 113.420\n",
            "epoch: 1945 loss: 101.747 val loss: 113.557\n",
            "epoch: 1946 loss: 85.506 val loss: 115.397\n",
            "epoch: 1947 loss: 110.571 val loss: 112.850\n",
            "epoch: 1948 loss: 112.671 val loss: 110.204\n",
            "epoch: 1949 loss: 140.465 val loss: 111.948\n",
            "epoch: 1950 loss: 75.745 val loss: 113.595\n",
            "epoch: 1951 loss: 134.852 val loss: 111.652\n",
            "epoch: 1952 loss: 111.411 val loss: 112.312\n",
            "epoch: 1953 loss: 106.491 val loss: 111.942\n",
            "epoch: 1954 loss: 106.076 val loss: 112.175\n",
            "epoch: 1955 loss: 122.928 val loss: 114.121\n",
            "epoch: 1956 loss: 90.667 val loss: 110.744\n",
            "epoch: 1957 loss: 117.624 val loss: 114.216\n",
            "epoch: 1958 loss: 101.348 val loss: 111.803\n",
            "epoch: 1959 loss: 111.566 val loss: 113.106\n",
            "epoch: 1960 loss: 104.452 val loss: 112.309\n",
            "epoch: 1961 loss: 133.810 val loss: 113.365\n",
            "epoch: 1962 loss: 110.415 val loss: 111.883\n",
            "epoch: 1963 loss: 92.866 val loss: 111.248\n",
            "epoch: 1964 loss: 193.001 val loss: 112.775\n",
            "epoch: 1965 loss: 96.661 val loss: 114.102\n",
            "epoch: 1966 loss: 157.183 val loss: 115.331\n",
            "epoch: 1967 loss: 114.333 val loss: 113.345\n",
            "epoch: 1968 loss: 79.515 val loss: 110.848\n",
            "epoch: 1969 loss: 97.033 val loss: 113.926\n",
            "epoch: 1970 loss: 110.842 val loss: 115.136\n",
            "epoch: 1971 loss: 107.422 val loss: 113.364\n",
            "epoch: 1972 loss: 78.681 val loss: 111.685\n",
            "epoch: 1973 loss: 129.168 val loss: 111.746\n",
            "epoch: 1974 loss: 91.550 val loss: 112.415\n",
            "epoch: 1975 loss: 114.276 val loss: 116.878\n",
            "epoch: 1976 loss: 130.054 val loss: 116.329\n",
            "epoch: 1977 loss: 101.278 val loss: 110.661\n",
            "epoch: 1978 loss: 151.563 val loss: 112.255\n",
            "epoch: 1979 loss: 93.511 val loss: 112.260\n",
            "epoch: 1980 loss: 92.011 val loss: 112.139\n",
            "epoch: 1981 loss: 133.546 val loss: 115.467\n",
            "epoch: 1982 loss: 67.944 val loss: 112.440\n",
            "epoch: 1983 loss: 100.412 val loss: 113.174\n",
            "epoch: 1984 loss: 128.974 val loss: 113.726\n",
            "epoch: 1985 loss: 108.560 val loss: 112.763\n",
            "epoch: 1986 loss: 103.923 val loss: 112.266\n",
            "epoch: 1987 loss: 97.893 val loss: 111.257\n",
            "epoch: 1988 loss: 77.595 val loss: 111.851\n",
            "epoch: 1989 loss: 94.047 val loss: 111.391\n",
            "epoch: 1990 loss: 150.995 val loss: 113.197\n",
            "epoch: 1991 loss: 117.717 val loss: 112.855\n",
            "epoch: 1992 loss: 100.861 val loss: 112.889\n",
            "epoch: 1993 loss: 114.237 val loss: 113.200\n",
            "epoch: 1994 loss: 105.383 val loss: 113.530\n",
            "epoch: 1995 loss: 74.681 val loss: 116.259\n",
            "epoch: 1996 loss: 108.974 val loss: 111.629\n",
            "epoch: 1997 loss: 84.616 val loss: 111.736\n",
            "epoch: 1998 loss: 103.257 val loss: 112.950\n",
            "epoch: 1999 loss: 77.856 val loss: 113.110\n",
            "epoch: 2000 loss: 90.525 val loss: 110.919\n",
            "epoch: 2001 loss: 101.599 val loss: 112.970\n",
            "epoch: 2002 loss: 81.147 val loss: 111.257\n",
            "epoch: 2003 loss: 136.462 val loss: 114.144\n",
            "epoch: 2004 loss: 92.833 val loss: 115.073\n",
            "epoch: 2005 loss: 110.193 val loss: 111.905\n",
            "epoch: 2006 loss: 110.295 val loss: 111.954\n",
            "epoch: 2007 loss: 93.963 val loss: 116.225\n",
            "epoch: 2008 loss: 125.439 val loss: 112.496\n",
            "epoch: 2009 loss: 127.879 val loss: 115.063\n",
            "epoch: 2010 loss: 130.793 val loss: 112.246\n",
            "epoch: 2011 loss: 124.995 val loss: 112.684\n",
            "epoch: 2012 loss: 103.798 val loss: 112.998\n",
            "epoch: 2013 loss: 96.725 val loss: 113.643\n",
            "epoch: 2014 loss: 160.365 val loss: 113.529\n",
            "epoch: 2015 loss: 88.538 val loss: 114.189\n",
            "epoch: 2016 loss: 124.455 val loss: 110.893\n",
            "epoch: 2017 loss: 126.180 val loss: 113.762\n",
            "epoch: 2018 loss: 144.742 val loss: 111.857\n",
            "epoch: 2019 loss: 125.233 val loss: 111.783\n",
            "epoch: 2020 loss: 102.367 val loss: 111.084\n",
            "epoch: 2021 loss: 73.253 val loss: 113.263\n",
            "epoch: 2022 loss: 99.455 val loss: 113.287\n",
            "epoch: 2023 loss: 79.903 val loss: 113.024\n",
            "epoch: 2024 loss: 103.046 val loss: 112.945\n",
            "epoch: 2025 loss: 121.393 val loss: 112.075\n",
            "epoch: 2026 loss: 112.812 val loss: 112.764\n",
            "epoch: 2027 loss: 86.152 val loss: 111.731\n",
            "epoch: 2028 loss: 112.598 val loss: 111.915\n",
            "epoch: 2029 loss: 101.120 val loss: 111.937\n",
            "epoch: 2030 loss: 138.289 val loss: 112.810\n",
            "epoch: 2031 loss: 92.165 val loss: 113.966\n",
            "epoch: 2032 loss: 111.015 val loss: 114.250\n",
            "epoch: 2033 loss: 104.147 val loss: 112.835\n",
            "epoch: 2034 loss: 95.869 val loss: 111.035\n",
            "epoch: 2035 loss: 111.128 val loss: 111.414\n",
            "epoch: 2036 loss: 75.329 val loss: 110.809\n",
            "epoch: 2037 loss: 106.789 val loss: 112.727\n",
            "epoch: 2038 loss: 76.636 val loss: 113.799\n",
            "epoch: 2039 loss: 120.120 val loss: 112.172\n",
            "epoch: 2040 loss: 94.303 val loss: 111.521\n",
            "epoch: 2041 loss: 108.668 val loss: 110.741\n",
            "epoch: 2042 loss: 100.398 val loss: 111.386\n",
            "epoch: 2043 loss: 82.541 val loss: 112.707\n",
            "epoch: 2044 loss: 155.363 val loss: 115.449\n",
            "epoch: 2045 loss: 134.677 val loss: 111.516\n",
            "epoch: 2046 loss: 177.052 val loss: 112.831\n",
            "epoch: 2047 loss: 172.900 val loss: 114.045\n",
            "epoch: 2048 loss: 117.199 val loss: 111.624\n",
            "epoch: 2049 loss: 105.929 val loss: 110.825\n",
            "epoch: 2050 loss: 131.017 val loss: 112.625\n",
            "epoch: 2051 loss: 108.379 val loss: 112.299\n",
            "epoch: 2052 loss: 82.679 val loss: 114.839\n",
            "epoch: 2053 loss: 88.023 val loss: 111.889\n",
            "epoch: 2054 loss: 118.391 val loss: 110.947\n",
            "epoch: 2055 loss: 73.477 val loss: 113.862\n",
            "epoch: 2056 loss: 119.769 val loss: 115.301\n",
            "epoch: 2057 loss: 119.766 val loss: 115.918\n",
            "epoch: 2058 loss: 99.315 val loss: 115.625\n",
            "epoch: 2059 loss: 101.757 val loss: 111.484\n",
            "epoch: 2060 loss: 92.876 val loss: 111.407\n",
            "epoch: 2061 loss: 112.389 val loss: 113.239\n",
            "epoch: 2062 loss: 132.920 val loss: 115.789\n",
            "epoch: 2063 loss: 161.311 val loss: 111.233\n",
            "epoch: 2064 loss: 125.907 val loss: 113.900\n",
            "epoch: 2065 loss: 92.158 val loss: 114.930\n",
            "epoch: 2066 loss: 107.533 val loss: 112.312\n",
            "epoch: 2067 loss: 81.374 val loss: 111.982\n",
            "epoch: 2068 loss: 109.425 val loss: 113.444\n",
            "epoch: 2069 loss: 167.773 val loss: 112.675\n",
            "epoch: 2070 loss: 113.580 val loss: 113.822\n",
            "epoch: 2071 loss: 145.909 val loss: 112.424\n",
            "epoch: 2072 loss: 128.512 val loss: 114.063\n",
            "epoch: 2073 loss: 117.661 val loss: 114.374\n",
            "epoch: 2074 loss: 92.344 val loss: 114.537\n",
            "epoch: 2075 loss: 97.897 val loss: 114.927\n",
            "epoch: 2076 loss: 77.758 val loss: 112.873\n",
            "epoch: 2077 loss: 83.425 val loss: 111.747\n",
            "epoch: 2078 loss: 159.424 val loss: 114.205\n",
            "epoch: 2079 loss: 100.738 val loss: 112.075\n",
            "epoch: 2080 loss: 132.329 val loss: 112.203\n",
            "epoch: 2081 loss: 105.481 val loss: 112.449\n",
            "epoch: 2082 loss: 74.252 val loss: 114.571\n",
            "epoch: 2083 loss: 132.175 val loss: 112.877\n",
            "epoch: 2084 loss: 129.431 val loss: 111.780\n",
            "epoch: 2085 loss: 123.468 val loss: 111.469\n",
            "epoch: 2086 loss: 92.280 val loss: 114.618\n",
            "epoch: 2087 loss: 120.415 val loss: 111.533\n",
            "epoch: 2088 loss: 113.696 val loss: 113.023\n",
            "epoch: 2089 loss: 117.958 val loss: 113.785\n",
            "epoch: 2090 loss: 121.552 val loss: 111.872\n",
            "epoch: 2091 loss: 131.793 val loss: 114.422\n",
            "epoch: 2092 loss: 127.421 val loss: 111.252\n",
            "epoch: 2093 loss: 104.719 val loss: 113.650\n",
            "epoch: 2094 loss: 148.786 val loss: 113.798\n",
            "epoch: 2095 loss: 114.798 val loss: 113.259\n",
            "epoch: 2096 loss: 95.950 val loss: 112.582\n",
            "epoch: 2097 loss: 122.461 val loss: 115.687\n",
            "epoch: 2098 loss: 125.703 val loss: 112.757\n",
            "epoch: 2099 loss: 96.759 val loss: 111.184\n",
            "epoch: 2100 loss: 103.132 val loss: 113.358\n",
            "epoch: 2101 loss: 79.530 val loss: 113.933\n",
            "epoch: 2102 loss: 93.236 val loss: 112.082\n",
            "epoch: 2103 loss: 137.716 val loss: 112.136\n",
            "epoch: 2104 loss: 104.397 val loss: 111.343\n",
            "epoch: 2105 loss: 117.882 val loss: 110.190\n",
            "epoch: 2106 loss: 101.579 val loss: 111.939\n",
            "epoch: 2107 loss: 106.293 val loss: 114.473\n",
            "epoch: 2108 loss: 133.555 val loss: 115.713\n",
            "epoch: 2109 loss: 117.646 val loss: 114.084\n",
            "epoch: 2110 loss: 174.608 val loss: 113.428\n",
            "epoch: 2111 loss: 94.533 val loss: 112.819\n",
            "epoch: 2112 loss: 147.965 val loss: 110.399\n",
            "epoch: 2113 loss: 96.563 val loss: 112.300\n",
            "epoch: 2114 loss: 110.426 val loss: 111.680\n",
            "epoch: 2115 loss: 129.871 val loss: 113.034\n",
            "epoch: 2116 loss: 135.535 val loss: 113.847\n",
            "epoch: 2117 loss: 127.216 val loss: 111.773\n",
            "epoch: 2118 loss: 125.970 val loss: 112.979\n",
            "epoch: 2119 loss: 152.909 val loss: 113.359\n",
            "epoch: 2120 loss: 82.322 val loss: 110.816\n",
            "epoch: 2121 loss: 111.764 val loss: 111.801\n",
            "epoch: 2122 loss: 140.201 val loss: 115.005\n",
            "epoch: 2123 loss: 97.982 val loss: 110.606\n",
            "epoch: 2124 loss: 89.301 val loss: 111.099\n",
            "epoch: 2125 loss: 180.133 val loss: 113.189\n",
            "epoch: 2126 loss: 113.489 val loss: 115.873\n",
            "epoch: 2127 loss: 89.613 val loss: 111.533\n",
            "epoch: 2128 loss: 112.799 val loss: 111.947\n",
            "epoch: 2129 loss: 117.491 val loss: 111.681\n",
            "epoch: 2130 loss: 117.152 val loss: 112.379\n",
            "epoch: 2131 loss: 129.771 val loss: 111.349\n",
            "epoch: 2132 loss: 113.540 val loss: 114.563\n",
            "epoch: 2133 loss: 101.120 val loss: 113.998\n",
            "epoch: 2134 loss: 106.457 val loss: 111.876\n",
            "epoch: 2135 loss: 85.296 val loss: 113.959\n",
            "epoch: 2136 loss: 96.231 val loss: 111.436\n",
            "epoch: 2137 loss: 90.067 val loss: 113.125\n",
            "epoch: 2138 loss: 96.278 val loss: 112.226\n",
            "epoch: 2139 loss: 108.056 val loss: 112.587\n",
            "epoch: 2140 loss: 106.004 val loss: 111.851\n",
            "epoch: 2141 loss: 105.188 val loss: 111.451\n",
            "epoch: 2142 loss: 113.294 val loss: 111.699\n",
            "epoch: 2143 loss: 112.589 val loss: 112.199\n",
            "epoch: 2144 loss: 90.586 val loss: 115.753\n",
            "epoch: 2145 loss: 93.962 val loss: 112.449\n",
            "epoch: 2146 loss: 80.386 val loss: 112.644\n",
            "epoch: 2147 loss: 119.780 val loss: 111.029\n",
            "epoch: 2148 loss: 91.507 val loss: 112.503\n",
            "epoch: 2149 loss: 116.423 val loss: 111.577\n",
            "epoch: 2150 loss: 80.955 val loss: 116.694\n",
            "epoch: 2151 loss: 86.139 val loss: 112.533\n",
            "epoch: 2152 loss: 70.893 val loss: 113.269\n",
            "epoch: 2153 loss: 85.988 val loss: 114.782\n",
            "epoch: 2154 loss: 121.538 val loss: 113.041\n",
            "epoch: 2155 loss: 104.521 val loss: 113.210\n",
            "epoch: 2156 loss: 77.888 val loss: 112.425\n",
            "epoch: 2157 loss: 85.696 val loss: 110.575\n",
            "epoch: 2158 loss: 118.798 val loss: 111.917\n",
            "epoch: 2159 loss: 93.609 val loss: 112.453\n",
            "epoch: 2160 loss: 83.951 val loss: 111.571\n",
            "epoch: 2161 loss: 103.786 val loss: 113.194\n",
            "epoch: 2162 loss: 128.435 val loss: 115.031\n",
            "epoch: 2163 loss: 92.903 val loss: 112.642\n",
            "epoch: 2164 loss: 139.859 val loss: 111.383\n",
            "epoch: 2165 loss: 109.005 val loss: 110.548\n",
            "epoch: 2166 loss: 125.509 val loss: 110.763\n",
            "epoch: 2167 loss: 135.882 val loss: 114.617\n",
            "epoch: 2168 loss: 78.695 val loss: 112.582\n",
            "epoch: 2169 loss: 103.364 val loss: 113.759\n",
            "epoch: 2170 loss: 112.475 val loss: 115.850\n",
            "epoch: 2171 loss: 89.573 val loss: 113.696\n",
            "epoch: 2172 loss: 83.278 val loss: 112.783\n",
            "epoch: 2173 loss: 87.129 val loss: 112.734\n",
            "epoch: 2174 loss: 107.065 val loss: 112.613\n",
            "epoch: 2175 loss: 133.616 val loss: 114.318\n",
            "epoch: 2176 loss: 118.669 val loss: 112.727\n",
            "epoch: 2177 loss: 125.319 val loss: 110.614\n",
            "epoch: 2178 loss: 68.064 val loss: 113.711\n",
            "epoch: 2179 loss: 123.857 val loss: 114.898\n",
            "epoch: 2180 loss: 107.889 val loss: 114.315\n",
            "epoch: 2181 loss: 123.381 val loss: 114.331\n",
            "epoch: 2182 loss: 122.646 val loss: 113.780\n",
            "epoch: 2183 loss: 137.742 val loss: 111.349\n",
            "epoch: 2184 loss: 93.671 val loss: 112.202\n",
            "epoch: 2185 loss: 142.930 val loss: 112.963\n",
            "epoch: 2186 loss: 133.993 val loss: 113.769\n",
            "epoch: 2187 loss: 102.578 val loss: 112.635\n",
            "epoch: 2188 loss: 109.478 val loss: 115.139\n",
            "epoch: 2189 loss: 106.066 val loss: 112.027\n",
            "epoch: 2190 loss: 111.262 val loss: 112.118\n",
            "epoch: 2191 loss: 98.702 val loss: 112.381\n",
            "epoch: 2192 loss: 97.168 val loss: 112.621\n",
            "epoch: 2193 loss: 95.535 val loss: 112.139\n",
            "epoch: 2194 loss: 144.913 val loss: 113.704\n",
            "epoch: 2195 loss: 91.904 val loss: 114.452\n",
            "epoch: 2196 loss: 127.521 val loss: 112.573\n",
            "epoch: 2197 loss: 113.908 val loss: 112.367\n",
            "epoch: 2198 loss: 116.097 val loss: 112.426\n",
            "epoch: 2199 loss: 96.477 val loss: 112.065\n",
            "epoch: 2200 loss: 111.133 val loss: 113.929\n",
            "epoch: 2201 loss: 94.007 val loss: 114.226\n",
            "epoch: 2202 loss: 71.869 val loss: 112.254\n",
            "epoch: 2203 loss: 89.894 val loss: 114.899\n",
            "epoch: 2204 loss: 112.217 val loss: 112.284\n",
            "epoch: 2205 loss: 95.947 val loss: 111.454\n",
            "epoch: 2206 loss: 120.019 val loss: 112.900\n",
            "epoch: 2207 loss: 88.213 val loss: 111.713\n",
            "epoch: 2208 loss: 76.310 val loss: 113.341\n",
            "epoch: 2209 loss: 97.127 val loss: 112.785\n",
            "epoch: 2210 loss: 119.348 val loss: 111.874\n",
            "epoch: 2211 loss: 153.146 val loss: 113.231\n",
            "epoch: 2212 loss: 140.594 val loss: 113.175\n",
            "epoch: 2213 loss: 98.661 val loss: 111.361\n",
            "epoch: 2214 loss: 121.757 val loss: 111.266\n",
            "epoch: 2215 loss: 108.852 val loss: 113.284\n",
            "epoch: 2216 loss: 89.516 val loss: 111.396\n",
            "epoch: 2217 loss: 134.715 val loss: 113.445\n",
            "epoch: 2218 loss: 142.271 val loss: 113.199\n",
            "epoch: 2219 loss: 101.333 val loss: 112.380\n",
            "epoch: 2220 loss: 105.116 val loss: 112.731\n",
            "epoch: 2221 loss: 132.022 val loss: 115.732\n",
            "epoch: 2222 loss: 134.378 val loss: 113.250\n",
            "epoch: 2223 loss: 115.041 val loss: 111.260\n",
            "epoch: 2224 loss: 93.522 val loss: 114.725\n",
            "epoch: 2225 loss: 129.197 val loss: 113.258\n",
            "epoch: 2226 loss: 125.419 val loss: 113.126\n",
            "epoch: 2227 loss: 108.823 val loss: 114.550\n",
            "epoch: 2228 loss: 111.643 val loss: 114.018\n",
            "epoch: 2229 loss: 153.790 val loss: 113.707\n",
            "epoch: 2230 loss: 121.198 val loss: 112.302\n",
            "epoch: 2231 loss: 133.575 val loss: 114.568\n",
            "epoch: 2232 loss: 82.893 val loss: 114.776\n",
            "epoch: 2233 loss: 95.666 val loss: 112.843\n",
            "epoch: 2234 loss: 117.713 val loss: 113.672\n",
            "epoch: 2235 loss: 110.737 val loss: 112.123\n",
            "epoch: 2236 loss: 153.709 val loss: 111.406\n",
            "epoch: 2237 loss: 126.414 val loss: 113.766\n",
            "epoch: 2238 loss: 98.456 val loss: 112.937\n",
            "epoch: 2239 loss: 87.611 val loss: 113.595\n",
            "epoch: 2240 loss: 86.477 val loss: 115.741\n",
            "epoch: 2241 loss: 99.435 val loss: 111.155\n",
            "epoch: 2242 loss: 118.035 val loss: 111.170\n",
            "epoch: 2243 loss: 126.757 val loss: 113.150\n",
            "epoch: 2244 loss: 84.169 val loss: 112.661\n",
            "epoch: 2245 loss: 94.847 val loss: 110.583\n",
            "epoch: 2246 loss: 166.275 val loss: 111.310\n",
            "epoch: 2247 loss: 112.763 val loss: 112.696\n",
            "epoch: 2248 loss: 139.693 val loss: 113.792\n",
            "epoch: 2249 loss: 130.927 val loss: 110.926\n",
            "epoch: 2250 loss: 121.042 val loss: 111.087\n",
            "epoch: 2251 loss: 106.168 val loss: 112.223\n",
            "epoch: 2252 loss: 146.742 val loss: 114.419\n",
            "epoch: 2253 loss: 94.575 val loss: 111.962\n",
            "epoch: 2254 loss: 106.677 val loss: 113.523\n",
            "epoch: 2255 loss: 98.607 val loss: 111.475\n",
            "epoch: 2256 loss: 107.260 val loss: 114.543\n",
            "epoch: 2257 loss: 142.082 val loss: 111.638\n",
            "epoch: 2258 loss: 121.971 val loss: 112.851\n",
            "epoch: 2259 loss: 78.087 val loss: 113.417\n",
            "epoch: 2260 loss: 110.557 val loss: 111.320\n",
            "epoch: 2261 loss: 136.431 val loss: 113.204\n",
            "epoch: 2262 loss: 105.746 val loss: 112.521\n",
            "epoch: 2263 loss: 115.776 val loss: 111.103\n",
            "epoch: 2264 loss: 96.137 val loss: 113.801\n",
            "epoch: 2265 loss: 126.370 val loss: 112.805\n",
            "epoch: 2266 loss: 86.012 val loss: 112.220\n",
            "epoch: 2267 loss: 127.278 val loss: 114.483\n",
            "epoch: 2268 loss: 64.335 val loss: 112.955\n",
            "epoch: 2269 loss: 174.658 val loss: 113.649\n",
            "epoch: 2270 loss: 85.732 val loss: 113.162\n",
            "epoch: 2271 loss: 100.506 val loss: 112.473\n",
            "epoch: 2272 loss: 124.842 val loss: 114.022\n",
            "epoch: 2273 loss: 79.508 val loss: 113.241\n",
            "epoch: 2274 loss: 138.908 val loss: 111.012\n",
            "epoch: 2275 loss: 95.487 val loss: 110.011\n",
            "epoch: 2276 loss: 119.271 val loss: 113.501\n",
            "epoch: 2277 loss: 120.572 val loss: 109.931\n",
            "epoch: 2278 loss: 115.524 val loss: 112.458\n",
            "epoch: 2279 loss: 117.194 val loss: 112.459\n",
            "epoch: 2280 loss: 126.495 val loss: 114.777\n",
            "epoch: 2281 loss: 109.359 val loss: 112.007\n",
            "epoch: 2282 loss: 121.752 val loss: 114.557\n",
            "epoch: 2283 loss: 122.270 val loss: 112.795\n",
            "epoch: 2284 loss: 161.150 val loss: 111.307\n",
            "epoch: 2285 loss: 115.184 val loss: 112.708\n",
            "epoch: 2286 loss: 112.468 val loss: 113.074\n",
            "epoch: 2287 loss: 97.327 val loss: 113.594\n",
            "epoch: 2288 loss: 100.867 val loss: 111.249\n",
            "epoch: 2289 loss: 119.000 val loss: 112.436\n",
            "epoch: 2290 loss: 151.583 val loss: 111.495\n",
            "epoch: 2291 loss: 105.332 val loss: 113.237\n",
            "epoch: 2292 loss: 193.160 val loss: 112.417\n",
            "epoch: 2293 loss: 117.529 val loss: 113.732\n",
            "epoch: 2294 loss: 81.711 val loss: 113.440\n",
            "epoch: 2295 loss: 91.870 val loss: 110.519\n",
            "epoch: 2296 loss: 100.684 val loss: 112.163\n",
            "epoch: 2297 loss: 158.024 val loss: 112.364\n",
            "epoch: 2298 loss: 150.613 val loss: 113.579\n",
            "epoch: 2299 loss: 146.182 val loss: 112.479\n",
            "epoch: 2300 loss: 112.934 val loss: 113.478\n",
            "epoch: 2301 loss: 134.106 val loss: 113.965\n",
            "epoch: 2302 loss: 125.901 val loss: 110.853\n",
            "epoch: 2303 loss: 148.534 val loss: 112.316\n",
            "epoch: 2304 loss: 118.145 val loss: 112.157\n",
            "epoch: 2305 loss: 128.083 val loss: 113.314\n",
            "epoch: 2306 loss: 101.030 val loss: 114.717\n",
            "epoch: 2307 loss: 92.746 val loss: 115.021\n",
            "epoch: 2308 loss: 101.980 val loss: 114.032\n",
            "epoch: 2309 loss: 128.643 val loss: 113.444\n",
            "epoch: 2310 loss: 158.414 val loss: 110.156\n",
            "epoch: 2311 loss: 104.742 val loss: 114.119\n",
            "epoch: 2312 loss: 121.286 val loss: 112.674\n",
            "epoch: 2313 loss: 86.221 val loss: 111.538\n",
            "epoch: 2314 loss: 106.581 val loss: 112.714\n",
            "epoch: 2315 loss: 94.544 val loss: 114.666\n",
            "epoch: 2316 loss: 124.679 val loss: 113.971\n",
            "epoch: 2317 loss: 80.928 val loss: 112.604\n",
            "epoch: 2318 loss: 92.708 val loss: 113.863\n",
            "epoch: 2319 loss: 123.773 val loss: 113.393\n",
            "epoch: 2320 loss: 115.696 val loss: 114.231\n",
            "epoch: 2321 loss: 73.235 val loss: 111.756\n",
            "epoch: 2322 loss: 113.748 val loss: 111.870\n",
            "epoch: 2323 loss: 109.391 val loss: 113.100\n",
            "epoch: 2324 loss: 104.810 val loss: 114.241\n",
            "epoch: 2325 loss: 102.245 val loss: 112.241\n",
            "epoch: 2326 loss: 126.384 val loss: 112.609\n",
            "epoch: 2327 loss: 96.685 val loss: 114.035\n",
            "epoch: 2328 loss: 104.574 val loss: 114.285\n",
            "epoch: 2329 loss: 117.762 val loss: 111.790\n",
            "epoch: 2330 loss: 143.962 val loss: 113.321\n",
            "epoch: 2331 loss: 123.224 val loss: 110.553\n",
            "epoch: 2332 loss: 78.332 val loss: 112.022\n",
            "epoch: 2333 loss: 102.984 val loss: 114.616\n",
            "epoch: 2334 loss: 102.980 val loss: 112.051\n",
            "epoch: 2335 loss: 121.151 val loss: 115.263\n",
            "epoch: 2336 loss: 118.550 val loss: 114.715\n",
            "epoch: 2337 loss: 104.624 val loss: 113.705\n",
            "epoch: 2338 loss: 85.587 val loss: 113.531\n",
            "epoch: 2339 loss: 105.418 val loss: 114.979\n",
            "epoch: 2340 loss: 127.376 val loss: 112.282\n",
            "epoch: 2341 loss: 122.077 val loss: 112.402\n",
            "epoch: 2342 loss: 135.629 val loss: 114.177\n",
            "epoch: 2343 loss: 135.088 val loss: 113.480\n",
            "epoch: 2344 loss: 104.376 val loss: 113.469\n",
            "epoch: 2345 loss: 98.093 val loss: 113.091\n",
            "epoch: 2346 loss: 132.092 val loss: 113.109\n",
            "epoch: 2347 loss: 123.590 val loss: 111.725\n",
            "epoch: 2348 loss: 91.946 val loss: 112.716\n",
            "epoch: 2349 loss: 138.962 val loss: 112.088\n",
            "epoch: 2350 loss: 96.474 val loss: 114.841\n",
            "epoch: 2351 loss: 152.829 val loss: 111.635\n",
            "epoch: 2352 loss: 103.054 val loss: 114.770\n",
            "epoch: 2353 loss: 134.466 val loss: 112.254\n",
            "epoch: 2354 loss: 163.512 val loss: 114.023\n",
            "epoch: 2355 loss: 76.974 val loss: 112.854\n",
            "epoch: 2356 loss: 147.766 val loss: 110.881\n",
            "epoch: 2357 loss: 102.727 val loss: 111.861\n",
            "epoch: 2358 loss: 108.942 val loss: 113.175\n",
            "epoch: 2359 loss: 125.056 val loss: 112.042\n",
            "epoch: 2360 loss: 101.543 val loss: 112.190\n",
            "epoch: 2361 loss: 86.237 val loss: 112.518\n",
            "epoch: 2362 loss: 124.631 val loss: 112.738\n",
            "epoch: 2363 loss: 125.880 val loss: 111.934\n",
            "epoch: 2364 loss: 91.343 val loss: 114.539\n",
            "epoch: 2365 loss: 127.129 val loss: 113.944\n",
            "epoch: 2366 loss: 82.949 val loss: 113.781\n",
            "epoch: 2367 loss: 73.775 val loss: 112.040\n",
            "epoch: 2368 loss: 102.130 val loss: 113.483\n",
            "epoch: 2369 loss: 108.214 val loss: 112.634\n",
            "epoch: 2370 loss: 121.944 val loss: 112.530\n",
            "epoch: 2371 loss: 72.598 val loss: 111.199\n",
            "epoch: 2372 loss: 70.090 val loss: 113.743\n",
            "epoch: 2373 loss: 105.911 val loss: 112.439\n",
            "epoch: 2374 loss: 107.724 val loss: 113.288\n",
            "epoch: 2375 loss: 130.384 val loss: 110.739\n",
            "epoch: 2376 loss: 93.968 val loss: 115.882\n",
            "epoch: 2377 loss: 132.498 val loss: 114.233\n",
            "epoch: 2378 loss: 100.053 val loss: 113.644\n",
            "epoch: 2379 loss: 91.100 val loss: 112.781\n",
            "epoch: 2380 loss: 97.783 val loss: 111.649\n",
            "epoch: 2381 loss: 79.408 val loss: 111.860\n",
            "epoch: 2382 loss: 93.984 val loss: 111.139\n",
            "epoch: 2383 loss: 73.207 val loss: 112.822\n",
            "epoch: 2384 loss: 109.002 val loss: 113.018\n",
            "epoch: 2385 loss: 129.177 val loss: 115.526\n",
            "epoch: 2386 loss: 129.934 val loss: 112.259\n",
            "epoch: 2387 loss: 112.569 val loss: 113.631\n",
            "epoch: 2388 loss: 81.404 val loss: 114.235\n",
            "epoch: 2389 loss: 94.727 val loss: 117.220\n",
            "epoch: 2390 loss: 93.084 val loss: 114.403\n",
            "epoch: 2391 loss: 123.761 val loss: 113.588\n",
            "epoch: 2392 loss: 138.909 val loss: 112.045\n",
            "epoch: 2393 loss: 88.060 val loss: 115.046\n",
            "epoch: 2394 loss: 146.333 val loss: 114.907\n",
            "epoch: 2395 loss: 115.217 val loss: 115.315\n",
            "epoch: 2396 loss: 136.753 val loss: 112.653\n",
            "epoch: 2397 loss: 156.847 val loss: 115.250\n",
            "epoch: 2398 loss: 89.873 val loss: 112.773\n",
            "epoch: 2399 loss: 99.264 val loss: 110.852\n",
            "epoch: 2400 loss: 109.622 val loss: 110.878\n",
            "epoch: 2401 loss: 101.288 val loss: 113.243\n",
            "epoch: 2402 loss: 93.625 val loss: 113.458\n",
            "epoch: 2403 loss: 112.574 val loss: 111.297\n",
            "epoch: 2404 loss: 143.360 val loss: 112.941\n",
            "epoch: 2405 loss: 107.869 val loss: 114.206\n",
            "epoch: 2406 loss: 98.765 val loss: 111.705\n",
            "epoch: 2407 loss: 106.006 val loss: 112.122\n",
            "epoch: 2408 loss: 116.651 val loss: 111.180\n",
            "epoch: 2409 loss: 152.315 val loss: 113.822\n",
            "epoch: 2410 loss: 90.887 val loss: 111.260\n",
            "epoch: 2411 loss: 108.776 val loss: 113.849\n",
            "epoch: 2412 loss: 133.134 val loss: 111.075\n",
            "epoch: 2413 loss: 151.880 val loss: 114.052\n",
            "epoch: 2414 loss: 158.122 val loss: 114.087\n",
            "epoch: 2415 loss: 95.055 val loss: 111.620\n",
            "epoch: 2416 loss: 99.857 val loss: 111.397\n",
            "epoch: 2417 loss: 141.986 val loss: 111.801\n",
            "epoch: 2418 loss: 104.420 val loss: 112.038\n",
            "epoch: 2419 loss: 112.912 val loss: 113.323\n",
            "epoch: 2420 loss: 100.503 val loss: 112.385\n",
            "epoch: 2421 loss: 84.093 val loss: 111.533\n",
            "epoch: 2422 loss: 155.511 val loss: 112.783\n",
            "epoch: 2423 loss: 162.587 val loss: 111.594\n",
            "epoch: 2424 loss: 124.655 val loss: 112.312\n",
            "epoch: 2425 loss: 132.066 val loss: 111.827\n",
            "epoch: 2426 loss: 105.788 val loss: 110.725\n",
            "epoch: 2427 loss: 155.561 val loss: 109.904\n",
            "epoch: 2428 loss: 79.004 val loss: 112.738\n",
            "epoch: 2429 loss: 110.624 val loss: 115.024\n",
            "epoch: 2430 loss: 107.846 val loss: 112.870\n",
            "epoch: 2431 loss: 71.957 val loss: 111.145\n",
            "epoch: 2432 loss: 107.618 val loss: 114.034\n",
            "epoch: 2433 loss: 90.657 val loss: 112.887\n",
            "epoch: 2434 loss: 117.239 val loss: 112.190\n",
            "epoch: 2435 loss: 130.446 val loss: 113.501\n",
            "epoch: 2436 loss: 129.770 val loss: 114.213\n",
            "epoch: 2437 loss: 133.945 val loss: 111.250\n",
            "epoch: 2438 loss: 119.948 val loss: 114.219\n",
            "epoch: 2439 loss: 80.320 val loss: 112.271\n",
            "epoch: 2440 loss: 128.093 val loss: 112.245\n",
            "epoch: 2441 loss: 95.932 val loss: 111.769\n",
            "epoch: 2442 loss: 130.532 val loss: 110.566\n",
            "epoch: 2443 loss: 114.157 val loss: 113.698\n",
            "epoch: 2444 loss: 114.534 val loss: 111.197\n",
            "epoch: 2445 loss: 89.362 val loss: 113.026\n",
            "epoch: 2446 loss: 136.449 val loss: 112.958\n",
            "epoch: 2447 loss: 113.308 val loss: 112.111\n",
            "epoch: 2448 loss: 110.536 val loss: 111.943\n",
            "epoch: 2449 loss: 135.845 val loss: 112.188\n",
            "epoch: 2450 loss: 80.474 val loss: 113.751\n",
            "epoch: 2451 loss: 177.410 val loss: 113.320\n",
            "epoch: 2452 loss: 133.814 val loss: 111.664\n",
            "epoch: 2453 loss: 136.306 val loss: 113.380\n",
            "epoch: 2454 loss: 79.217 val loss: 112.656\n",
            "epoch: 2455 loss: 84.712 val loss: 113.574\n",
            "epoch: 2456 loss: 164.610 val loss: 113.992\n",
            "epoch: 2457 loss: 120.915 val loss: 112.823\n",
            "epoch: 2458 loss: 107.773 val loss: 112.955\n",
            "epoch: 2459 loss: 67.855 val loss: 111.681\n",
            "epoch: 2460 loss: 94.827 val loss: 111.292\n",
            "epoch: 2461 loss: 133.500 val loss: 110.701\n",
            "epoch: 2462 loss: 123.072 val loss: 111.120\n",
            "epoch: 2463 loss: 109.346 val loss: 113.941\n",
            "epoch: 2464 loss: 110.311 val loss: 110.681\n",
            "epoch: 2465 loss: 134.884 val loss: 111.615\n",
            "epoch: 2466 loss: 133.915 val loss: 117.126\n",
            "epoch: 2467 loss: 91.634 val loss: 113.723\n",
            "epoch: 2468 loss: 96.621 val loss: 112.412\n",
            "epoch: 2469 loss: 117.147 val loss: 111.558\n",
            "epoch: 2470 loss: 121.762 val loss: 113.163\n",
            "epoch: 2471 loss: 117.085 val loss: 113.476\n",
            "epoch: 2472 loss: 142.695 val loss: 110.745\n",
            "epoch: 2473 loss: 85.769 val loss: 112.466\n",
            "epoch: 2474 loss: 99.476 val loss: 110.943\n",
            "epoch: 2475 loss: 97.575 val loss: 112.929\n",
            "epoch: 2476 loss: 124.990 val loss: 112.440\n",
            "epoch: 2477 loss: 104.397 val loss: 112.844\n",
            "epoch: 2478 loss: 158.451 val loss: 114.451\n",
            "epoch: 2479 loss: 156.626 val loss: 111.337\n",
            "epoch: 2480 loss: 121.909 val loss: 112.152\n",
            "epoch: 2481 loss: 121.307 val loss: 112.010\n",
            "epoch: 2482 loss: 125.495 val loss: 111.489\n",
            "epoch: 2483 loss: 105.830 val loss: 113.167\n",
            "epoch: 2484 loss: 96.322 val loss: 112.867\n",
            "epoch: 2485 loss: 134.137 val loss: 111.464\n",
            "epoch: 2486 loss: 145.274 val loss: 114.115\n",
            "epoch: 2487 loss: 110.110 val loss: 112.347\n",
            "epoch: 2488 loss: 82.108 val loss: 113.669\n",
            "epoch: 2489 loss: 156.778 val loss: 110.798\n",
            "epoch: 2490 loss: 98.474 val loss: 113.726\n",
            "epoch: 2491 loss: 101.597 val loss: 113.533\n",
            "epoch: 2492 loss: 148.223 val loss: 114.437\n",
            "epoch: 2493 loss: 139.452 val loss: 111.368\n",
            "epoch: 2494 loss: 107.129 val loss: 112.262\n",
            "epoch: 2495 loss: 120.482 val loss: 112.069\n",
            "epoch: 2496 loss: 115.424 val loss: 111.755\n",
            "epoch: 2497 loss: 89.248 val loss: 115.414\n",
            "epoch: 2498 loss: 113.624 val loss: 111.813\n",
            "epoch: 2499 loss: 124.671 val loss: 113.252\n",
            "epoch: 2500 loss: 124.630 val loss: 113.354\n",
            "epoch: 2501 loss: 79.541 val loss: 112.730\n",
            "epoch: 2502 loss: 98.072 val loss: 113.777\n",
            "epoch: 2503 loss: 113.255 val loss: 114.544\n",
            "epoch: 2504 loss: 126.567 val loss: 112.115\n",
            "epoch: 2505 loss: 109.975 val loss: 113.617\n",
            "epoch: 2506 loss: 140.382 val loss: 111.780\n",
            "epoch: 2507 loss: 104.977 val loss: 113.092\n",
            "epoch: 2508 loss: 97.275 val loss: 113.184\n",
            "epoch: 2509 loss: 110.036 val loss: 113.678\n",
            "epoch: 2510 loss: 101.682 val loss: 111.425\n",
            "epoch: 2511 loss: 106.046 val loss: 113.122\n",
            "epoch: 2512 loss: 127.473 val loss: 113.616\n",
            "epoch: 2513 loss: 129.322 val loss: 112.867\n",
            "epoch: 2514 loss: 113.263 val loss: 111.860\n",
            "epoch: 2515 loss: 96.722 val loss: 114.974\n",
            "epoch: 2516 loss: 126.861 val loss: 112.341\n",
            "epoch: 2517 loss: 112.969 val loss: 112.059\n",
            "epoch: 2518 loss: 118.228 val loss: 112.064\n",
            "epoch: 2519 loss: 152.049 val loss: 112.938\n",
            "epoch: 2520 loss: 154.816 val loss: 112.458\n",
            "epoch: 2521 loss: 121.240 val loss: 111.815\n",
            "epoch: 2522 loss: 133.287 val loss: 112.353\n",
            "epoch: 2523 loss: 113.916 val loss: 111.935\n",
            "epoch: 2524 loss: 88.275 val loss: 113.886\n",
            "epoch: 2525 loss: 97.622 val loss: 113.350\n",
            "epoch: 2526 loss: 136.340 val loss: 112.859\n",
            "epoch: 2527 loss: 145.896 val loss: 112.729\n",
            "epoch: 2528 loss: 123.896 val loss: 112.170\n",
            "epoch: 2529 loss: 131.556 val loss: 113.624\n",
            "epoch: 2530 loss: 123.474 val loss: 112.191\n",
            "epoch: 2531 loss: 153.730 val loss: 112.399\n",
            "epoch: 2532 loss: 99.247 val loss: 111.401\n",
            "epoch: 2533 loss: 106.398 val loss: 115.344\n",
            "epoch: 2534 loss: 136.689 val loss: 112.445\n",
            "epoch: 2535 loss: 82.026 val loss: 113.038\n",
            "epoch: 2536 loss: 146.734 val loss: 113.896\n",
            "epoch: 2537 loss: 120.180 val loss: 115.135\n",
            "epoch: 2538 loss: 91.100 val loss: 111.894\n",
            "epoch: 2539 loss: 86.741 val loss: 113.886\n",
            "epoch: 2540 loss: 107.343 val loss: 114.568\n",
            "epoch: 2541 loss: 128.416 val loss: 111.571\n",
            "epoch: 2542 loss: 100.468 val loss: 113.017\n",
            "epoch: 2543 loss: 144.818 val loss: 112.948\n",
            "epoch: 2544 loss: 112.792 val loss: 112.743\n",
            "epoch: 2545 loss: 95.706 val loss: 114.039\n",
            "epoch: 2546 loss: 94.969 val loss: 112.818\n",
            "epoch: 2547 loss: 126.788 val loss: 111.662\n",
            "epoch: 2548 loss: 107.369 val loss: 111.827\n",
            "epoch: 2549 loss: 139.434 val loss: 111.962\n",
            "epoch: 2550 loss: 105.452 val loss: 113.282\n",
            "epoch: 2551 loss: 102.582 val loss: 111.569\n",
            "epoch: 2552 loss: 55.473 val loss: 116.034\n",
            "epoch: 2553 loss: 153.415 val loss: 112.982\n",
            "epoch: 2554 loss: 85.480 val loss: 112.047\n",
            "epoch: 2555 loss: 84.230 val loss: 112.307\n",
            "epoch: 2556 loss: 101.639 val loss: 114.788\n",
            "epoch: 2557 loss: 81.341 val loss: 113.912\n",
            "epoch: 2558 loss: 94.742 val loss: 111.758\n",
            "epoch: 2559 loss: 99.575 val loss: 113.639\n",
            "epoch: 2560 loss: 93.215 val loss: 113.118\n",
            "epoch: 2561 loss: 90.667 val loss: 114.055\n",
            "epoch: 2562 loss: 126.763 val loss: 113.975\n",
            "epoch: 2563 loss: 75.528 val loss: 111.759\n",
            "epoch: 2564 loss: 102.356 val loss: 114.452\n",
            "epoch: 2565 loss: 101.211 val loss: 113.744\n",
            "epoch: 2566 loss: 90.393 val loss: 112.799\n",
            "epoch: 2567 loss: 113.237 val loss: 113.320\n",
            "epoch: 2568 loss: 88.671 val loss: 111.136\n",
            "epoch: 2569 loss: 110.698 val loss: 111.970\n",
            "epoch: 2570 loss: 137.552 val loss: 115.612\n",
            "epoch: 2571 loss: 115.378 val loss: 111.741\n",
            "epoch: 2572 loss: 131.912 val loss: 112.038\n",
            "epoch: 2573 loss: 91.617 val loss: 112.163\n",
            "epoch: 2574 loss: 65.024 val loss: 114.532\n",
            "epoch: 2575 loss: 108.528 val loss: 113.099\n",
            "epoch: 2576 loss: 117.467 val loss: 112.640\n",
            "epoch: 2577 loss: 111.481 val loss: 112.039\n",
            "epoch: 2578 loss: 130.545 val loss: 113.261\n",
            "epoch: 2579 loss: 119.785 val loss: 114.682\n",
            "epoch: 2580 loss: 105.522 val loss: 113.686\n",
            "epoch: 2581 loss: 104.756 val loss: 111.550\n",
            "epoch: 2582 loss: 141.340 val loss: 111.345\n",
            "epoch: 2583 loss: 83.483 val loss: 115.462\n",
            "epoch: 2584 loss: 85.668 val loss: 114.674\n",
            "epoch: 2585 loss: 106.487 val loss: 112.442\n",
            "epoch: 2586 loss: 121.910 val loss: 111.226\n",
            "epoch: 2587 loss: 74.330 val loss: 113.534\n",
            "epoch: 2588 loss: 85.658 val loss: 112.488\n",
            "epoch: 2589 loss: 144.629 val loss: 111.792\n",
            "epoch: 2590 loss: 118.712 val loss: 114.079\n",
            "epoch: 2591 loss: 109.071 val loss: 112.536\n",
            "epoch: 2592 loss: 134.892 val loss: 112.183\n",
            "epoch: 2593 loss: 136.229 val loss: 114.521\n",
            "epoch: 2594 loss: 98.840 val loss: 112.310\n",
            "epoch: 2595 loss: 116.413 val loss: 114.407\n",
            "epoch: 2596 loss: 97.141 val loss: 114.224\n",
            "epoch: 2597 loss: 109.484 val loss: 114.780\n",
            "epoch: 2598 loss: 139.052 val loss: 110.766\n",
            "epoch: 2599 loss: 104.079 val loss: 115.256\n",
            "epoch: 2600 loss: 93.439 val loss: 111.120\n",
            "epoch: 2601 loss: 99.161 val loss: 111.864\n",
            "epoch: 2602 loss: 68.258 val loss: 113.192\n",
            "epoch: 2603 loss: 143.147 val loss: 113.755\n",
            "epoch: 2604 loss: 85.274 val loss: 114.587\n",
            "epoch: 2605 loss: 98.738 val loss: 113.609\n",
            "epoch: 2606 loss: 96.693 val loss: 111.455\n",
            "epoch: 2607 loss: 75.040 val loss: 111.040\n",
            "epoch: 2608 loss: 96.819 val loss: 116.837\n",
            "epoch: 2609 loss: 104.317 val loss: 111.085\n",
            "epoch: 2610 loss: 75.189 val loss: 113.229\n",
            "epoch: 2611 loss: 114.783 val loss: 113.134\n",
            "epoch: 2612 loss: 172.899 val loss: 113.594\n",
            "epoch: 2613 loss: 97.505 val loss: 113.442\n",
            "epoch: 2614 loss: 144.476 val loss: 113.764\n",
            "epoch: 2615 loss: 113.264 val loss: 112.648\n",
            "epoch: 2616 loss: 60.251 val loss: 111.293\n",
            "epoch: 2617 loss: 131.075 val loss: 112.460\n",
            "epoch: 2618 loss: 98.614 val loss: 114.861\n",
            "epoch: 2619 loss: 76.202 val loss: 112.305\n",
            "epoch: 2620 loss: 111.170 val loss: 113.103\n",
            "epoch: 2621 loss: 91.711 val loss: 111.043\n",
            "epoch: 2622 loss: 109.785 val loss: 113.320\n",
            "epoch: 2623 loss: 97.838 val loss: 111.912\n",
            "epoch: 2624 loss: 117.942 val loss: 114.238\n",
            "epoch: 2625 loss: 104.753 val loss: 113.011\n",
            "epoch: 2626 loss: 162.257 val loss: 112.095\n",
            "epoch: 2627 loss: 121.692 val loss: 113.827\n",
            "epoch: 2628 loss: 121.770 val loss: 112.662\n",
            "epoch: 2629 loss: 122.706 val loss: 110.538\n",
            "epoch: 2630 loss: 90.053 val loss: 111.990\n",
            "epoch: 2631 loss: 125.884 val loss: 113.592\n",
            "epoch: 2632 loss: 127.843 val loss: 111.827\n",
            "epoch: 2633 loss: 145.905 val loss: 114.850\n",
            "epoch: 2634 loss: 119.800 val loss: 111.499\n",
            "epoch: 2635 loss: 118.040 val loss: 113.416\n",
            "epoch: 2636 loss: 115.334 val loss: 111.470\n",
            "epoch: 2637 loss: 105.137 val loss: 112.479\n",
            "epoch: 2638 loss: 157.553 val loss: 114.134\n",
            "epoch: 2639 loss: 84.779 val loss: 112.745\n",
            "epoch: 2640 loss: 162.177 val loss: 111.946\n",
            "epoch: 2641 loss: 127.002 val loss: 112.635\n",
            "epoch: 2642 loss: 108.768 val loss: 115.548\n",
            "epoch: 2643 loss: 123.031 val loss: 113.240\n",
            "epoch: 2644 loss: 121.438 val loss: 112.877\n",
            "epoch: 2645 loss: 99.650 val loss: 113.388\n",
            "epoch: 2646 loss: 113.560 val loss: 113.524\n",
            "epoch: 2647 loss: 113.911 val loss: 111.246\n",
            "epoch: 2648 loss: 93.766 val loss: 110.933\n",
            "epoch: 2649 loss: 111.693 val loss: 112.862\n",
            "epoch: 2650 loss: 109.153 val loss: 115.080\n",
            "epoch: 2651 loss: 129.973 val loss: 112.465\n",
            "epoch: 2652 loss: 174.847 val loss: 113.920\n",
            "epoch: 2653 loss: 110.454 val loss: 113.366\n",
            "epoch: 2654 loss: 134.674 val loss: 112.048\n",
            "epoch: 2655 loss: 84.130 val loss: 113.671\n",
            "epoch: 2656 loss: 71.271 val loss: 113.509\n",
            "epoch: 2657 loss: 104.048 val loss: 114.555\n",
            "epoch: 2658 loss: 72.567 val loss: 113.920\n",
            "epoch: 2659 loss: 99.641 val loss: 110.749\n",
            "epoch: 2660 loss: 133.948 val loss: 112.745\n",
            "epoch: 2661 loss: 91.299 val loss: 113.706\n",
            "epoch: 2662 loss: 102.605 val loss: 110.083\n",
            "epoch: 2663 loss: 108.603 val loss: 116.326\n",
            "epoch: 2664 loss: 127.199 val loss: 116.579\n",
            "epoch: 2665 loss: 144.951 val loss: 111.887\n",
            "epoch: 2666 loss: 99.943 val loss: 112.736\n",
            "epoch: 2667 loss: 135.554 val loss: 114.122\n",
            "epoch: 2668 loss: 158.346 val loss: 113.601\n",
            "epoch: 2669 loss: 100.502 val loss: 111.409\n",
            "epoch: 2670 loss: 107.600 val loss: 114.322\n",
            "epoch: 2671 loss: 114.077 val loss: 113.822\n",
            "epoch: 2672 loss: 144.557 val loss: 111.662\n",
            "epoch: 2673 loss: 105.053 val loss: 110.882\n",
            "epoch: 2674 loss: 111.477 val loss: 112.123\n",
            "epoch: 2675 loss: 116.628 val loss: 114.070\n",
            "epoch: 2676 loss: 138.469 val loss: 112.888\n",
            "epoch: 2677 loss: 90.405 val loss: 112.919\n",
            "epoch: 2678 loss: 119.587 val loss: 113.206\n",
            "epoch: 2679 loss: 81.978 val loss: 113.874\n",
            "epoch: 2680 loss: 146.861 val loss: 111.192\n",
            "epoch: 2681 loss: 102.362 val loss: 113.514\n",
            "epoch: 2682 loss: 76.941 val loss: 114.887\n",
            "epoch: 2683 loss: 176.264 val loss: 111.268\n",
            "epoch: 2684 loss: 94.848 val loss: 115.194\n",
            "epoch: 2685 loss: 85.996 val loss: 113.203\n",
            "epoch: 2686 loss: 125.880 val loss: 111.882\n",
            "epoch: 2687 loss: 89.773 val loss: 111.861\n",
            "epoch: 2688 loss: 128.200 val loss: 112.774\n",
            "epoch: 2689 loss: 116.711 val loss: 112.879\n",
            "epoch: 2690 loss: 147.810 val loss: 111.564\n",
            "epoch: 2691 loss: 145.140 val loss: 112.198\n",
            "epoch: 2692 loss: 89.614 val loss: 112.808\n",
            "epoch: 2693 loss: 116.520 val loss: 112.040\n",
            "epoch: 2694 loss: 130.980 val loss: 114.956\n",
            "epoch: 2695 loss: 105.596 val loss: 112.404\n",
            "epoch: 2696 loss: 89.970 val loss: 115.060\n",
            "epoch: 2697 loss: 145.633 val loss: 112.084\n",
            "epoch: 2698 loss: 113.882 val loss: 114.789\n",
            "epoch: 2699 loss: 117.547 val loss: 112.507\n",
            "epoch: 2700 loss: 96.997 val loss: 111.937\n",
            "epoch: 2701 loss: 120.845 val loss: 117.575\n",
            "epoch: 2702 loss: 139.811 val loss: 111.628\n",
            "epoch: 2703 loss: 148.651 val loss: 112.639\n",
            "epoch: 2704 loss: 76.799 val loss: 111.089\n",
            "epoch: 2705 loss: 122.214 val loss: 112.221\n",
            "epoch: 2706 loss: 101.561 val loss: 115.186\n",
            "epoch: 2707 loss: 111.095 val loss: 112.183\n",
            "epoch: 2708 loss: 84.682 val loss: 110.600\n",
            "epoch: 2709 loss: 101.997 val loss: 112.929\n",
            "epoch: 2710 loss: 126.708 val loss: 113.980\n",
            "epoch: 2711 loss: 108.899 val loss: 112.959\n",
            "epoch: 2712 loss: 102.125 val loss: 112.144\n",
            "epoch: 2713 loss: 86.907 val loss: 110.383\n",
            "epoch: 2714 loss: 80.402 val loss: 110.873\n",
            "epoch: 2715 loss: 126.247 val loss: 114.973\n",
            "epoch: 2716 loss: 96.306 val loss: 113.910\n",
            "epoch: 2717 loss: 119.787 val loss: 112.352\n",
            "epoch: 2718 loss: 135.326 val loss: 113.114\n",
            "epoch: 2719 loss: 114.105 val loss: 110.620\n",
            "epoch: 2720 loss: 70.941 val loss: 110.868\n",
            "epoch: 2721 loss: 121.804 val loss: 113.628\n",
            "epoch: 2722 loss: 99.151 val loss: 113.057\n",
            "epoch: 2723 loss: 127.020 val loss: 111.511\n",
            "epoch: 2724 loss: 120.660 val loss: 113.246\n",
            "epoch: 2725 loss: 146.198 val loss: 114.197\n",
            "epoch: 2726 loss: 79.915 val loss: 113.393\n",
            "epoch: 2727 loss: 67.719 val loss: 110.908\n",
            "epoch: 2728 loss: 122.621 val loss: 112.653\n",
            "epoch: 2729 loss: 140.260 val loss: 111.180\n",
            "epoch: 2730 loss: 93.090 val loss: 110.663\n",
            "epoch: 2731 loss: 119.508 val loss: 112.758\n",
            "epoch: 2732 loss: 127.380 val loss: 113.470\n",
            "epoch: 2733 loss: 140.801 val loss: 112.381\n",
            "epoch: 2734 loss: 165.110 val loss: 113.898\n",
            "epoch: 2735 loss: 112.068 val loss: 111.486\n",
            "epoch: 2736 loss: 103.590 val loss: 112.206\n",
            "epoch: 2737 loss: 139.548 val loss: 114.727\n",
            "epoch: 2738 loss: 93.831 val loss: 113.106\n",
            "epoch: 2739 loss: 127.466 val loss: 114.236\n",
            "epoch: 2740 loss: 116.896 val loss: 114.894\n",
            "epoch: 2741 loss: 143.557 val loss: 111.938\n",
            "epoch: 2742 loss: 110.524 val loss: 114.004\n",
            "epoch: 2743 loss: 120.537 val loss: 112.166\n",
            "epoch: 2744 loss: 97.675 val loss: 111.604\n",
            "epoch: 2745 loss: 101.582 val loss: 111.141\n",
            "epoch: 2746 loss: 109.550 val loss: 113.354\n",
            "epoch: 2747 loss: 84.335 val loss: 113.327\n",
            "epoch: 2748 loss: 107.527 val loss: 110.691\n",
            "epoch: 2749 loss: 120.626 val loss: 112.535\n",
            "epoch: 2750 loss: 121.648 val loss: 112.397\n",
            "epoch: 2751 loss: 78.446 val loss: 117.639\n",
            "epoch: 2752 loss: 82.743 val loss: 110.949\n",
            "epoch: 2753 loss: 125.661 val loss: 112.606\n",
            "epoch: 2754 loss: 131.624 val loss: 113.319\n",
            "epoch: 2755 loss: 106.330 val loss: 113.303\n",
            "epoch: 2756 loss: 147.216 val loss: 110.850\n",
            "epoch: 2757 loss: 91.876 val loss: 112.199\n",
            "epoch: 2758 loss: 126.476 val loss: 114.623\n",
            "epoch: 2759 loss: 100.050 val loss: 114.751\n",
            "epoch: 2760 loss: 124.491 val loss: 112.913\n",
            "epoch: 2761 loss: 110.032 val loss: 110.746\n",
            "epoch: 2762 loss: 94.138 val loss: 112.990\n",
            "epoch: 2763 loss: 94.457 val loss: 113.114\n",
            "epoch: 2764 loss: 84.390 val loss: 113.507\n",
            "epoch: 2765 loss: 120.338 val loss: 111.472\n",
            "epoch: 2766 loss: 122.649 val loss: 114.305\n",
            "epoch: 2767 loss: 76.489 val loss: 113.036\n",
            "epoch: 2768 loss: 101.757 val loss: 113.545\n",
            "epoch: 2769 loss: 98.309 val loss: 112.951\n",
            "epoch: 2770 loss: 90.008 val loss: 113.545\n",
            "epoch: 2771 loss: 78.769 val loss: 111.249\n",
            "epoch: 2772 loss: 84.640 val loss: 114.120\n",
            "epoch: 2773 loss: 130.750 val loss: 111.465\n",
            "epoch: 2774 loss: 101.611 val loss: 113.377\n",
            "epoch: 2775 loss: 96.680 val loss: 112.953\n",
            "epoch: 2776 loss: 118.947 val loss: 113.472\n",
            "epoch: 2777 loss: 103.463 val loss: 112.984\n",
            "epoch: 2778 loss: 157.546 val loss: 112.799\n",
            "epoch: 2779 loss: 120.989 val loss: 114.087\n",
            "epoch: 2780 loss: 158.317 val loss: 114.249\n",
            "epoch: 2781 loss: 93.380 val loss: 114.270\n",
            "epoch: 2782 loss: 66.588 val loss: 115.967\n",
            "epoch: 2783 loss: 88.256 val loss: 112.957\n",
            "epoch: 2784 loss: 149.031 val loss: 114.771\n",
            "epoch: 2785 loss: 77.748 val loss: 114.110\n",
            "epoch: 2786 loss: 115.641 val loss: 111.343\n",
            "epoch: 2787 loss: 55.133 val loss: 113.576\n",
            "epoch: 2788 loss: 126.073 val loss: 111.903\n",
            "epoch: 2789 loss: 151.880 val loss: 111.591\n",
            "epoch: 2790 loss: 103.620 val loss: 113.528\n",
            "epoch: 2791 loss: 119.495 val loss: 115.810\n",
            "epoch: 2792 loss: 132.932 val loss: 114.048\n",
            "epoch: 2793 loss: 148.304 val loss: 114.264\n",
            "epoch: 2794 loss: 111.474 val loss: 110.591\n",
            "epoch: 2795 loss: 123.247 val loss: 114.025\n",
            "epoch: 2796 loss: 109.381 val loss: 111.790\n",
            "epoch: 2797 loss: 99.972 val loss: 113.103\n",
            "epoch: 2798 loss: 203.319 val loss: 111.963\n",
            "epoch: 2799 loss: 125.498 val loss: 114.236\n",
            "epoch: 2800 loss: 126.398 val loss: 111.482\n",
            "epoch: 2801 loss: 69.629 val loss: 113.683\n",
            "epoch: 2802 loss: 117.489 val loss: 110.558\n",
            "epoch: 2803 loss: 80.746 val loss: 116.626\n",
            "epoch: 2804 loss: 136.826 val loss: 112.271\n",
            "epoch: 2805 loss: 129.704 val loss: 113.373\n",
            "epoch: 2806 loss: 113.865 val loss: 113.986\n",
            "epoch: 2807 loss: 106.050 val loss: 115.041\n",
            "epoch: 2808 loss: 133.310 val loss: 113.061\n",
            "epoch: 2809 loss: 113.419 val loss: 113.384\n",
            "epoch: 2810 loss: 143.563 val loss: 114.281\n",
            "epoch: 2811 loss: 54.402 val loss: 112.320\n",
            "epoch: 2812 loss: 101.509 val loss: 112.787\n",
            "epoch: 2813 loss: 121.358 val loss: 111.563\n",
            "epoch: 2814 loss: 119.378 val loss: 112.377\n",
            "epoch: 2815 loss: 114.960 val loss: 114.918\n",
            "epoch: 2816 loss: 88.425 val loss: 113.441\n",
            "epoch: 2817 loss: 85.600 val loss: 112.926\n",
            "epoch: 2818 loss: 104.598 val loss: 111.639\n",
            "epoch: 2819 loss: 97.315 val loss: 113.452\n",
            "epoch: 2820 loss: 125.588 val loss: 117.136\n",
            "epoch: 2821 loss: 120.881 val loss: 111.480\n",
            "epoch: 2822 loss: 113.703 val loss: 113.120\n",
            "epoch: 2823 loss: 110.676 val loss: 113.625\n",
            "epoch: 2824 loss: 172.324 val loss: 114.902\n",
            "epoch: 2825 loss: 110.032 val loss: 112.067\n",
            "epoch: 2826 loss: 80.941 val loss: 112.201\n",
            "epoch: 2827 loss: 93.601 val loss: 113.101\n",
            "epoch: 2828 loss: 79.467 val loss: 111.801\n",
            "epoch: 2829 loss: 133.046 val loss: 112.408\n",
            "epoch: 2830 loss: 86.581 val loss: 112.927\n",
            "epoch: 2831 loss: 97.162 val loss: 113.807\n",
            "epoch: 2832 loss: 112.326 val loss: 113.354\n",
            "epoch: 2833 loss: 126.454 val loss: 114.121\n",
            "epoch: 2834 loss: 130.187 val loss: 112.102\n",
            "epoch: 2835 loss: 151.273 val loss: 114.117\n",
            "epoch: 2836 loss: 90.375 val loss: 115.361\n",
            "epoch: 2837 loss: 150.213 val loss: 113.203\n",
            "epoch: 2838 loss: 118.360 val loss: 111.197\n",
            "epoch: 2839 loss: 127.322 val loss: 111.699\n",
            "epoch: 2840 loss: 123.177 val loss: 111.977\n",
            "epoch: 2841 loss: 101.830 val loss: 114.588\n",
            "epoch: 2842 loss: 132.760 val loss: 113.774\n",
            "epoch: 2843 loss: 135.089 val loss: 111.040\n",
            "epoch: 2844 loss: 104.875 val loss: 115.417\n",
            "epoch: 2845 loss: 144.554 val loss: 113.111\n",
            "epoch: 2846 loss: 120.153 val loss: 112.575\n",
            "epoch: 2847 loss: 109.466 val loss: 113.981\n",
            "epoch: 2848 loss: 103.687 val loss: 111.859\n",
            "epoch: 2849 loss: 93.025 val loss: 113.089\n",
            "epoch: 2850 loss: 99.608 val loss: 112.488\n",
            "epoch: 2851 loss: 130.416 val loss: 113.315\n",
            "epoch: 2852 loss: 133.167 val loss: 111.649\n",
            "epoch: 2853 loss: 103.704 val loss: 114.675\n",
            "epoch: 2854 loss: 119.448 val loss: 113.736\n",
            "epoch: 2855 loss: 120.941 val loss: 113.306\n",
            "epoch: 2856 loss: 110.844 val loss: 112.854\n",
            "epoch: 2857 loss: 99.446 val loss: 112.599\n",
            "epoch: 2858 loss: 111.797 val loss: 111.245\n",
            "epoch: 2859 loss: 86.097 val loss: 112.481\n",
            "epoch: 2860 loss: 124.015 val loss: 111.959\n",
            "epoch: 2861 loss: 74.080 val loss: 112.696\n",
            "epoch: 2862 loss: 152.680 val loss: 113.693\n",
            "epoch: 2863 loss: 81.827 val loss: 113.791\n",
            "epoch: 2864 loss: 111.267 val loss: 111.187\n",
            "epoch: 2865 loss: 138.832 val loss: 112.249\n",
            "epoch: 2866 loss: 161.021 val loss: 112.259\n",
            "epoch: 2867 loss: 101.884 val loss: 112.350\n",
            "epoch: 2868 loss: 154.931 val loss: 111.829\n",
            "epoch: 2869 loss: 67.513 val loss: 111.645\n",
            "epoch: 2870 loss: 102.311 val loss: 113.226\n",
            "epoch: 2871 loss: 92.356 val loss: 113.753\n",
            "epoch: 2872 loss: 155.949 val loss: 111.577\n",
            "epoch: 2873 loss: 96.188 val loss: 112.371\n",
            "epoch: 2874 loss: 157.171 val loss: 113.830\n",
            "epoch: 2875 loss: 97.709 val loss: 112.477\n",
            "epoch: 2876 loss: 97.489 val loss: 112.114\n",
            "epoch: 2877 loss: 106.736 val loss: 111.618\n",
            "epoch: 2878 loss: 114.285 val loss: 114.105\n",
            "epoch: 2879 loss: 153.442 val loss: 110.888\n",
            "epoch: 2880 loss: 123.544 val loss: 111.551\n",
            "epoch: 2881 loss: 102.837 val loss: 113.501\n",
            "epoch: 2882 loss: 77.907 val loss: 112.430\n",
            "epoch: 2883 loss: 120.245 val loss: 114.052\n",
            "epoch: 2884 loss: 129.850 val loss: 112.610\n",
            "epoch: 2885 loss: 84.126 val loss: 116.826\n",
            "epoch: 2886 loss: 99.759 val loss: 112.275\n",
            "epoch: 2887 loss: 116.731 val loss: 116.316\n",
            "epoch: 2888 loss: 100.676 val loss: 111.793\n",
            "epoch: 2889 loss: 124.530 val loss: 111.590\n",
            "epoch: 2890 loss: 113.214 val loss: 115.161\n",
            "epoch: 2891 loss: 65.433 val loss: 114.464\n",
            "epoch: 2892 loss: 93.946 val loss: 113.815\n",
            "epoch: 2893 loss: 148.483 val loss: 114.821\n",
            "epoch: 2894 loss: 109.627 val loss: 110.683\n",
            "epoch: 2895 loss: 124.263 val loss: 114.187\n",
            "epoch: 2896 loss: 85.722 val loss: 112.311\n",
            "epoch: 2897 loss: 113.112 val loss: 112.520\n",
            "epoch: 2898 loss: 107.349 val loss: 115.854\n",
            "epoch: 2899 loss: 91.945 val loss: 110.971\n",
            "epoch: 2900 loss: 87.956 val loss: 112.613\n",
            "epoch: 2901 loss: 149.396 val loss: 111.579\n",
            "epoch: 2902 loss: 95.100 val loss: 114.862\n",
            "epoch: 2903 loss: 113.078 val loss: 112.647\n",
            "epoch: 2904 loss: 129.196 val loss: 112.053\n",
            "epoch: 2905 loss: 81.777 val loss: 112.485\n",
            "epoch: 2906 loss: 150.641 val loss: 113.170\n",
            "epoch: 2907 loss: 83.811 val loss: 113.866\n",
            "epoch: 2908 loss: 115.339 val loss: 112.950\n",
            "epoch: 2909 loss: 105.961 val loss: 112.033\n",
            "epoch: 2910 loss: 151.239 val loss: 110.259\n",
            "epoch: 2911 loss: 94.145 val loss: 112.052\n",
            "epoch: 2912 loss: 86.034 val loss: 111.951\n",
            "epoch: 2913 loss: 124.074 val loss: 113.226\n",
            "epoch: 2914 loss: 93.180 val loss: 111.270\n",
            "epoch: 2915 loss: 108.984 val loss: 113.224\n",
            "epoch: 2916 loss: 120.017 val loss: 111.481\n",
            "epoch: 2917 loss: 114.284 val loss: 112.839\n",
            "epoch: 2918 loss: 120.970 val loss: 113.946\n",
            "epoch: 2919 loss: 90.503 val loss: 110.338\n",
            "epoch: 2920 loss: 114.249 val loss: 111.959\n",
            "epoch: 2921 loss: 119.188 val loss: 110.792\n",
            "epoch: 2922 loss: 94.011 val loss: 114.418\n",
            "epoch: 2923 loss: 126.088 val loss: 113.988\n",
            "epoch: 2924 loss: 110.472 val loss: 112.425\n",
            "epoch: 2925 loss: 144.596 val loss: 110.764\n",
            "epoch: 2926 loss: 134.010 val loss: 110.964\n",
            "epoch: 2927 loss: 93.326 val loss: 110.936\n",
            "epoch: 2928 loss: 137.670 val loss: 110.626\n",
            "epoch: 2929 loss: 117.773 val loss: 113.028\n",
            "epoch: 2930 loss: 91.954 val loss: 114.467\n",
            "epoch: 2931 loss: 92.674 val loss: 114.985\n",
            "epoch: 2932 loss: 82.130 val loss: 114.539\n",
            "epoch: 2933 loss: 104.152 val loss: 112.520\n",
            "epoch: 2934 loss: 88.545 val loss: 113.954\n",
            "epoch: 2935 loss: 110.184 val loss: 111.646\n",
            "epoch: 2936 loss: 96.570 val loss: 113.934\n",
            "epoch: 2937 loss: 81.583 val loss: 114.483\n",
            "epoch: 2938 loss: 99.424 val loss: 113.030\n",
            "epoch: 2939 loss: 76.136 val loss: 114.891\n",
            "epoch: 2940 loss: 144.179 val loss: 113.759\n",
            "epoch: 2941 loss: 86.403 val loss: 112.840\n",
            "epoch: 2942 loss: 98.049 val loss: 113.288\n",
            "epoch: 2943 loss: 167.127 val loss: 112.327\n",
            "epoch: 2944 loss: 109.004 val loss: 112.090\n",
            "epoch: 2945 loss: 119.828 val loss: 114.701\n",
            "epoch: 2946 loss: 207.046 val loss: 111.193\n",
            "epoch: 2947 loss: 117.340 val loss: 113.341\n",
            "epoch: 2948 loss: 93.358 val loss: 110.802\n",
            "epoch: 2949 loss: 97.196 val loss: 113.449\n",
            "epoch: 2950 loss: 119.420 val loss: 111.405\n",
            "epoch: 2951 loss: 103.366 val loss: 112.832\n",
            "epoch: 2952 loss: 111.242 val loss: 113.396\n",
            "epoch: 2953 loss: 127.142 val loss: 113.040\n",
            "epoch: 2954 loss: 82.975 val loss: 111.576\n",
            "epoch: 2955 loss: 96.473 val loss: 112.097\n",
            "epoch: 2956 loss: 114.796 val loss: 112.575\n",
            "epoch: 2957 loss: 119.828 val loss: 113.274\n",
            "epoch: 2958 loss: 108.647 val loss: 114.023\n",
            "epoch: 2959 loss: 100.240 val loss: 111.964\n",
            "epoch: 2960 loss: 117.805 val loss: 111.399\n",
            "epoch: 2961 loss: 117.060 val loss: 112.798\n",
            "epoch: 2962 loss: 98.300 val loss: 111.756\n",
            "epoch: 2963 loss: 78.333 val loss: 110.649\n",
            "epoch: 2964 loss: 131.751 val loss: 113.557\n",
            "epoch: 2965 loss: 84.169 val loss: 113.906\n",
            "epoch: 2966 loss: 147.820 val loss: 113.226\n",
            "epoch: 2967 loss: 112.380 val loss: 113.156\n",
            "epoch: 2968 loss: 139.475 val loss: 113.883\n",
            "epoch: 2969 loss: 150.337 val loss: 116.783\n",
            "epoch: 2970 loss: 149.905 val loss: 111.592\n",
            "epoch: 2971 loss: 107.335 val loss: 112.281\n",
            "epoch: 2972 loss: 133.032 val loss: 111.084\n",
            "epoch: 2973 loss: 73.981 val loss: 111.754\n",
            "epoch: 2974 loss: 107.786 val loss: 111.374\n",
            "epoch: 2975 loss: 112.857 val loss: 111.400\n",
            "epoch: 2976 loss: 114.451 val loss: 113.125\n",
            "epoch: 2977 loss: 161.750 val loss: 111.978\n",
            "epoch: 2978 loss: 138.913 val loss: 113.043\n",
            "epoch: 2979 loss: 120.801 val loss: 113.492\n",
            "epoch: 2980 loss: 143.017 val loss: 112.270\n",
            "epoch: 2981 loss: 135.980 val loss: 111.463\n",
            "epoch: 2982 loss: 94.385 val loss: 112.917\n",
            "epoch: 2983 loss: 117.567 val loss: 113.416\n",
            "epoch: 2984 loss: 124.463 val loss: 114.035\n",
            "epoch: 2985 loss: 141.422 val loss: 111.953\n",
            "epoch: 2986 loss: 107.279 val loss: 113.033\n",
            "epoch: 2987 loss: 114.708 val loss: 112.169\n",
            "epoch: 2988 loss: 114.689 val loss: 111.992\n",
            "epoch: 2989 loss: 156.855 val loss: 112.412\n",
            "epoch: 2990 loss: 156.655 val loss: 114.538\n",
            "epoch: 2991 loss: 103.377 val loss: 115.122\n",
            "epoch: 2992 loss: 113.895 val loss: 112.414\n",
            "epoch: 2993 loss: 109.026 val loss: 110.537\n",
            "epoch: 2994 loss: 96.275 val loss: 112.831\n",
            "epoch: 2995 loss: 110.530 val loss: 112.132\n",
            "epoch: 2996 loss: 130.785 val loss: 115.373\n",
            "epoch: 2997 loss: 88.481 val loss: 109.882\n",
            "epoch: 2998 loss: 85.211 val loss: 112.019\n",
            "epoch: 2999 loss: 116.164 val loss: 110.364\n",
            "epoch: 3000 loss: 119.172 val loss: 111.208\n",
            "epoch: 3001 loss: 99.203 val loss: 111.932\n",
            "epoch: 3002 loss: 102.709 val loss: 111.871\n",
            "epoch: 3003 loss: 66.773 val loss: 112.141\n",
            "epoch: 3004 loss: 92.527 val loss: 112.415\n",
            "epoch: 3005 loss: 114.085 val loss: 112.867\n",
            "epoch: 3006 loss: 126.093 val loss: 114.015\n",
            "epoch: 3007 loss: 151.254 val loss: 113.049\n",
            "epoch: 3008 loss: 102.105 val loss: 113.943\n",
            "epoch: 3009 loss: 129.527 val loss: 113.235\n",
            "epoch: 3010 loss: 105.800 val loss: 111.859\n",
            "epoch: 3011 loss: 134.141 val loss: 111.761\n",
            "epoch: 3012 loss: 113.866 val loss: 112.562\n",
            "epoch: 3013 loss: 132.720 val loss: 113.489\n",
            "epoch: 3014 loss: 127.942 val loss: 111.151\n",
            "epoch: 3015 loss: 122.697 val loss: 112.197\n",
            "epoch: 3016 loss: 133.985 val loss: 110.525\n",
            "epoch: 3017 loss: 111.389 val loss: 111.604\n",
            "epoch: 3018 loss: 115.439 val loss: 112.031\n",
            "epoch: 3019 loss: 112.444 val loss: 112.400\n",
            "epoch: 3020 loss: 113.969 val loss: 114.824\n",
            "epoch: 3021 loss: 72.049 val loss: 112.236\n",
            "epoch: 3022 loss: 91.292 val loss: 111.546\n",
            "epoch: 3023 loss: 74.757 val loss: 111.278\n",
            "epoch: 3024 loss: 177.567 val loss: 111.551\n",
            "epoch: 3025 loss: 103.257 val loss: 115.908\n",
            "epoch: 3026 loss: 125.080 val loss: 112.034\n",
            "epoch: 3027 loss: 201.995 val loss: 112.282\n",
            "epoch: 3028 loss: 85.817 val loss: 111.779\n",
            "epoch: 3029 loss: 97.038 val loss: 111.873\n",
            "epoch: 3030 loss: 83.320 val loss: 115.071\n",
            "epoch: 3031 loss: 143.676 val loss: 110.992\n",
            "epoch: 3032 loss: 104.939 val loss: 114.077\n",
            "epoch: 3033 loss: 113.641 val loss: 115.033\n",
            "epoch: 3034 loss: 106.004 val loss: 113.421\n",
            "epoch: 3035 loss: 122.882 val loss: 114.907\n",
            "epoch: 3036 loss: 127.868 val loss: 111.713\n",
            "epoch: 3037 loss: 108.924 val loss: 112.600\n",
            "epoch: 3038 loss: 106.712 val loss: 110.715\n",
            "epoch: 3039 loss: 72.840 val loss: 114.434\n",
            "epoch: 3040 loss: 92.233 val loss: 112.747\n",
            "epoch: 3041 loss: 156.661 val loss: 115.655\n",
            "epoch: 3042 loss: 157.023 val loss: 110.371\n",
            "epoch: 3043 loss: 119.134 val loss: 111.931\n",
            "epoch: 3044 loss: 83.809 val loss: 113.015\n",
            "epoch: 3045 loss: 126.645 val loss: 114.606\n",
            "epoch: 3046 loss: 107.050 val loss: 111.716\n",
            "epoch: 3047 loss: 91.771 val loss: 112.909\n",
            "epoch: 3048 loss: 151.120 val loss: 113.863\n",
            "epoch: 3049 loss: 86.883 val loss: 113.785\n",
            "epoch: 3050 loss: 135.054 val loss: 114.801\n",
            "epoch: 3051 loss: 175.499 val loss: 113.539\n",
            "epoch: 3052 loss: 104.608 val loss: 113.559\n",
            "epoch: 3053 loss: 91.959 val loss: 112.834\n",
            "epoch: 3054 loss: 123.832 val loss: 111.348\n",
            "epoch: 3055 loss: 93.689 val loss: 113.049\n",
            "epoch: 3056 loss: 81.275 val loss: 112.489\n",
            "epoch: 3057 loss: 79.507 val loss: 113.651\n",
            "epoch: 3058 loss: 111.306 val loss: 113.199\n",
            "epoch: 3059 loss: 149.843 val loss: 112.146\n",
            "epoch: 3060 loss: 135.183 val loss: 112.193\n",
            "epoch: 3061 loss: 163.287 val loss: 112.803\n",
            "epoch: 3062 loss: 128.653 val loss: 112.328\n",
            "epoch: 3063 loss: 137.640 val loss: 112.084\n",
            "epoch: 3064 loss: 104.350 val loss: 112.305\n",
            "epoch: 3065 loss: 133.357 val loss: 111.987\n",
            "epoch: 3066 loss: 176.134 val loss: 116.003\n",
            "epoch: 3067 loss: 107.278 val loss: 113.573\n",
            "epoch: 3068 loss: 95.131 val loss: 113.691\n",
            "epoch: 3069 loss: 87.185 val loss: 110.446\n",
            "epoch: 3070 loss: 112.019 val loss: 112.571\n",
            "epoch: 3071 loss: 107.144 val loss: 113.173\n",
            "epoch: 3072 loss: 96.596 val loss: 111.305\n",
            "epoch: 3073 loss: 106.721 val loss: 113.575\n",
            "epoch: 3074 loss: 92.869 val loss: 114.404\n",
            "epoch: 3075 loss: 157.320 val loss: 113.689\n",
            "epoch: 3076 loss: 108.147 val loss: 110.841\n",
            "epoch: 3077 loss: 106.433 val loss: 113.231\n",
            "epoch: 3078 loss: 148.128 val loss: 114.078\n",
            "epoch: 3079 loss: 67.551 val loss: 112.572\n",
            "epoch: 3080 loss: 123.147 val loss: 112.731\n",
            "epoch: 3081 loss: 115.695 val loss: 112.363\n",
            "epoch: 3082 loss: 146.946 val loss: 111.284\n",
            "epoch: 3083 loss: 133.818 val loss: 112.209\n",
            "epoch: 3084 loss: 174.573 val loss: 112.703\n",
            "epoch: 3085 loss: 101.606 val loss: 113.142\n",
            "epoch: 3086 loss: 184.717 val loss: 113.814\n",
            "epoch: 3087 loss: 83.629 val loss: 114.152\n",
            "epoch: 3088 loss: 119.651 val loss: 112.651\n",
            "epoch: 3089 loss: 112.590 val loss: 115.561\n",
            "epoch: 3090 loss: 99.999 val loss: 112.676\n",
            "epoch: 3091 loss: 141.078 val loss: 113.141\n",
            "epoch: 3092 loss: 146.349 val loss: 110.958\n",
            "epoch: 3093 loss: 91.753 val loss: 112.470\n",
            "epoch: 3094 loss: 95.170 val loss: 111.314\n",
            "epoch: 3095 loss: 98.445 val loss: 111.947\n",
            "epoch: 3096 loss: 118.681 val loss: 113.512\n",
            "epoch: 3097 loss: 98.524 val loss: 113.921\n",
            "epoch: 3098 loss: 81.949 val loss: 111.764\n",
            "epoch: 3099 loss: 146.494 val loss: 111.308\n",
            "epoch: 3100 loss: 119.604 val loss: 114.432\n",
            "epoch: 3101 loss: 109.089 val loss: 113.880\n",
            "epoch: 3102 loss: 102.680 val loss: 111.536\n",
            "epoch: 3103 loss: 102.220 val loss: 113.984\n",
            "epoch: 3104 loss: 90.224 val loss: 112.292\n",
            "epoch: 3105 loss: 125.781 val loss: 115.485\n",
            "epoch: 3106 loss: 146.110 val loss: 113.648\n",
            "epoch: 3107 loss: 126.930 val loss: 114.681\n",
            "epoch: 3108 loss: 177.358 val loss: 114.908\n",
            "epoch: 3109 loss: 97.963 val loss: 113.459\n",
            "epoch: 3110 loss: 153.764 val loss: 110.990\n",
            "epoch: 3111 loss: 127.699 val loss: 113.417\n",
            "epoch: 3112 loss: 123.701 val loss: 110.482\n",
            "epoch: 3113 loss: 123.487 val loss: 112.677\n",
            "epoch: 3114 loss: 137.899 val loss: 113.080\n",
            "epoch: 3115 loss: 139.580 val loss: 110.788\n",
            "epoch: 3116 loss: 128.561 val loss: 113.463\n",
            "epoch: 3117 loss: 89.884 val loss: 114.611\n",
            "epoch: 3118 loss: 87.816 val loss: 114.566\n",
            "epoch: 3119 loss: 109.193 val loss: 115.253\n",
            "epoch: 3120 loss: 110.331 val loss: 115.123\n",
            "epoch: 3121 loss: 131.189 val loss: 111.831\n",
            "epoch: 3122 loss: 88.443 val loss: 114.922\n",
            "epoch: 3123 loss: 151.567 val loss: 111.388\n",
            "epoch: 3124 loss: 161.736 val loss: 112.617\n",
            "epoch: 3125 loss: 132.077 val loss: 112.919\n",
            "epoch: 3126 loss: 110.062 val loss: 113.381\n",
            "epoch: 3127 loss: 110.410 val loss: 112.041\n",
            "epoch: 3128 loss: 110.023 val loss: 112.803\n",
            "epoch: 3129 loss: 121.890 val loss: 111.227\n",
            "epoch: 3130 loss: 103.606 val loss: 115.454\n",
            "epoch: 3131 loss: 121.786 val loss: 112.908\n",
            "epoch: 3132 loss: 110.300 val loss: 111.872\n",
            "epoch: 3133 loss: 88.745 val loss: 112.574\n",
            "epoch: 3134 loss: 100.976 val loss: 113.542\n",
            "epoch: 3135 loss: 134.317 val loss: 114.303\n",
            "epoch: 3136 loss: 77.368 val loss: 111.980\n",
            "epoch: 3137 loss: 113.353 val loss: 112.623\n",
            "epoch: 3138 loss: 99.114 val loss: 112.468\n",
            "epoch: 3139 loss: 93.981 val loss: 112.172\n",
            "epoch: 3140 loss: 102.809 val loss: 111.366\n",
            "epoch: 3141 loss: 99.520 val loss: 112.441\n",
            "epoch: 3142 loss: 99.208 val loss: 113.192\n",
            "epoch: 3143 loss: 158.864 val loss: 111.786\n",
            "epoch: 3144 loss: 120.818 val loss: 114.202\n",
            "epoch: 3145 loss: 91.393 val loss: 111.276\n",
            "epoch: 3146 loss: 108.173 val loss: 110.476\n",
            "epoch: 3147 loss: 79.782 val loss: 112.432\n",
            "epoch: 3148 loss: 115.630 val loss: 112.496\n",
            "epoch: 3149 loss: 119.074 val loss: 112.898\n",
            "epoch: 3150 loss: 101.873 val loss: 115.338\n",
            "epoch: 3151 loss: 143.717 val loss: 111.573\n",
            "epoch: 3152 loss: 146.714 val loss: 113.134\n",
            "epoch: 3153 loss: 75.253 val loss: 110.907\n",
            "epoch: 3154 loss: 132.212 val loss: 111.521\n",
            "epoch: 3155 loss: 139.941 val loss: 111.092\n",
            "epoch: 3156 loss: 98.719 val loss: 113.592\n",
            "epoch: 3157 loss: 99.859 val loss: 109.746\n",
            "epoch: 3158 loss: 132.398 val loss: 112.047\n",
            "epoch: 3159 loss: 101.228 val loss: 111.282\n",
            "epoch: 3160 loss: 76.587 val loss: 112.348\n",
            "epoch: 3161 loss: 136.725 val loss: 111.445\n",
            "epoch: 3162 loss: 89.572 val loss: 113.868\n",
            "epoch: 3163 loss: 132.420 val loss: 114.336\n",
            "epoch: 3164 loss: 106.638 val loss: 112.518\n",
            "epoch: 3165 loss: 110.543 val loss: 112.200\n",
            "epoch: 3166 loss: 92.161 val loss: 112.455\n",
            "epoch: 3167 loss: 127.077 val loss: 111.792\n",
            "epoch: 3168 loss: 127.650 val loss: 111.192\n",
            "epoch: 3169 loss: 105.304 val loss: 112.423\n",
            "epoch: 3170 loss: 82.677 val loss: 112.560\n",
            "epoch: 3171 loss: 94.606 val loss: 113.196\n",
            "epoch: 3172 loss: 78.240 val loss: 112.208\n",
            "epoch: 3173 loss: 76.473 val loss: 113.654\n",
            "epoch: 3174 loss: 87.835 val loss: 112.552\n",
            "epoch: 3175 loss: 120.757 val loss: 113.608\n",
            "epoch: 3176 loss: 107.619 val loss: 113.220\n",
            "epoch: 3177 loss: 120.360 val loss: 111.901\n",
            "epoch: 3178 loss: 83.966 val loss: 113.911\n",
            "epoch: 3179 loss: 124.408 val loss: 114.633\n",
            "epoch: 3180 loss: 102.722 val loss: 111.836\n",
            "epoch: 3181 loss: 93.826 val loss: 113.118\n",
            "epoch: 3182 loss: 107.458 val loss: 111.955\n",
            "epoch: 3183 loss: 129.561 val loss: 116.213\n",
            "epoch: 3184 loss: 107.731 val loss: 112.735\n",
            "epoch: 3185 loss: 89.774 val loss: 112.877\n",
            "epoch: 3186 loss: 152.564 val loss: 112.366\n",
            "epoch: 3187 loss: 119.726 val loss: 115.821\n",
            "epoch: 3188 loss: 104.020 val loss: 111.501\n",
            "epoch: 3189 loss: 124.699 val loss: 113.841\n",
            "epoch: 3190 loss: 79.478 val loss: 111.513\n",
            "epoch: 3191 loss: 79.516 val loss: 112.005\n",
            "epoch: 3192 loss: 112.410 val loss: 112.073\n",
            "epoch: 3193 loss: 127.695 val loss: 113.435\n",
            "epoch: 3194 loss: 91.855 val loss: 111.028\n",
            "epoch: 3195 loss: 134.924 val loss: 115.223\n",
            "epoch: 3196 loss: 112.590 val loss: 111.836\n",
            "epoch: 3197 loss: 168.152 val loss: 114.713\n",
            "epoch: 3198 loss: 160.479 val loss: 111.838\n",
            "epoch: 3199 loss: 119.694 val loss: 114.141\n",
            "epoch: 3200 loss: 135.598 val loss: 112.021\n",
            "epoch: 3201 loss: 101.195 val loss: 112.684\n",
            "epoch: 3202 loss: 107.498 val loss: 112.717\n",
            "epoch: 3203 loss: 127.109 val loss: 114.677\n",
            "epoch: 3204 loss: 109.874 val loss: 112.313\n",
            "epoch: 3205 loss: 150.108 val loss: 114.097\n",
            "epoch: 3206 loss: 104.732 val loss: 112.695\n",
            "epoch: 3207 loss: 111.463 val loss: 112.951\n",
            "epoch: 3208 loss: 72.829 val loss: 110.890\n",
            "epoch: 3209 loss: 82.456 val loss: 111.654\n",
            "epoch: 3210 loss: 117.294 val loss: 111.257\n",
            "epoch: 3211 loss: 111.818 val loss: 113.471\n",
            "epoch: 3212 loss: 76.587 val loss: 115.682\n",
            "epoch: 3213 loss: 86.724 val loss: 112.915\n",
            "epoch: 3214 loss: 101.368 val loss: 113.446\n",
            "epoch: 3215 loss: 122.744 val loss: 113.119\n",
            "epoch: 3216 loss: 137.039 val loss: 112.882\n",
            "epoch: 3217 loss: 75.026 val loss: 111.686\n",
            "epoch: 3218 loss: 101.612 val loss: 112.975\n",
            "epoch: 3219 loss: 89.818 val loss: 112.838\n",
            "epoch: 3220 loss: 111.323 val loss: 111.837\n",
            "epoch: 3221 loss: 108.595 val loss: 112.665\n",
            "epoch: 3222 loss: 92.927 val loss: 115.960\n",
            "epoch: 3223 loss: 114.415 val loss: 111.721\n",
            "epoch: 3224 loss: 90.189 val loss: 113.874\n",
            "epoch: 3225 loss: 118.288 val loss: 112.658\n",
            "epoch: 3226 loss: 129.531 val loss: 112.552\n",
            "epoch: 3227 loss: 110.361 val loss: 111.987\n",
            "epoch: 3228 loss: 128.060 val loss: 113.763\n",
            "epoch: 3229 loss: 92.535 val loss: 113.142\n",
            "epoch: 3230 loss: 108.187 val loss: 113.943\n",
            "epoch: 3231 loss: 118.047 val loss: 112.623\n",
            "epoch: 3232 loss: 137.049 val loss: 114.716\n",
            "epoch: 3233 loss: 84.409 val loss: 115.174\n",
            "epoch: 3234 loss: 105.294 val loss: 111.101\n",
            "epoch: 3235 loss: 113.748 val loss: 110.970\n",
            "epoch: 3236 loss: 149.581 val loss: 111.031\n",
            "epoch: 3237 loss: 94.996 val loss: 114.397\n",
            "epoch: 3238 loss: 92.823 val loss: 112.364\n",
            "epoch: 3239 loss: 79.285 val loss: 113.381\n",
            "epoch: 3240 loss: 124.071 val loss: 112.451\n",
            "epoch: 3241 loss: 101.284 val loss: 113.103\n",
            "epoch: 3242 loss: 127.266 val loss: 111.708\n",
            "epoch: 3243 loss: 121.664 val loss: 111.586\n",
            "epoch: 3244 loss: 107.221 val loss: 113.139\n",
            "epoch: 3245 loss: 75.410 val loss: 113.975\n",
            "epoch: 3246 loss: 109.958 val loss: 113.397\n",
            "epoch: 3247 loss: 81.063 val loss: 113.623\n",
            "epoch: 3248 loss: 125.050 val loss: 113.547\n",
            "epoch: 3249 loss: 110.566 val loss: 112.226\n",
            "epoch: 3250 loss: 131.304 val loss: 113.675\n",
            "epoch: 3251 loss: 129.469 val loss: 113.393\n",
            "epoch: 3252 loss: 118.127 val loss: 113.358\n",
            "epoch: 3253 loss: 167.674 val loss: 110.655\n",
            "epoch: 3254 loss: 126.602 val loss: 113.392\n",
            "epoch: 3255 loss: 104.190 val loss: 113.367\n",
            "epoch: 3256 loss: 139.504 val loss: 114.232\n",
            "epoch: 3257 loss: 112.038 val loss: 113.703\n",
            "epoch: 3258 loss: 114.534 val loss: 114.108\n",
            "epoch: 3259 loss: 135.193 val loss: 112.374\n",
            "epoch: 3260 loss: 117.015 val loss: 112.863\n",
            "epoch: 3261 loss: 123.901 val loss: 112.544\n",
            "epoch: 3262 loss: 94.847 val loss: 111.738\n",
            "epoch: 3263 loss: 125.618 val loss: 114.852\n",
            "epoch: 3264 loss: 97.532 val loss: 110.326\n",
            "epoch: 3265 loss: 106.979 val loss: 112.971\n",
            "epoch: 3266 loss: 112.967 val loss: 112.909\n",
            "epoch: 3267 loss: 125.875 val loss: 112.990\n",
            "epoch: 3268 loss: 99.819 val loss: 111.935\n",
            "epoch: 3269 loss: 103.035 val loss: 112.604\n",
            "epoch: 3270 loss: 153.250 val loss: 114.783\n",
            "epoch: 3271 loss: 107.837 val loss: 112.485\n",
            "epoch: 3272 loss: 78.655 val loss: 112.729\n",
            "epoch: 3273 loss: 104.063 val loss: 113.801\n",
            "epoch: 3274 loss: 62.966 val loss: 113.901\n",
            "epoch: 3275 loss: 104.540 val loss: 112.038\n",
            "epoch: 3276 loss: 131.335 val loss: 111.989\n",
            "epoch: 3277 loss: 120.505 val loss: 111.700\n",
            "epoch: 3278 loss: 109.871 val loss: 112.640\n",
            "epoch: 3279 loss: 140.539 val loss: 112.653\n",
            "epoch: 3280 loss: 117.505 val loss: 114.416\n",
            "epoch: 3281 loss: 160.357 val loss: 113.139\n",
            "epoch: 3282 loss: 89.623 val loss: 110.670\n",
            "epoch: 3283 loss: 102.694 val loss: 113.166\n",
            "epoch: 3284 loss: 117.948 val loss: 110.072\n",
            "epoch: 3285 loss: 67.278 val loss: 112.665\n",
            "epoch: 3286 loss: 170.850 val loss: 112.307\n",
            "epoch: 3287 loss: 103.862 val loss: 112.721\n",
            "epoch: 3288 loss: 102.831 val loss: 113.162\n",
            "epoch: 3289 loss: 102.602 val loss: 110.650\n",
            "epoch: 3290 loss: 123.500 val loss: 110.859\n",
            "epoch: 3291 loss: 130.716 val loss: 110.916\n",
            "epoch: 3292 loss: 115.614 val loss: 113.967\n",
            "epoch: 3293 loss: 129.271 val loss: 112.746\n",
            "epoch: 3294 loss: 107.752 val loss: 111.732\n",
            "epoch: 3295 loss: 122.490 val loss: 114.766\n",
            "epoch: 3296 loss: 105.467 val loss: 113.230\n",
            "epoch: 3297 loss: 142.973 val loss: 113.320\n",
            "epoch: 3298 loss: 144.759 val loss: 113.781\n",
            "epoch: 3299 loss: 155.225 val loss: 111.934\n",
            "epoch: 3300 loss: 112.482 val loss: 113.068\n",
            "epoch: 3301 loss: 114.790 val loss: 113.469\n",
            "epoch: 3302 loss: 119.749 val loss: 112.072\n",
            "epoch: 3303 loss: 117.513 val loss: 111.589\n",
            "epoch: 3304 loss: 101.539 val loss: 112.714\n",
            "epoch: 3305 loss: 89.008 val loss: 114.468\n",
            "epoch: 3306 loss: 118.886 val loss: 111.739\n",
            "epoch: 3307 loss: 129.924 val loss: 113.719\n",
            "epoch: 3308 loss: 110.861 val loss: 112.372\n",
            "epoch: 3309 loss: 115.948 val loss: 112.202\n",
            "epoch: 3310 loss: 94.399 val loss: 114.812\n",
            "epoch: 3311 loss: 124.421 val loss: 112.265\n",
            "epoch: 3312 loss: 113.249 val loss: 113.826\n",
            "epoch: 3313 loss: 85.392 val loss: 112.357\n",
            "epoch: 3314 loss: 148.909 val loss: 113.249\n",
            "epoch: 3315 loss: 150.549 val loss: 112.989\n",
            "epoch: 3316 loss: 155.282 val loss: 112.706\n",
            "epoch: 3317 loss: 125.139 val loss: 112.924\n",
            "epoch: 3318 loss: 81.987 val loss: 111.566\n",
            "epoch: 3319 loss: 99.981 val loss: 114.003\n",
            "epoch: 3320 loss: 92.566 val loss: 110.166\n",
            "epoch: 3321 loss: 95.783 val loss: 111.821\n",
            "epoch: 3322 loss: 104.749 val loss: 113.649\n",
            "epoch: 3323 loss: 145.317 val loss: 111.805\n",
            "epoch: 3324 loss: 111.571 val loss: 111.090\n",
            "epoch: 3325 loss: 142.988 val loss: 115.327\n",
            "epoch: 3326 loss: 162.448 val loss: 112.133\n",
            "epoch: 3327 loss: 113.869 val loss: 111.750\n",
            "epoch: 3328 loss: 145.129 val loss: 113.425\n",
            "epoch: 3329 loss: 130.314 val loss: 115.189\n",
            "epoch: 3330 loss: 117.857 val loss: 114.395\n",
            "epoch: 3331 loss: 73.087 val loss: 110.725\n",
            "epoch: 3332 loss: 155.205 val loss: 113.548\n",
            "epoch: 3333 loss: 117.645 val loss: 112.774\n",
            "epoch: 3334 loss: 136.245 val loss: 114.847\n",
            "epoch: 3335 loss: 100.019 val loss: 111.522\n",
            "epoch: 3336 loss: 148.928 val loss: 111.593\n",
            "epoch: 3337 loss: 153.195 val loss: 115.692\n",
            "epoch: 3338 loss: 78.181 val loss: 112.923\n",
            "epoch: 3339 loss: 127.189 val loss: 112.723\n",
            "epoch: 3340 loss: 129.546 val loss: 112.228\n",
            "epoch: 3341 loss: 107.561 val loss: 117.579\n",
            "epoch: 3342 loss: 179.282 val loss: 112.226\n",
            "epoch: 3343 loss: 89.248 val loss: 110.854\n",
            "epoch: 3344 loss: 68.068 val loss: 113.540\n",
            "epoch: 3345 loss: 116.601 val loss: 111.204\n",
            "epoch: 3346 loss: 95.947 val loss: 111.780\n",
            "epoch: 3347 loss: 78.630 val loss: 113.482\n",
            "epoch: 3348 loss: 94.902 val loss: 110.061\n",
            "epoch: 3349 loss: 142.642 val loss: 110.273\n",
            "epoch: 3350 loss: 127.072 val loss: 114.841\n",
            "epoch: 3351 loss: 144.182 val loss: 109.944\n",
            "epoch: 3352 loss: 122.437 val loss: 115.456\n",
            "epoch: 3353 loss: 138.392 val loss: 112.162\n",
            "epoch: 3354 loss: 118.552 val loss: 111.653\n",
            "epoch: 3355 loss: 116.206 val loss: 114.662\n",
            "epoch: 3356 loss: 90.260 val loss: 111.908\n",
            "epoch: 3357 loss: 161.552 val loss: 110.818\n",
            "epoch: 3358 loss: 111.284 val loss: 115.114\n",
            "epoch: 3359 loss: 123.608 val loss: 113.399\n",
            "epoch: 3360 loss: 105.078 val loss: 114.489\n",
            "epoch: 3361 loss: 115.723 val loss: 114.019\n",
            "epoch: 3362 loss: 148.531 val loss: 111.484\n",
            "epoch: 3363 loss: 71.694 val loss: 111.098\n",
            "epoch: 3364 loss: 138.498 val loss: 112.945\n",
            "epoch: 3365 loss: 144.823 val loss: 112.082\n",
            "epoch: 3366 loss: 89.346 val loss: 111.735\n",
            "epoch: 3367 loss: 102.901 val loss: 112.886\n",
            "epoch: 3368 loss: 143.462 val loss: 110.461\n",
            "epoch: 3369 loss: 127.570 val loss: 113.930\n",
            "epoch: 3370 loss: 112.827 val loss: 113.680\n",
            "epoch: 3371 loss: 133.068 val loss: 112.762\n",
            "epoch: 3372 loss: 87.624 val loss: 111.685\n",
            "epoch: 3373 loss: 96.715 val loss: 112.072\n",
            "epoch: 3374 loss: 163.844 val loss: 113.948\n",
            "epoch: 3375 loss: 132.380 val loss: 114.058\n",
            "epoch: 3376 loss: 112.466 val loss: 113.788\n",
            "epoch: 3377 loss: 139.219 val loss: 111.864\n",
            "epoch: 3378 loss: 143.196 val loss: 113.295\n",
            "epoch: 3379 loss: 119.224 val loss: 113.660\n",
            "epoch: 3380 loss: 120.273 val loss: 114.557\n",
            "epoch: 3381 loss: 98.220 val loss: 113.202\n",
            "epoch: 3382 loss: 91.560 val loss: 112.325\n",
            "epoch: 3383 loss: 115.146 val loss: 111.419\n",
            "epoch: 3384 loss: 142.377 val loss: 112.806\n",
            "epoch: 3385 loss: 104.107 val loss: 112.998\n",
            "epoch: 3386 loss: 140.802 val loss: 111.619\n",
            "epoch: 3387 loss: 120.090 val loss: 112.173\n",
            "epoch: 3388 loss: 114.987 val loss: 114.293\n",
            "epoch: 3389 loss: 122.427 val loss: 112.376\n",
            "epoch: 3390 loss: 106.168 val loss: 112.305\n",
            "epoch: 3391 loss: 130.928 val loss: 112.357\n",
            "epoch: 3392 loss: 129.556 val loss: 114.545\n",
            "epoch: 3393 loss: 120.507 val loss: 113.720\n",
            "epoch: 3394 loss: 92.495 val loss: 113.397\n",
            "epoch: 3395 loss: 112.770 val loss: 112.630\n",
            "epoch: 3396 loss: 102.461 val loss: 111.049\n",
            "epoch: 3397 loss: 110.274 val loss: 110.727\n",
            "epoch: 3398 loss: 97.127 val loss: 111.219\n",
            "epoch: 3399 loss: 76.446 val loss: 112.084\n",
            "epoch: 3400 loss: 89.696 val loss: 112.920\n",
            "epoch: 3401 loss: 76.251 val loss: 112.329\n",
            "epoch: 3402 loss: 126.919 val loss: 111.845\n",
            "epoch: 3403 loss: 136.383 val loss: 114.817\n",
            "epoch: 3404 loss: 89.180 val loss: 114.035\n",
            "epoch: 3405 loss: 123.228 val loss: 114.237\n",
            "epoch: 3406 loss: 143.104 val loss: 112.227\n",
            "epoch: 3407 loss: 114.937 val loss: 111.934\n",
            "epoch: 3408 loss: 94.352 val loss: 110.050\n",
            "epoch: 3409 loss: 141.966 val loss: 112.015\n",
            "epoch: 3410 loss: 100.155 val loss: 112.329\n",
            "epoch: 3411 loss: 160.866 val loss: 113.096\n",
            "epoch: 3412 loss: 120.169 val loss: 113.487\n",
            "epoch: 3413 loss: 96.544 val loss: 112.894\n",
            "epoch: 3414 loss: 111.724 val loss: 110.047\n",
            "epoch: 3415 loss: 106.924 val loss: 115.943\n",
            "epoch: 3416 loss: 122.665 val loss: 112.522\n",
            "epoch: 3417 loss: 67.575 val loss: 112.389\n",
            "epoch: 3418 loss: 123.523 val loss: 111.921\n",
            "epoch: 3419 loss: 103.822 val loss: 110.685\n",
            "epoch: 3420 loss: 140.583 val loss: 112.702\n",
            "epoch: 3421 loss: 142.142 val loss: 114.301\n",
            "epoch: 3422 loss: 91.904 val loss: 111.650\n",
            "epoch: 3423 loss: 129.983 val loss: 113.353\n",
            "epoch: 3424 loss: 119.251 val loss: 113.503\n",
            "epoch: 3425 loss: 117.247 val loss: 113.595\n",
            "epoch: 3426 loss: 85.113 val loss: 114.796\n",
            "epoch: 3427 loss: 138.224 val loss: 112.033\n",
            "epoch: 3428 loss: 109.666 val loss: 112.734\n",
            "epoch: 3429 loss: 85.811 val loss: 111.609\n",
            "epoch: 3430 loss: 132.852 val loss: 110.931\n",
            "epoch: 3431 loss: 132.362 val loss: 112.080\n",
            "epoch: 3432 loss: 114.936 val loss: 113.241\n",
            "epoch: 3433 loss: 104.440 val loss: 113.246\n",
            "epoch: 3434 loss: 118.544 val loss: 112.547\n",
            "epoch: 3435 loss: 121.771 val loss: 111.098\n",
            "epoch: 3436 loss: 93.197 val loss: 112.922\n",
            "epoch: 3437 loss: 88.586 val loss: 114.389\n",
            "epoch: 3438 loss: 71.796 val loss: 112.962\n",
            "epoch: 3439 loss: 117.083 val loss: 113.476\n",
            "epoch: 3440 loss: 98.562 val loss: 111.416\n",
            "epoch: 3441 loss: 133.207 val loss: 113.022\n",
            "epoch: 3442 loss: 89.290 val loss: 111.406\n",
            "epoch: 3443 loss: 113.410 val loss: 114.497\n",
            "epoch: 3444 loss: 95.985 val loss: 114.404\n",
            "epoch: 3445 loss: 121.823 val loss: 112.474\n",
            "epoch: 3446 loss: 125.436 val loss: 112.865\n",
            "epoch: 3447 loss: 123.351 val loss: 112.113\n",
            "epoch: 3448 loss: 105.328 val loss: 112.564\n",
            "epoch: 3449 loss: 137.828 val loss: 112.837\n",
            "epoch: 3450 loss: 92.861 val loss: 111.661\n",
            "epoch: 3451 loss: 103.213 val loss: 112.687\n",
            "epoch: 3452 loss: 142.020 val loss: 113.427\n",
            "epoch: 3453 loss: 111.126 val loss: 112.953\n",
            "epoch: 3454 loss: 73.956 val loss: 111.949\n",
            "epoch: 3455 loss: 105.800 val loss: 115.499\n",
            "epoch: 3456 loss: 81.286 val loss: 112.957\n",
            "epoch: 3457 loss: 163.136 val loss: 113.413\n",
            "epoch: 3458 loss: 122.693 val loss: 111.478\n",
            "epoch: 3459 loss: 104.975 val loss: 112.256\n",
            "epoch: 3460 loss: 102.435 val loss: 115.877\n",
            "epoch: 3461 loss: 97.015 val loss: 111.695\n",
            "epoch: 3462 loss: 113.620 val loss: 113.628\n",
            "epoch: 3463 loss: 92.352 val loss: 112.598\n",
            "epoch: 3464 loss: 103.611 val loss: 112.438\n",
            "epoch: 3465 loss: 107.680 val loss: 112.976\n",
            "epoch: 3466 loss: 132.230 val loss: 113.422\n",
            "epoch: 3467 loss: 92.723 val loss: 110.849\n",
            "epoch: 3468 loss: 114.945 val loss: 111.497\n",
            "epoch: 3469 loss: 85.565 val loss: 110.999\n",
            "epoch: 3470 loss: 130.284 val loss: 111.918\n",
            "epoch: 3471 loss: 100.445 val loss: 115.322\n",
            "epoch: 3472 loss: 111.009 val loss: 110.698\n",
            "epoch: 3473 loss: 100.859 val loss: 114.307\n",
            "epoch: 3474 loss: 124.654 val loss: 112.060\n",
            "epoch: 3475 loss: 135.393 val loss: 113.393\n",
            "epoch: 3476 loss: 149.641 val loss: 114.363\n",
            "epoch: 3477 loss: 119.019 val loss: 111.676\n",
            "epoch: 3478 loss: 124.074 val loss: 111.861\n",
            "epoch: 3479 loss: 102.329 val loss: 111.197\n",
            "epoch: 3480 loss: 145.642 val loss: 110.059\n",
            "epoch: 3481 loss: 123.894 val loss: 113.378\n",
            "epoch: 3482 loss: 180.392 val loss: 112.721\n",
            "epoch: 3483 loss: 124.004 val loss: 114.143\n",
            "epoch: 3484 loss: 76.967 val loss: 115.209\n",
            "epoch: 3485 loss: 97.690 val loss: 115.486\n",
            "epoch: 3486 loss: 84.177 val loss: 110.931\n",
            "epoch: 3487 loss: 166.054 val loss: 113.115\n",
            "epoch: 3488 loss: 99.929 val loss: 110.972\n",
            "epoch: 3489 loss: 139.563 val loss: 114.064\n",
            "epoch: 3490 loss: 127.423 val loss: 111.460\n",
            "epoch: 3491 loss: 74.891 val loss: 112.054\n",
            "epoch: 3492 loss: 105.474 val loss: 111.639\n",
            "epoch: 3493 loss: 92.321 val loss: 111.830\n",
            "epoch: 3494 loss: 133.249 val loss: 115.961\n",
            "epoch: 3495 loss: 123.904 val loss: 112.483\n",
            "epoch: 3496 loss: 115.867 val loss: 115.259\n",
            "epoch: 3497 loss: 128.747 val loss: 111.671\n",
            "epoch: 3498 loss: 137.468 val loss: 114.404\n",
            "epoch: 3499 loss: 155.748 val loss: 111.346\n",
            "epoch: 3500 loss: 102.204 val loss: 113.528\n"
          ]
        }
      ],
      "source": [
        "model = regressionHead(kernel=3)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "loss= torch.nn.MSELoss()\n",
        "epochs = 3500\n",
        "train_LLMRegresor(train_loader, val_loader, model, device, loss, opt, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set, test_labels = val_loader.dataset.tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(112.8791, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "test_set = test_set.to(dtype=torch.float32, device=device)\n",
        "test_labels = test_labels.to(dtype=torch.float32, device=device) \n",
        "outcome = model(test_set)\n",
        "loss_test = loss(outcome, test_labels.unsqueeze(-1))\n",
        "print(loss_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w1mseO1-5eNd",
        "outputId": "b0841ce8-3f72-4a7a-e7c0-942628fbe26a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sequence_Example = \"A E T C Z A O\"\\nsequence_Example = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\\nencoded_input = tokenizer(sequence_Example, return_tensors=\\'pt\\')\\noutput = model(**encoded_input)\\nprint(output)'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "'''sequence_Example = \"A E T C Z A O\"\n",
        "sequence_Example = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\n",
        "encoded_input = tokenizer(sequence_Example, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "print(output)'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "latent_msaprot_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0179d901a24a49bebcf619dd9b23f499": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "051662d4e71b4ec4b580f161c9cb5238": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "105c86e728904d7c868a64e72122fcaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f8c6cc382d4731ab5324bd90fb352b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_368d849d84f94ec0a421534f2a84d1cf",
            "placeholder": "​",
            "style": "IPY_MODEL_e4092e002d6a4930bdb5782e94faa99d",
            "value": " 1.68G/1.68G [00:11&lt;00:00, 174MB/s]"
          }
        },
        "18a7e1404b6049e4af2f4bbabfdefb36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20d45acbe62f4e94bf702a4ace6d61fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "229c8aab3d6a45d79295c7df8e3f586e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22a0da479c2849cbb4ba95617989dc40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "244e52841e8e406cbc5609b5aa8ffe2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c7db96ef65d45e185ba2b3b5fc6f6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "368d849d84f94ec0a421534f2a84d1cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "388ad816a93643fea127cd9c33afd97a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d634485f9ef4cd88cfd45397b9bfac0",
            "max": 86,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ea050c4f9324584b513f835c95218fe",
            "value": 86
          }
        },
        "3a8b58ac59844cebb6207b37fe6c7be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80678ab5c8e546119affbb9b6446fa43",
            "placeholder": "​",
            "style": "IPY_MODEL_18a7e1404b6049e4af2f4bbabfdefb36",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "3d4ea54c2fb944978474e6ee09e96afc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dda1a7c8ab140a8ad2595b21af90c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea050c4f9324584b513f835c95218fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44c8715553cf4a3a95936549cc74bb89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45a2a0e196f742c59dcb34a6e9346718": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_787271a9e3164beaa8a1195854e7325e",
            "placeholder": "​",
            "style": "IPY_MODEL_0179d901a24a49bebcf619dd9b23f499",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "4bfb0e43cc2f4991b4510a055baca842": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_befa4bccfd4e494a8abda6c807b71528",
            "max": 81,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b4eb5c771e7425db55130f7b16162db",
            "value": 81
          }
        },
        "4bffb5bb6bae4a86851743382bbda61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a8b58ac59844cebb6207b37fe6c7be2",
              "IPY_MODEL_388ad816a93643fea127cd9c33afd97a",
              "IPY_MODEL_e4dc6ab93bac45a28824e19a1f829c1c"
            ],
            "layout": "IPY_MODEL_8e0ce9e69c564fbc98ce7b5eab24d310"
          }
        },
        "515397afe01c47d083f764ed86033ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5812d055512344d299a37b583163c601": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d634485f9ef4cd88cfd45397b9bfac0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71155af0479f4b59806c50857e5f1c18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77b3df2f0e084407b8cc442539e65bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cd9a2aa719242419d7bc8273aa4a80a",
            "max": 361,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_244e52841e8e406cbc5609b5aa8ffe2f",
            "value": 361
          }
        },
        "7822a461ad034eacacacb56f82d8dfb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db4c9b0b328c4917abe4401d3f9dc033",
              "IPY_MODEL_4bfb0e43cc2f4991b4510a055baca842",
              "IPY_MODEL_e4f552e570964de98ab9e0310ad9b33f"
            ],
            "layout": "IPY_MODEL_3dda1a7c8ab140a8ad2595b21af90c6f"
          }
        },
        "787271a9e3164beaa8a1195854e7325e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dc0fd88e2864a1e8fc2e1215e9a4c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80678ab5c8e546119affbb9b6446fa43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8766f189f5034beda0e0d7aa90c2459c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_105c86e728904d7c868a64e72122fcaa",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_515397afe01c47d083f764ed86033ccb",
            "value": 112
          }
        },
        "8e0ce9e69c564fbc98ce7b5eab24d310": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "941d8050984b4331a0936a1da042768c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ade7c5f1507a4f7690dd110f43da6667",
              "IPY_MODEL_77b3df2f0e084407b8cc442539e65bdf",
              "IPY_MODEL_e0cff3879a01467e9325af0f09e7a18b"
            ],
            "layout": "IPY_MODEL_3d4ea54c2fb944978474e6ee09e96afc"
          }
        },
        "9b4eb5c771e7425db55130f7b16162db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cd9a2aa719242419d7bc8273aa4a80a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a16db5ff745b454eb37563e0936f05c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a85157025c7449e5939f8343888be8f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ade7c5f1507a4f7690dd110f43da6667": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22a0da479c2849cbb4ba95617989dc40",
            "placeholder": "​",
            "style": "IPY_MODEL_2c7db96ef65d45e185ba2b3b5fc6f6bd",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "b1d408429eaa4b3b90055bc2f587f0fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3dbe6a79a954a33b6c51f2f56bafd9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b949f362b2054ffb85c5995fc15159eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_051662d4e71b4ec4b580f161c9cb5238",
            "placeholder": "​",
            "style": "IPY_MODEL_be8e30dcb69745499ba1da6c109591c2",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "ba1440563002411791477f1eb9be3423": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "baf8a5845c7547048a6e8e70a2718747": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be8e30dcb69745499ba1da6c109591c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "befa4bccfd4e494a8abda6c807b71528": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c213b24d72a343e9b8e97737914ac89c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1d408429eaa4b3b90055bc2f587f0fc",
            "placeholder": "​",
            "style": "IPY_MODEL_a85157025c7449e5939f8343888be8f3",
            "value": " 112/112 [00:00&lt;00:00, 1.35kB/s]"
          }
        },
        "db4c9b0b328c4917abe4401d3f9dc033": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44c8715553cf4a3a95936549cc74bb89",
            "placeholder": "​",
            "style": "IPY_MODEL_7dc0fd88e2864a1e8fc2e1215e9a4c8d",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "db61f8623f99475687c3d1300b0fdf3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0cff3879a01467e9325af0f09e7a18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3dbe6a79a954a33b6c51f2f56bafd9c",
            "placeholder": "​",
            "style": "IPY_MODEL_f7f24861c60f4f189170bb7ade386147",
            "value": " 361/361 [00:00&lt;00:00, 6.56kB/s]"
          }
        },
        "e4092e002d6a4930bdb5782e94faa99d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4dc6ab93bac45a28824e19a1f829c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71155af0479f4b59806c50857e5f1c18",
            "placeholder": "​",
            "style": "IPY_MODEL_5812d055512344d299a37b583163c601",
            "value": " 86.0/86.0 [00:00&lt;00:00, 1.43kB/s]"
          }
        },
        "e4f552e570964de98ab9e0310ad9b33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a16db5ff745b454eb37563e0936f05c2",
            "placeholder": "​",
            "style": "IPY_MODEL_ba1440563002411791477f1eb9be3423",
            "value": " 81.0/81.0 [00:00&lt;00:00, 1.60kB/s]"
          }
        },
        "e568a877298f43a0b51dda9f618ee888": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b949f362b2054ffb85c5995fc15159eb",
              "IPY_MODEL_e9126118107f416a85f3c9ad8e49d0ac",
              "IPY_MODEL_13f8c6cc382d4731ab5324bd90fb352b"
            ],
            "layout": "IPY_MODEL_20d45acbe62f4e94bf702a4ace6d61fc"
          }
        },
        "e9126118107f416a85f3c9ad8e49d0ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baf8a5845c7547048a6e8e70a2718747",
            "max": 1684058277,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_229c8aab3d6a45d79295c7df8e3f586e",
            "value": 1684058277
          }
        },
        "f2814c1cd10f4d819acaaa57126dc3e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45a2a0e196f742c59dcb34a6e9346718",
              "IPY_MODEL_8766f189f5034beda0e0d7aa90c2459c",
              "IPY_MODEL_c213b24d72a343e9b8e97737914ac89c"
            ],
            "layout": "IPY_MODEL_db61f8623f99475687c3d1300b0fdf3e"
          }
        },
        "f7f24861c60f4f189170bb7ade386147": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
