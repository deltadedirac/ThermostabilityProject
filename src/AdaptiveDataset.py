import torch
from tqdm.auto import tqdm
import random
import math

class AdaptiveDataset(torch.utils.data.Dataset):
    
    def __init__(self,data, path_dir_esm2, path_dir_PiFold, emb_ensemble = 'single', type_emb='esm2', **kargs):
        self.data = data
        self.data.target = self.data.target.astype(float)
        self.emb_ensemble = emb_ensemble
        self.type_emb = type_emb
        self.path_dir_esm2 = path_dir_esm2
        self.path_dir_PiFold = path_dir_PiFold
        self.correspondence_idx2file = kargs['correspondence_idx']
        
        if 'sampler_use' in kargs:
            self.batch_samplerUse = kargs['sampler_use']
        else:
            self.batch_samplerUse = False #( False, kargs['sampler_use'] )[ 'sampler_use' in kargs ]             
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        #import ipdb; ipdb.set_trace()
        # if batch_sampler is enable, that means that the custome batch sampler will return you a pandas dataframe 
        # index, instead of the index generated by the length of the amount fo samples, i.e. I am not gonna get the index 
        # related to the position in the all structure (if structure has 1000 elements, the value will be from 0 - 999 )
        # but related to dataframe index (would be just related to those inside the indexing in the dataframe)
        if self.batch_samplerUse:
            index = idx
        else:
            index = self.data.index[idx]
        #sample, label = self.data.index[idx], self.data.target[idx] 
        sample, label = index, self.data.target[index]
        return sample, label
        #return self.data.index[idx], self.data.target
    
    def _mergeEmbeddings(self, idx, **kargs):
        
        max_pad_seq_size=kargs['max_pad_seq_size']
        max_pad_emb_dim=kargs['max_pad_emb_dim']
        
        if self.emb_ensemble != 'single':
            #tmp_esm2_emb =  torch.load(f"{self.path_dir_esm2}{self.data.index.get_loc(idx.item())}_{self.data.full_name[idx.item()]}.pt")
            #tmp_PiFold_emb = torch.load( f"{self.path_dir_PiFold}{self.data.index.get_loc(idx.item())}_{self.data.Original_ID[idx.item()]}.pt" )
            tmp_esm2_emb =  torch.load(f"{self.path_dir_esm2}{ self.correspondence_idx2file[idx.item()] }_{self.data.full_name[idx.item()]}.pt")
            tmp_PiFold_emb = torch.load( f"{self.path_dir_PiFold}{ self.correspondence_idx2file[idx.item()] }_{self.data.Original_ID[idx.item()]}.pt" )
            
            max_pad_seq_size = max( max_pad_seq_size, tmp_esm2_emb.shape[0], tmp_PiFold_emb.shape[0])
            max_pad_emb_dim = tmp_esm2_emb.shape[1] + tmp_PiFold_emb.shape[1] #max_pad_emb_dim, tmp_esm2_emb.shape[1], tmp_PiFold_emb[1])

            if 'emb_esm2' in kargs:
                kargs['emb_esm2'].append( tmp_esm2_emb )
            if 'emb_PiFold' in kargs:
                kargs['emb_PiFold'].append( tmp_PiFold_emb )
        else:        
            if self.type_emb == 'esm2':
                #tmp_esm2_emb =  torch.load(f"{self.path_dir_esm2}{self.data.index.get_loc(idx.item())}_{self.data.full_name[idx.item()]}.pt")
                tmp_esm2_emb =  torch.load(f"{self.path_dir_esm2}{ self.correspondence_idx2file[idx.item()] }_{self.data.full_name[idx.item()]}.pt")
                max_pad_seq_size = max( max_pad_seq_size, tmp_esm2_emb.shape[0])
                max_pad_emb_dim = tmp_esm2_emb.shape[1]
                kargs['emb_esm2'].append( tmp_esm2_emb )
            
            else:
                #tmp_PiFold_emb = torch.load( f"{self.path_dir_PiFold}{self.data.index.get_loc(idx.item())}_{self.data.Original_ID[idx.item()]}.pt" )
                tmp_PiFold_emb = torch.load( f"{self.path_dir_PiFold}{ self.correspondence_idx2file[idx.item()] }_{self.data.Original_ID[idx.item()]}.pt" )
                max_pad_seq_size = max( max_pad_seq_size, tmp_PiFold_emb.shape[0])
                max_pad_emb_dim = tmp_PiFold_emb.shape[1]
                kargs['emb_PiFold'].append( tmp_PiFold_emb )
        
        return max_pad_seq_size, max_pad_emb_dim
    
    def _fill_padded_matrix(self, matrix, a, b, batch_idx=0):
        
        #shape_a, shape_b  = a[batch_idx].shape, b[batch_idx].shape
        if batch_idx < matrix.shape[0]:
            if self.emb_ensemble != 'single':
                matrix[batch_idx, 0:a[batch_idx].shape[0], 0:a[batch_idx].shape[1]] = a[batch_idx]
                matrix[batch_idx, 0:b[batch_idx].shape[0], a[batch_idx].shape[1]:( a[batch_idx].shape[1] + b[batch_idx].shape[1] ) ] = b[batch_idx]
            else:
                tmp =  a[batch_idx] if len(a)!=0 else b[batch_idx]
                matrix[batch_idx, 0:tmp.shape[0], 0:tmp.shape[1]] = tmp

            self._fill_padded_matrix( matrix, a, b, batch_idx = batch_idx+1)
            #return matrix
                
            
    def getdata_fromQuery(self,idx_set):
        emb_esm2 = []
        emb_PiFold = []
        max_pad_seq_size = 0; max_pad_emb_dim=0
        #import ipdb; ipdb.set_trace()
        
        # fix the way to do the embeddings (to save), in order to avoid the for extra for-loop
        #for i in tqdm(idx_set, desc="loading individual embeddings:", leave=False):
        #print("loading individual embeddings.. \n")
        
        for i in idx_set:
            max_pad_seq_size, max_pad_emb_dim = self._mergeEmbeddings(i, emb_esm2=emb_esm2, emb_PiFold = emb_PiFold, 
                                                                      max_pad_seq_size = max_pad_seq_size , max_pad_emb_dim=max_pad_emb_dim)
        
        batch_ensembled = torch.zeros(len(idx_set), max_pad_seq_size, max_pad_emb_dim)
        
        self._fill_padded_matrix( batch_ensembled, emb_esm2, emb_PiFold)

        
        return batch_ensembled



class Species_BatchSample:
    
    """Stratified batch sampling
    Provides equal representation of target classes in each batch
    """
    def __init__(self, df, batch_size, shuffle=True):
        #import ipdb; ipdb.set_trace()#442 the amount of batches should be
        import math
        self.batch_size = batch_size
        self.df = df
        self.shuffle = shuffle
        species = df.organism.unique().tolist()
        #gg=df.organism.value_counts().tolist()
        self.length=sum( list(map(lambda el: math.ceil(el/51), df.organism.value_counts().tolist())) )
        #print(sum(ff))

    def __split__(self, list_a, chunk_size):
        for i in range(0, len(list_a), chunk_size):
            yield list_a[i:i + chunk_size]
    
    def _split_idx_per_species(self, species):
        df_species1=self.df[self.df.organism==species]
        idx_species=df_species1.index.tolist()
        if self.shuffle==True:
            random.shuffle(idx_species)
            
        return list(self.__split__(idx_species, self.batch_size))
    
    def _make_species_partitions(self):
        #import ipdb; ipdb.set_trace()
        idx_list_per_organism = []
        organism_list = self.df.organism.unique().tolist()
        
        for species in organism_list:
            idx_list_per_organism.extend(
                                            self._split_idx_per_species(species)   
                                         )
        
        if self.shuffle == True:
            random.shuffle(idx_list_per_organism)
        
        return idx_list_per_organism
        
    
    def __iter__(self):

        for species_idxs in self._make_species_partitions():
            yield species_idxs

    def __len__(self):
        return self.length#len(self.df)
    
    


class Species_BatchSample2:
    
    """Stratified batch sampling just ORDERING ACCORDING TO TAXONOMY, 
    AND DELIVER BATCHES MAJORITARY TO ONE SPECIES BUT WITH SOME PART OF CONTAMINATION
    IN SOME BATCHES
    Provides equal representation of target classes in each batch
    """
    def __init__(self, df, batch_size, shuffle=True):
        #import ipdb; ipdb.set_trace()#442 the amount of batches should be
        
        self.batch_size = batch_size
        self.df = df.sort_values(by=['organism'])
        #self.shuffle = shuffle
        species = df.organism.unique().tolist()


    def __split__(self, list_a, chunk_size):
        for i in range(0, len(list_a), chunk_size):
            yield list_a[i:i + chunk_size]
    
    def _split_idx_per_species(self, idx_lists):
        idx_species=idx_lists
        return list(self.__split__(idx_species, self.batch_size))        
    
    def __iter__(self):
        idx_ordered_species = self.df.organism.index.tolist()
        for species_idxs in self._split_idx_per_species(idx_ordered_species):
            yield species_idxs

    def __len__(self):
        return math.ceil( len(self.df)/self.batch_size )